{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeaf7c44",
   "metadata": {},
   "source": [
    "Ce notebook a comme source principale [Carlucci et al. (2019)](https://arxiv.org/pdf/1903.06864.pdf).\n",
    "\n",
    "Nous commençons par expliquer les méthodes et les procédures en détail avant de les appliquer à un ensemble de données différent de celui utilisé dans l'article (PACS).   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05aedb4",
   "metadata": {},
   "source": [
    "<font size=\"5\"> **Méthodes et procédures utilisées pour la généralisation de domaine en résolvant des casse-têtes/puzzles**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf67ef1",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "L'être humain sait naturellement apprendre en combinant les méthodes supervisées (le savoir transmis par les parents à l'enfant) et non-supervisées (la découverte par l'enfant lui-même à travers ses expériences). Graĉe à ça, il arrive facilement à détecter les irrégularités et à gérer les invariances pour généraliser son savoir. Alors qu'un humain sachant reconnaître un cheval, pourra aussi bien le reconnaître sur une photographie que sur une peinture, c'est une tâche un peu plus complexe et laborieuse pour les algorithmes d'IA en reconnaissance d'images, auxquels il manque encore souvent cette capacité de généralisation.\n",
    "\n",
    "Ce manque de généralisation dans la tâche de reconnaissance d'images a tendance à être pallié par des méthodes supervisées. Par exemple, dans un précédent TP portant sur la classification d'images, nous avions entrainé des réseaux convolutionnels à l'aide d'images labélisées, et pour tenter d'augmenter la capacité de généralisation, nous avions appliqué de multiples transformations afin de déformer nos images, en conservant les labels, puis nous avions entrainé les réseaux en incluant ces images déformées dans notre jeu de données d'entrainement.\n",
    "\n",
    "Cependant, les méthodes supervisées sont par définition limitées par leur non-exaustivité. C'est pourquoi il est intéressant d'explorer la puissance de l'apprentissage non-supervisé, et nous allons ici l'utiliser à des fins de généralisation entre domaines de style. Le but final est d'obtenir un algorithme de reconnaissance d'images capable par exemple de recconaître un cheval sur une image, que celle-ci soit une photographie, une peinture à l'aquarelle, ou issue d'un cartoon (3 domaines différents, 3 styles).\n",
    "\n",
    "Pour cela, nous allons présenter la méthode JiGen développée par F. Carlucci, A. D'Innocente, S. Bucci, B. Caputo, et T. Tommasi, des chercheurs italiens, en 2019. L'idée est la suivante : améliorer la généralisation en reconnaissance d'images provenant de différents domaines en combinant une clasiffication supervisée et une résolution de puzzle de type JigSaw de ces même images afin d'ajouter une cohérence spatiale à la classification.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada85906",
   "metadata": {},
   "source": [
    "# Les puzzles JigSaw dans la méthode JiGen\n",
    "\n",
    "Un *JigSaw Puzzle* se traduit simplement en français par puzzle, la segmentation d'une image en fragments de formes différentes, qu'il faut réassembler pour retrouver l'image initiale. Ces puzzles sont souvent utilisés pour apprendre la cohérence spaciale aux enfants en les encoureageant à apprendre les relations entre les différentes parties des objets. Cet apprentissage est une des clés donnant aux humains cette grande capacité de généralisation visuelle et c'est un mimétisme informatique de cela qui est mis en place à travers la méthode JiGen.\n",
    "\n",
    "Ici, toutes nos images seront de forme carrée, de taille 227 par 227, et lorsqu'on parlera de puzzle JigSaw, les fragments ne seront pas de formes aléatoires mais simplement un découpage en plus petits segments carrés de tailles identiques.\n",
    "\n",
    "On va donc se concentrer sur la résolution de puzzle JigSaw (retrouver l'image initiale à partir d'une image ayant été mélangée) de façon non-supervisée. Cette résolution ne sera pas l'objectif principal, qui reste la classification des images, mais nous allons voir qu'elle améliore les capacités de généralisation de la méthode globale. Les images originales et les images mélangées ayant la même taille, elles peuvent partager le même réseau convolutionnel, afin de combiner la résolution de puzzles apportant la partie cohérence spatiale de l'image, et la classification, apportant la partie le contenu de l'image.\n",
    "\n",
    "C'est cette double résolution simultanée qui va permettre à la méthode JiGen d'être si performante en matière de généralisation à travers les différents domaines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e35dfe",
   "metadata": {},
   "source": [
    "## Généralisation de domaine\n",
    "\n",
    "La généralisation de domaine fait référence à la capacité d'un modèle de machine learning à généraliser ses capacités sur des domaines non vus ou des données hors distribution. Cela s'oppose à l'apprentissage supervisé traditionnel, qui suppose que les données d'entraînement et de test proviennent du même domaine ou de la même distribution. [Wang et al. (2022)]\n",
    "\n",
    "Le modèle est entraîné sur différents domaines, les données sources (si l'on considère un style d'image, cela pourrait être un dessin, une peinture, un dessin animé, etc.). Nous voulons que le modèle soit capable de prédire avec précision la classe dans un domaine non vu, les données cibles (par exemple, des photos).\n",
    "\n",
    "*Définition 1 (Domaine).*\n",
    "Soit X un espace d'entrée non vide (input) et Y un espace de sortie (output). Un domaine est composé de données échantillonnées à partir d'une distribution. Nous le notons comme $S = {(x_i, y_i)}^n_{i=1}∼P_{XY}$ , où $x \\in X \\subset \\mathbb{R}^d$, $y \\in Y \\subset \\mathbb{R}$ représente le label, et $P_{XY}$ représente la distribution conjointe de l'échantillon d'entrée et du label de sortie. X et Y désignent les variables aléatoires correspondantes. [Wang et al. (2022)]\n",
    "\n",
    "*Définition 2 (Généralisation de domaine).*\n",
    "En généralisation de domaine, on pose M domaines d'entraînement (source) $S_{train} = \\{S^i | i = 1, ..., M \\}$, où $S^i = {(x^i_j , y^i_j )}^{n_i}_{j=1}$ désigne le i-ème domaine. Les distributions conjointes entre chaque paire de domaines sont différentes : $P^i_{XY} \\neq P^j_{XY}$, $1 \\leq i \\neq j \\leq M$. L'objectif de la généralisation de domaine est d'apprendre une fonction prédictive robuste et généralisable $h : X → Y$ à partir des M domaines d'entraînement pour obtenir une erreur de prédiction minimale sur un domaine de test non vu $S_{test}$ (c'est-à-dire, $S_{test}$ n'est pas accessible lors de l'entraînement et $P^{test}_{XY} \\neq P^i_{XY}$ pour $i \\in \\{1, ..., M\\}$):\n",
    "$$ \\min_h \\mathbb{E}_{(x,y) \\in S_{test}} [\\ell(h(x), y)] $$\n",
    "où $\\ell(·, ·)$ est la fonction de perte. [Wang et al. (2022)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d08c2e2",
   "metadata": {},
   "source": [
    "## JiGENDG\n",
    "L'algorithme repose sur l'idée d'utiliser des casse-têtes/puzzles pour entraîner un modèle à être invariant sur différents domaines. Le réseau apprend simultanément à résoudre les casse-têtes et à classifier les images. [Carlucci et al. (2019)]\n",
    "\n",
    "\n",
    "### Dataset\n",
    "Les données d'entrée sont un ensemble de N images provenant de S domaines. Dans chaque domaine i, nous avons $N_i$ observations labellisées. Nous écrivons $\\{ x^i_j, y^i_j \\} _{j=1}^{N_i}$, ce qui signifie que pour la j-ème image du i-ème domaine $x^i_j$, le label associé est $y^i_j$.\n",
    "\n",
    "Nous avons $x^i_j \\in \\mathbb{R}^{n_p \\times n_p}$ où $n_p \\times n_p$ est la taille des images en pixels, en supposant que les images sont des carrés. Nous avons $y^i_j \\in \\mathbb{R}^{C}$ où C est le nombre de classes, car l'étiquette $y^i_j$ est encodée en one-hot.\n",
    "\n",
    "En termes de dimensions, $\\{x^i_j, y^i_j\\}_{j=1}^{N_i} \\in \\left( \\left( \\mathbb{R}^{n_p \\times n_p} \\times \\mathbb{R}^{C} \\right) ^ {N_i} \\right) ^ {S}$ où $N_i \\times S \\leq N$ car le nombre d'images étiquetées $N_i \\times S$ ne dépasse pas le nombre total d'images $N$.\n",
    "\n",
    "\n",
    "### Dataset permuté\n",
    "À partir de l'ensemble de données non permuté, nous créons un nouveau jeu de données utilisé pour la tâche de résolution de casse-têtes. Nous considérons des permutations sur une grille $n \\times n$ (dans l'article et notre travail, nous fixons $n=3$).\n",
    "\n",
    "Bien que nous ayons un total de $n^2!$ permutations possibles, nous n'en considérons que P. Nous les choisissons en fonction de la distance de Hamming, ce qui signifie que nous ne conservons que celles avec le moins de différences de position. Cela permet de simplifier un peu la tâche et également de réduire le temps d'inférence (utiliser les $n^2!=362 880$ possibilités serait beaucoup plus chronophage que d'utiliser $P=30$ permutations).\n",
    "\n",
    "La non-permutation est toujours incluse dans le sous-ensemble des permutations P.\n",
    "\n",
    "Chaque permutation possible est associée à un indice qui permet de traiter le problème comme une tâche de classification où l'étiquette est un vecteur encodé en one-hot des indices des permutations.\n",
    "\n",
    "Nous notons $\\left\\{z^i_k, p^i_k\\right\\}_{k=1}^{K_i} \\in \\left( \\left( \\mathbb{R}^{n_p \\times n_p} \\times \\mathbb{R}^{P} \\right) ^ {K_i} \\right) ^ {S}$ où $z^i_k$ est l'image permutée, $p^i_k$ est l'indice de la permutation utilisée sur l'image associée, $K_i$ est le nombre d'instances étiquetées et $P$ est le nombre de permutations considérées.\n",
    "\n",
    "\n",
    "\n",
    "###  Fonction de perte\n",
    "Rappelons brièvement comment un réseau de neurones est entraîné :\n",
    "- Le modèle traite un batch de $b$ échantillons d'entrée. Chaque échantillon passe à travers le réseau, et la sortie est calculée.\n",
    "- La fonction de perte est appliquée à la sortie prédite et aux valeurs cibles pour le lot. Cette perte représente la dissimilarité entre les valeurs prédites et réelles.\n",
    "- Les gradients cumulés de la perte sont calculés par rapport à chaque paramètre.\n",
    "- Les paramètres du modèle sont mis à jour en fonction des gradients calculés. Le taux d'apprentissage $\\eta$ contrôle dans quelle mesure les paramètres du modèle changent dans la direction qui minimise la perte.\n",
    "\n",
    "Ce processus est répété pendant $E$ époques. Chaque époque implique le traitement de l'ensemble du jeu de données.\n",
    "\n",
    "Les batches sont composés d'un mélange d'images ordonnées et mélangées. Le ratio est défini par $\\beta$ : pour $\\beta=0.75$, 75 % du batch est composé d'images ordonnées et le reste d'images mélangées. Si nous avons un batch de taille $b=128$, cela signifierait que nous avons $N_i=0.75\\times128=96$ et $K_i=(1-0.75)\\times128=32$.\n",
    "\n",
    "Dans JiGen, la fonction de perte prend une forme particulière car deux tâches sont apprises.\n",
    "\n",
    "\n",
    "#### Cas supervisé\n",
    "Nous cherchons à optimiser les paramètres avec le problème de minimisation suivant :\n",
    "\n",
    "$$ argmin_{\\theta_f, \\theta_p, \\theta_c} \\sum_{i=1}^{S} \\sum_{j=1}^{N_i} \\mathcal{L}_c \\left( h(x^i_j|\\theta_f, \\theta_c), y^i_j\\right) + \\sum_{k=1}^{K_i} \\alpha \\mathcal{L}_p \\left( h(z^i_k|\\theta_f, \\theta_p), p^i_k\\right) $$\n",
    "\n",
    "- $\\mathcal{L}_c$ est une perte (la cross-entropy) pour la tâche de classification d'image. Nous rappelons que $\\mathcal{L}c \\left( h(x^i_j|\\theta_f, \\theta_c), y^i_j\\right) = - \\sum_{c \\in C} y^i_j \\log(\\mathbb{P}(h(x^i_j|\\theta_f, \\theta_c)=c))$ ;\n",
    "- $\\mathcal{L}_p$ est une perte (la cross-entropy) pour la tâche de résolution de casse-têtes ;\n",
    "- $\\alpha$ est le poids de la perte pour le casse-tête (l'importance que nous accordons à la tâche de résolution de casse-tête par rapport à celle de la tâche de classification) ;\n",
    "- $h$ est la fonction d'activation du modèle profond (deep model), elle prédit le label ;\n",
    "- $\\theta_f$ est l'ensemble des paramètres (poids et biais) pour la couche entièrement connectée (fully connected layer);\n",
    "- $\\theta_p$ est l'ensemble des paramètres pour la dernière couche entièrement connectée dédiée à la reconnaissance de permutation ;\n",
    "- $\\theta_c$ est l'ensemble des paramètres pour la couche de convolution.\n",
    "\n",
    "La perte du casse-tête $\\mathcal{L}_p$ est calculée sur l'image ordonnée, mais la perte de classification $\\mathcal{L}_c$ n'est pas calculée sur les images mélangées car cela rendrait la reconnaissance d'objets plus difficile.\n",
    "\n",
    "\n",
    "#### Unsupervised case\n",
    "JiGen a été conçu dans le but de la généralisation de domaine non supervisée. La seule différence avec JiGen dans le cas supervisé réside dans la perte pour la tâche de classification d'image :\n",
    "\n",
    "$$ argmin_{\\theta_f, \\theta_p, \\theta_c} \\sum_{i=1}^{S} \\mathcal{L}_E (x^i) + \\sum_{k=1}^{K_i} \\alpha \\mathcal{L}_p \\left( h(z^i_k|\\theta_f, \\theta_p), p^i_k\\right) $$\n",
    "\n",
    "avec $\\mathcal{L}_E (x^i) = \\sum_{y \\in \\mathcal{Y}} h(x^i|\\theta_f, \\theta_c) \\log(h(x^i|\\theta_f, \\theta_c))$, la cross-entropy empirique.\n",
    "\n",
    "Remarque : la somme $\\sum_{j=1}^{N_i}$ disparaît car nous considérons toutes les images et non pas seulement celles étiquetées.\n",
    "\n",
    "\n",
    "### Test\n",
    "Pour tester le modèle, nous ne considérons que la partie classification du réseau : nous n'utilisons pas la couche entièrement connectée finale qui sert à la résolution de casse-tête. Cela revient à fixer $\\alpha=0$.\n",
    "\n",
    "\n",
    "### Parameters\n",
    "Pour toutes les expériences, nous préciserons clairement les valeurs des caractéristiques de l'ensemble de données : les tailles des images $n_b$, le nombre d'images $N$, et le nombre de classes $C$.\n",
    "\n",
    "Pour tous les paramètres, nous considérerons les mêmes paramètres pour le casse-tête : la taille de la grille $n$, le nombre de permutations considérées $P$, et le biais des données $\\beta$. Les auteurs choisissent ces paramètres avec une validation croisée sur 10 % de l'ensemble de données, pour chaque expérience.\n",
    "\n",
    "Nous fixerons les paramètres d'expérience : la taille des batchs $b$, le nombre d'époques $E$, le taux d'apprentissage $\\eta$, et le poids du casse-tête $\\alpha$. (Les auteurs ont fixé ...)\n",
    "\n",
    "Les paramètres du modèle optimisés par rétropropagation et non choisis par l'utilisateur sont $\\theta_f$, $\\theta_p$, et $\\theta_c$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d617ee3b",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f966e6f2",
   "metadata": {},
   "source": [
    "**[Carlucci et al. (2019)]** Carlucci, F. M., D'Innocente, A., Bucci, S., Caputo, B., & Tommasi, T. (2019). Domain Generalization by Solving Jigsaw Puzzles. arXiv preprint arXiv:1903.06864. [URL](https://arxiv.org/pdf/1903.06864.pdf)\n",
    "\n",
    "**[Wang et al. (2022)]** Wang, J., Lan, C., Liu, C., Ouyang, Y., Qin, T., Lu, W., Chen, Y., Zeng, W., & Yu, P. S. (2022). Generalizing to Unseen Domains: A Survey on Domain Generalization. arXiv preprint arXiv:2103.03097. [URL](https://arxiv.org/pdf/2103.03097.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416d1ada",
   "metadata": {},
   "source": [
    "# Jeu de données PACS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa5545e",
   "metadata": {},
   "source": [
    "## Présentation de la base de données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167a5e8a",
   "metadata": {},
   "source": [
    "Le dataset PACS est un jeu de données qui est souvent utilisé pour évaluer les performances d'algorithmes de machine learning sur des tâches de généralisation de domaine.\n",
    "\n",
    "Il comporte en tout 19982 images, de quatres domaines différents :\n",
    "\n",
    "- *Photo* : des photographies réelles,\n",
    "- *Art Painting* : des oeuvres d'art en peinture,\n",
    "- *Cartoon* : des dessins,\n",
    "- *Sketch* : des esquisses en noir et blanc.\n",
    "\n",
    "De plus, le dataset comporte sept catégories, c'est à dire que les sujets des images (de n'importe quel domaine) sont soit :\n",
    "- des chiens,\n",
    "- des élephants,\n",
    "- des girafes,\n",
    "- des guitares,\n",
    "- des cheveaux,\n",
    "- des maisons,\n",
    "- ou des humains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8562507a",
   "metadata": {},
   "source": [
    "Des exemples issus du dataset PACS sont présentés ci-dessous, pour chaque domaine et pour chaque catégorie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3121f1",
   "metadata": {},
   "source": [
    "![images exemple](exemples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764f7e27",
   "metadata": {},
   "source": [
    "## Importation des échantillons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbe7f03",
   "metadata": {},
   "source": [
    "Pour importer les images du jeu de données, on utilise *deeplake*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74c5013",
   "metadata": {},
   "source": [
    "Trois échantillons ont déjà été formés :\n",
    "- un échantillon d'apprentissage avec 8977 images (45 % de l'ensemble des données)\n",
    "- un échantillon de validation avec 1014 images (5 % de l'ensemble des données)\n",
    "- un échantillon de test avec 9991 images (50 % de l'ensemble des données)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e9a352",
   "metadata": {},
   "source": [
    "## Structure de la base de données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae4dc37",
   "metadata": {},
   "source": [
    "Pour chaque index, on a trois élements :\n",
    "- un tenseur qui contient une image en couleur de taille $227 \\times 227$,\n",
    "- un tenseur qui représente le label de cette image par un chiffre entre 0 et 6,\n",
    "- un tenseur qui indique son domaine avec un chiffre entre 0 et 4.\n",
    "\n",
    "Les correspondances entre les valeurs et les noms des labels et des domaines peuvent être affichées comme suit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fe44bb",
   "metadata": {},
   "source": [
    "## Répartition des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2c827a",
   "metadata": {},
   "source": [
    "Il est possible d'étudier la répartition des données à l'intérieur de chacun des trois échantillons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cf63be",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{|c|c|c|c|c|c|c|c|}\n",
    "\\hline\n",
    "        {\\textbf{Dataset}} & \\textbf{Chien (%)} & \\textbf{Elephant (%)} & \\textbf{Girafe (%)} & \\textbf{Guitare (%)} & \\textbf{Cheval (%)} & \\textbf{Maison (%)} & \\textbf{Personne (%)} \\\\ \\hline\n",
    "        \\textbf{Répartition} & 17.32 & 16.55 & 15.67 & 11.14 & 15.42 & 9.42 & 14.47 \\\\ \\hline\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b13388f",
   "metadata": {},
   "source": [
    "Le tableau ci-dessus présente les pourcentages d'images de chaque catégorie dans chacun des échantillons : par exemple, 17.32% des images de l'échantillon d'apprentissage représentent des chiens (label 0).\n",
    "\n",
    "Ce tableau met donc en évidence que les données sont réparties relativement équitablement entre les sept catégories. Les images les plus présentes sont celles de chiens et d'élephants (labels 0 et 1), alors qu'on a un peu moins d'images de maisons (label 5)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16a7bd3",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{|c|c|c|c|c|}\n",
    "\\hline\n",
    "\\textbf{Dataset} & \\textbf{Art Painting (%)} & \\textbf{Cartoon (%)} & \\textbf{Photo (%)} & \\textbf{Sketch (%)}  \\\\ \\hline\n",
    "\\textbf{Répartition} & 20.50 & 23.47 & 16.70 & 39.33 \\\\ \\hline\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7648d852",
   "metadata": {},
   "source": [
    "Le tableau ci-dessus présente les pourcentages d'images de chaque domaine pour tous les échantillons. On observe qu'il y a plus de croquis (domain 3) que de photographies (domain 0) mais que les données restent bien équilibrées."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5083e865",
   "metadata": {},
   "source": [
    "# Using JiGen on PACS (as in the article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16989a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stocker le dossier PACS au même endroit que ce notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "415285ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-24 15:22:41.699457: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from IPython.core.debugger import set_trace\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torch.autograd import Function\n",
    "from torchvision.models.resnet import BasicBlock,Bottleneck\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch import optim\n",
    "import torchvision\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import join, dirname\n",
    "from collections import OrderedDict\n",
    "from itertools import chain\n",
    "from PIL import Image\n",
    "from random import sample, random\n",
    "import bisect\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import scipy.misc \n",
    "try:\n",
    "    from StringIO import StringIO  # Python 2.7\n",
    "except ImportError:\n",
    "    from io import BytesIO         # Python 3.x\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1443b1",
   "metadata": {},
   "source": [
    "## Fichiers de /model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afac8f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common to all networks definition\n",
    "class Id(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Id, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8555331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_utils.py\n",
    "\n",
    "class GradientKillerLayer(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, **kwargs):\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return None, None\n",
    "\n",
    "\n",
    "class ReverseLayerF(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_val):\n",
    "        ctx.lambda_val = lambda_val\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.lambda_val\n",
    "\n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbf8cf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet.py\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, jigsaw_classes=1000, classes=100, domains=3):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.jigsaw_classifier = nn.Linear(512 * block.expansion, jigsaw_classes)\n",
    "        self.class_classifier = nn.Linear(512 * block.expansion, classes)\n",
    "        #self.domain_classifier = nn.Linear(512 * block.expansion, domains)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def is_patch_based(self):\n",
    "        return False\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.jigsaw_classifier(x),self.class_classifier(x)\n",
    "\n",
    "\n",
    "def resnet18(pretrained=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    print(\"Using ResNet-18\")\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url('https://download.pytorch.org/models/resnet18-5c106cde.pth'), strict=False)\n",
    "        #model.load_state_dict(model_zoo.load_url(model_urls['resnet18']), strict=False)\n",
    "    return model\n",
    "\n",
    "def resnet50(pretrained=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    print(\"Using ResNet-50\")\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url('https://download.pytorch.org/models/resnet50-19c8e357.pth'), strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48112cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_factory.py\n",
    "\n",
    "nets_map = {\n",
    "    'resnet18': resnet18,\n",
    "    'resnet50': resnet50,\n",
    "}\n",
    "\n",
    "\n",
    "def get_network(name):\n",
    "    if name not in nets_map:\n",
    "        raise ValueError('Name of network unknown %s' % name)\n",
    "\n",
    "    def get_network_fn(**kwargs):\n",
    "        return nets_map[name](**kwargs)\n",
    "\n",
    "    return get_network_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984998b8",
   "metadata": {},
   "source": [
    "## Fichiers de /data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f2e3fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardDataset.py\n",
    "\n",
    "def get_dataset(path, mode, image_size):\n",
    "    if mode == \"train\":\n",
    "        img_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(image_size, scale=(0.7, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[1/256., 1/256., 1/256.])  # std=[1/256., 1/256., 1/256.] #[0.229, 0.224, 0.225]\n",
    "        ])\n",
    "    else:\n",
    "        img_transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            # transforms.CenterCrop(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], std=[1/256., 1/256., 1/256.])  # std=[1/256., 1/256., 1/256.]\n",
    "        ])\n",
    "    return datasets.ImageFolder(path, transform=img_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d03bdae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JigsawLoader.py\n",
    "\n",
    "\n",
    "def get_random_subset(names, labels, percent):\n",
    "    \"\"\"\n",
    "\n",
    "    :param names: list of names\n",
    "    :param labels:  list of labels\n",
    "    :param percent: 0 < float < 1\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    samples = len(names)\n",
    "    amount = int(samples * percent)\n",
    "    random_index = sample(range(samples), amount)\n",
    "    name_val = [names[k] for k in random_index]\n",
    "    name_train = [v for k, v in enumerate(names) if k not in random_index]\n",
    "    labels_val = [labels[k] for k in random_index]\n",
    "    labels_train = [v for k, v in enumerate(labels) if k not in random_index]\n",
    "    return name_train, name_val, labels_train, labels_val\n",
    "\n",
    "\n",
    "def _dataset_info(txt_labels):\n",
    "    with open(txt_labels, 'r') as f:\n",
    "        images_list = f.readlines()\n",
    "\n",
    "    file_names = []\n",
    "    labels = []\n",
    "    for row in images_list:\n",
    "        row = row.split(' ')\n",
    "        file_names.append(row[0])\n",
    "        labels.append(int(row[1]))\n",
    "\n",
    "    return file_names, labels\n",
    "\n",
    "\n",
    "def get_split_dataset_info(txt_list, val_percentage):\n",
    "    names, labels = _dataset_info(txt_list)\n",
    "    return get_random_subset(names, labels, val_percentage)\n",
    "\n",
    "\n",
    "class JigsawDataset(data.Dataset):\n",
    "    def __init__(self, names, labels, jig_classes=100, img_transformer=None, tile_transformer=None, patches=True, bias_whole_image=None):\n",
    "        self.data_path = \"\"\n",
    "        self.names = names\n",
    "        self.labels = labels\n",
    "\n",
    "        self.N = len(self.names)\n",
    "        self.permutations = self.__retrieve_permutations(jig_classes)\n",
    "        self.grid_size = 3\n",
    "        self.bias_whole_image = bias_whole_image\n",
    "        if patches:\n",
    "            self.patch_size = 64\n",
    "        self._image_transformer = img_transformer\n",
    "        self._augment_tile = tile_transformer\n",
    "        if patches:\n",
    "            self.returnFunc = lambda x: x\n",
    "        else:\n",
    "            def make_grid(x):\n",
    "                return torchvision.utils.make_grid(x, self.grid_size, padding=0)\n",
    "            self.returnFunc = make_grid\n",
    "\n",
    "    def get_tile(self, img, n):\n",
    "        w = float(img.size[0]) / self.grid_size\n",
    "        y = int(n / self.grid_size)\n",
    "        x = n % self.grid_size\n",
    "        tile = img.crop([x * w, y * w, (x + 1) * w, (y + 1) * w])\n",
    "        tile = self._augment_tile(tile)\n",
    "        return tile\n",
    "    \n",
    "    def get_image(self, index):\n",
    "        framename = self.data_path + self.names[index]\n",
    "        img = Image.open(framename).convert('RGB')\n",
    "        return self._image_transformer(img)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = self.get_image(index)\n",
    "        n_grids = self.grid_size ** 2\n",
    "        tiles = [None] * n_grids\n",
    "        for n in range(n_grids):\n",
    "            tiles[n] = self.get_tile(img, n)\n",
    "\n",
    "        order = np.random.randint(len(self.permutations) + 1)  # added 1 for class 0: unsorted\n",
    "        if self.bias_whole_image:\n",
    "            if self.bias_whole_image > random():\n",
    "                order = 0\n",
    "        if order == 0:\n",
    "            data = tiles\n",
    "        else:\n",
    "            data = [tiles[self.permutations[order - 1][t]] for t in range(n_grids)]\n",
    "            \n",
    "        data = torch.stack(data, 0)\n",
    "        return self.returnFunc(data), int(order), int(self.labels[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __retrieve_permutations(self, classes):\n",
    "        all_perm = np.load('permutations_%d.npy' % (classes))\n",
    "        # from range [1,9] to [0,8]\n",
    "        if all_perm.min() == 1:\n",
    "            all_perm = all_perm - 1\n",
    "\n",
    "        return all_perm\n",
    "\n",
    "\n",
    "class JigsawTestDataset(JigsawDataset):\n",
    "    def __init__(self, *args, **xargs):\n",
    "        super().__init__(*args, **xargs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        framename = self.data_path + self.names[index]\n",
    "        img = Image.open(framename).convert('RGB')\n",
    "        return self._image_transformer(img), 0, int(self.labels[index])\n",
    "\n",
    "\n",
    "class JigsawTestDatasetMultiple(JigsawDataset):\n",
    "    def __init__(self, *args, **xargs):\n",
    "        super().__init__(*args, **xargs)\n",
    "        self._image_transformer = transforms.Compose([\n",
    "            transforms.Resize(255, Image.BILINEAR),\n",
    "        ])\n",
    "        self._image_transformer_full = transforms.Compose([\n",
    "            transforms.Resize(225, Image.BILINEAR),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self._augment_tile = transforms.Compose([\n",
    "            transforms.Resize((75, 75), Image.BILINEAR),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        framename = self.data_path + self.names[index]\n",
    "        _img = Image.open(framename).convert('RGB')\n",
    "        img = self._image_transformer(_img)\n",
    "\n",
    "        w = float(img.size[0]) / self.grid_size\n",
    "        n_grids = self.grid_size ** 2\n",
    "        images = []\n",
    "        jig_labels = []\n",
    "        tiles = [None] * n_grids\n",
    "        for n in range(n_grids):\n",
    "            y = int(n / self.grid_size)\n",
    "            x = n % self.grid_size\n",
    "            tile = img.crop([x * w, y * w, (x + 1) * w, (y + 1) * w])\n",
    "            tile = self._augment_tile(tile)\n",
    "            tiles[n] = tile\n",
    "        for order in range(0, len(self.permutations)+1, 3):\n",
    "            if order==0:\n",
    "                data = tiles\n",
    "            else:\n",
    "                data = [tiles[self.permutations[order-1][t]] for t in range(n_grids)]\n",
    "            data = self.returnFunc(torch.stack(data, 0))\n",
    "            images.append(data)\n",
    "            jig_labels.append(order)\n",
    "        images = torch.stack(images, 0)\n",
    "        jig_labels = torch.LongTensor(jig_labels)\n",
    "        return images, jig_labels, int(self.labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4877e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat_dataset.py\n",
    "\n",
    "\n",
    "class ConcatDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset to concatenate multiple datasets.\n",
    "    Purpose: useful to assemble different existing datasets, possibly\n",
    "    large-scale datasets as the concatenation operation is done in an\n",
    "    on-the-fly manner.\n",
    "\n",
    "    Arguments:\n",
    "        datasets (sequence): List of datasets to be concatenated\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def cumsum(sequence):\n",
    "        r, s = [], 0\n",
    "        for e in sequence:\n",
    "            l = len(e)\n",
    "            r.append(l + s)\n",
    "            s += l\n",
    "        return r\n",
    "\n",
    "    def isMulti(self):\n",
    "        return isinstance(self.datasets[0], JigsawTestDatasetMultiple)\n",
    "\n",
    "    def __init__(self, datasets):\n",
    "        super(ConcatDataset, self).__init__()\n",
    "        assert len(datasets) > 0, 'datasets should not be an empty iterable'\n",
    "        self.datasets = list(datasets)\n",
    "        self.cumulative_sizes = self.cumsum(self.datasets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.cumulative_sizes[-1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)\n",
    "        if dataset_idx == 0:\n",
    "            sample_idx = idx\n",
    "        else:\n",
    "            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]\n",
    "        return self.datasets[dataset_idx][sample_idx], dataset_idx\n",
    "\n",
    "    @property\n",
    "    def cummulative_sizes(self):\n",
    "        warnings.warn(\"cummulative_sizes attribute is renamed to \"\n",
    "                      \"cumulative_sizes\", DeprecationWarning, stacklevel=2)\n",
    "        return self.cumulative_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e4ab900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_helper.py\n",
    "\n",
    "mnist = 'mnist'\n",
    "mnist_m = 'mnist_m'\n",
    "svhn = 'svhn'\n",
    "synth = 'synth'\n",
    "usps = 'usps'\n",
    "\n",
    "vlcs_datasets = [\"CALTECH\", \"LABELME\", \"PASCAL\", \"SUN\"]\n",
    "pacs_datasets = [\"art_painting\", \"cartoon\", \"photo\", \"sketch\"]\n",
    "office_datasets = [\"amazon\", \"dslr\", \"webcam\"]\n",
    "officehome_datasets = [\"art\", \"clipart\", \"real_world\", \"product\"]\n",
    "digits_datasets = [mnist, mnist, svhn, usps]\n",
    "available_datasets = office_datasets + pacs_datasets + vlcs_datasets + digits_datasets \n",
    "#office_paths = {dataset: \"/home/enoon/data/images/office/%s\" % dataset for dataset in office_datasets}\n",
    "#pacs_paths = {dataset: \"/home/enoon/data/images/PACS/kfold/%s\" % dataset for dataset in pacs_datasets}\n",
    "#vlcs_paths = {dataset: \"/home/goulmdata/images/VLCS/%s/test\" % dataset for dataset in vlcs_datasets}\n",
    "#paths = {**office_paths, **pacs_paths, **vlcs_paths}\n",
    "\n",
    "dataset_std = {mnist: (0.30280363, 0.30280363, 0.30280363),\n",
    "               mnist_m: (0.2384788, 0.22375608, 0.24496263),\n",
    "               svhn: (0.1951134, 0.19804622, 0.19481073),\n",
    "               synth: (0.29410212, 0.2939651, 0.29404707),\n",
    "               usps: (0.25887518, 0.25887518, 0.25887518),\n",
    "               }\n",
    "\n",
    "dataset_mean = {mnist: (0.13909429, 0.13909429, 0.13909429),\n",
    "                mnist_m: (0.45920207, 0.46326601, 0.41085603),\n",
    "                svhn: (0.43744073, 0.4437959, 0.4733686),\n",
    "                synth: (0.46332872, 0.46316052, 0.46327512),\n",
    "                usps: (0.17025368, 0.17025368, 0.17025368),\n",
    "                }\n",
    "\n",
    "\n",
    "class Subset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, limit):\n",
    "        indices = torch.randperm(len(dataset))[:limit]\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "\n",
    "def get_train_dataloader(args, patches):\n",
    "    dataset_list = args.source\n",
    "    assert isinstance(dataset_list, list)\n",
    "    datasets = []\n",
    "    val_datasets = []\n",
    "    img_transformer, tile_transformer = get_train_transformers(args)\n",
    "    limit = args.limit_source\n",
    "    for dname in dataset_list:\n",
    "        name_train, name_val, labels_train, labels_val = get_split_dataset_info(join(os.path.abspath(''), 'data/txt_lists', '%s_train.txt' % dname), args.val_size)\n",
    "        train_dataset = JigsawDataset(name_train, labels_train, patches=patches, img_transformer=img_transformer,\n",
    "                                      tile_transformer=tile_transformer, jig_classes=args.jigsaw_n_classes, bias_whole_image=args.bias_whole_image)\n",
    "        if limit:\n",
    "            train_dataset = Subset(train_dataset, limit)\n",
    "        datasets.append(train_dataset)\n",
    "        val_datasets.append(JigsawTestDataset(name_val, labels_val, img_transformer=get_val_transformer(args),\n",
    "                              patches=patches, jig_classes=args.jigsaw_n_classes))\n",
    "    dataset = ConcatDataset(datasets)\n",
    "    val_dataset = ConcatDataset(val_datasets)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True, drop_last=False)\n",
    "    return loader, val_loader\n",
    "\n",
    "\n",
    "def get_val_dataloader(args, patches=False):\n",
    "    names, labels = _dataset_info(join(os.path.abspath(''), 'data/txt_lists', '%s_test.txt' % args.target))\n",
    "    img_tr = get_val_transformer(args)\n",
    "    val_dataset = JigsawTestDataset(names, labels, patches=patches, img_transformer=img_tr, jig_classes=args.jigsaw_n_classes)\n",
    "    if args.limit_target and len(val_dataset) > args.limit_target:\n",
    "        val_dataset = Subset(val_dataset, args.limit_target)\n",
    "        print(\"Using %d subset of val dataset\" % args.limit_target)\n",
    "    dataset = ConcatDataset([val_dataset])\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True, drop_last=False)\n",
    "    return loader\n",
    "\n",
    "\n",
    "def get_jigsaw_val_dataloader(args, patches=False):\n",
    "    names, labels = _dataset_info(join(os.path.abspath(''), 'data/txt_lists', '%s_test.txt' % args.target))\n",
    "    img_tr = [transforms.Resize((args.image_size, args.image_size))]\n",
    "    tile_tr = [transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
    "    img_transformer = transforms.Compose(img_tr)\n",
    "    tile_transformer = transforms.Compose(tile_tr)\n",
    "    val_dataset = JigsawDataset(names, labels, patches=patches, img_transformer=img_transformer,\n",
    "                                      tile_transformer=tile_transformer, jig_classes=args.jigsaw_n_classes, bias_whole_image=args.bias_whole_image)\n",
    "    if args.limit_target and len(val_dataset) > args.limit_target:\n",
    "        val_dataset = Subset(val_dataset, args.limit_target)\n",
    "        print(\"Using %d subset of val dataset\" % args.limit_target)\n",
    "    dataset = ConcatDataset([val_dataset])\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True, drop_last=False)\n",
    "    return loader\n",
    "\n",
    "\n",
    "def get_train_transformers(args):\n",
    "    img_tr = [transforms.RandomResizedCrop((int(args.image_size), int(args.image_size)), (args.min_scale, args.max_scale))]\n",
    "    if args.random_horiz_flip > 0.0:\n",
    "        img_tr.append(transforms.RandomHorizontalFlip(args.random_horiz_flip))\n",
    "    if args.jitter > 0.0:\n",
    "        img_tr.append(transforms.ColorJitter(brightness=args.jitter, contrast=args.jitter, saturation=args.jitter, hue=min(0.5, args.jitter)))\n",
    "\n",
    "    tile_tr = []\n",
    "    if args.tile_random_grayscale:\n",
    "        tile_tr.append(transforms.RandomGrayscale(args.tile_random_grayscale))\n",
    "    tile_tr = tile_tr + [transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
    "\n",
    "    return transforms.Compose(img_tr), transforms.Compose(tile_tr)\n",
    "\n",
    "\n",
    "def get_val_transformer(args):\n",
    "    img_tr = [transforms.Resize((args.image_size, args.image_size)), transforms.ToTensor(),transforms.Normalize([0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
    "    return transforms.Compose(img_tr)\n",
    "\n",
    "\n",
    "def get_target_jigsaw_loader(args):\n",
    "    img_transformer, tile_transformer = get_train_transformers(args)\n",
    "    name_train, _, labels_train, _ = get_split_dataset_info(join(os.path.abspath(''), 'data/txt_lists', '%s_train.txt' % args.target), 0)\n",
    "    dataset = JigsawDataset(name_train, labels_train, patches=False, img_transformer=img_transformer,tile_transformer=tile_transformer, jig_classes=args.jigsaw_n_classes, bias_whole_image=args.bias_whole_image)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e04504a",
   "metadata": {},
   "source": [
    "## Fichier de /optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4999283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer_helper.py\n",
    "\n",
    "def get_optim_and_scheduler(network, epochs, lr, train_all, nesterov=False):\n",
    "    if train_all:\n",
    "        params = network.parameters()\n",
    "    else:\n",
    "        params = network.get_params(lr)\n",
    "    optimizer = optim.SGD(params, weight_decay=.0005, momentum=.9, nesterov=nesterov, lr=lr)\n",
    "    #optimizer = optim.Adam(params, lr=lr)\n",
    "    step_size = int(epochs * .8)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size)\n",
    "    print(\"Step size: %d\" % step_size)\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29b0b71",
   "metadata": {},
   "source": [
    "## Fichiers de /utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0b803f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_logger.py\n",
    "\n",
    "class TFLogger(object):\n",
    "    \n",
    "    def __init__(self, log_dir):\n",
    "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
    "        self.writer = tf.summary.create_file_writer(log_dir)\n",
    "        #self.writer = tf.summary.FileWriter(log_dir)\n",
    "\n",
    "    def scalar_summary(self, tag, value, step):\n",
    "        \"\"\"Log a scalar variable.\"\"\"\n",
    "        #summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
    "        #self.writer.add_summary(summary, step)\n",
    "        with self.writer.as_default():\n",
    "            tf.summary.scalar(tag, value, step=step)\n",
    "            self.writer.flush()\n",
    "            \n",
    "    def image_summary(self, tag, images, step):\n",
    "        \"\"\"Log a list of images.\"\"\"\n",
    "\n",
    "        img_summaries = []\n",
    "        for i, img in enumerate(images):\n",
    "            # Write the image to a string\n",
    "            try:\n",
    "                s = StringIO()\n",
    "            except:\n",
    "                s = BytesIO()\n",
    "            scipy.misc.toimage(img).save(s, format=\"png\")\n",
    "\n",
    "            # Create an Image object\n",
    "            img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n",
    "                                       height=img.shape[0],\n",
    "                                       width=img.shape[1])\n",
    "            # Create a Summary value\n",
    "            img_summaries.append(tf.Summary.Value(tag='%s/%d' % (tag, i), image=img_sum))\n",
    "\n",
    "        # Create and write Summary\n",
    "        summary = tf.Summary(value=img_summaries)\n",
    "        self.writer.add_summary(summary, step)\n",
    "        \n",
    "    def histo_summary(self, tag, values, step, bins=1000):\n",
    "        \"\"\"Log a histogram of the tensor of values.\"\"\"\n",
    "\n",
    "        # Create a histogram using numpy\n",
    "        counts, bin_edges = np.histogram(values, bins=bins)\n",
    "\n",
    "        # Fill the fields of the histogram proto\n",
    "        hist = tf.HistogramProto()\n",
    "        hist.min = float(np.min(values))\n",
    "        hist.max = float(np.max(values))\n",
    "        hist.num = int(np.prod(values.shape))\n",
    "        hist.sum = float(np.sum(values))\n",
    "        hist.sum_squares = float(np.sum(values**2))\n",
    "\n",
    "        # Drop the start of the first bin\n",
    "        bin_edges = bin_edges[1:]\n",
    "\n",
    "        # Add bin edges and counts\n",
    "        for edge in bin_edges:\n",
    "            hist.bucket_limit.append(edge)\n",
    "        for c in counts:\n",
    "            hist.bucket.append(c)\n",
    "\n",
    "        # Create and write Summary\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "        self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7049ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger.py\n",
    "\n",
    "\n",
    "_log_path = join(os.path.abspath(''), '../logs')\n",
    "\n",
    "\n",
    "# high level wrapper for tf_logger.TFLogger\n",
    "class Logger():\n",
    "    def __init__(self, args, update_frequency=10):\n",
    "        self.current_epoch = 0\n",
    "        self.max_epochs = args.epochs\n",
    "        self.last_update = time()\n",
    "        self.start_time = time()\n",
    "        self._clean_epoch_stats()\n",
    "        self.update_f = update_frequency\n",
    "        folder, logname = self.get_name_from_args(args)\n",
    "        log_path = join(_log_path, folder, logname)\n",
    "        if args.tf_logger:\n",
    "            self.tf_logger = TFLogger(log_path)\n",
    "            print(\"Saving to %s\" % log_path)\n",
    "        else:\n",
    "            self.tf_logger = None\n",
    "        self.current_iter = 0\n",
    "\n",
    "    def new_epoch(self, learning_rates):\n",
    "        self.current_epoch += 1\n",
    "        self.last_update = time()\n",
    "        self.lrs = learning_rates\n",
    "        print(\"New epoch - lr: %s\" % \", \".join([str(lr) for lr in self.lrs]))\n",
    "        self._clean_epoch_stats()\n",
    "        if self.tf_logger:\n",
    "            for n, v in enumerate(self.lrs):\n",
    "                self.tf_logger.scalar_summary(\"aux/lr%d\" % n, v, self.current_iter)\n",
    "\n",
    "    def log(self, it, iters, losses, samples_right, total_samples):\n",
    "        self.current_iter += 1\n",
    "        loss_string = \", \".join([\"%s : %.3f\" % (k, v) for k, v in losses.items()])\n",
    "        for k, v in samples_right.items():\n",
    "            past = self.epoch_stats.get(k, 0.0)\n",
    "            self.epoch_stats[k] = past + v\n",
    "        self.total += total_samples\n",
    "        acc_string = \", \".join([\"%s : %.2f\" % (k, 100 * (v / total_samples)) for k, v in samples_right.items()])\n",
    "        if it % self.update_f == 0:\n",
    "            print(\"%d/%d of epoch %d/%d %s - acc %s [bs:%d]\" % (it, iters, self.current_epoch, self.max_epochs, loss_string,\n",
    "                                                                acc_string, total_samples))\n",
    "            # update tf log\n",
    "            if self.tf_logger:\n",
    "                for k, v in losses.items(): self.tf_logger.scalar_summary(\"train/loss_%s\" % k, v, self.current_iter)\n",
    "\n",
    "    def _clean_epoch_stats(self):\n",
    "        self.epoch_stats = {}\n",
    "        self.total = 0\n",
    "\n",
    "    def log_test(self, phase, accuracies):\n",
    "        print(\"Accuracies on %s: \" % phase + \", \".join([\"%s : %.2f\" % (k, v * 100) for k, v in accuracies.items()]))\n",
    "        if self.tf_logger:\n",
    "            for k, v in accuracies.items(): self.tf_logger.scalar_summary(\"%s/acc_%s\" % (phase, k), v, self.current_iter)\n",
    "\n",
    "    def save_best(self, val_test, best_test):\n",
    "        print(\"It took %g\" % (time() - self.start_time))\n",
    "        if self.tf_logger:\n",
    "            for x in range(10):\n",
    "                self.tf_logger.scalar_summary(\"best/from_val_test\", val_test, x)\n",
    "                self.tf_logger.scalar_summary(\"best/max_test\", best_test, x)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_name_from_args(args):\n",
    "        folder_name = \"%s_to_%s\" % (\"-\".join(sorted(args.source)), args.target)\n",
    "        if args.folder_name:\n",
    "            folder_name = join(args.folder_name, folder_name)\n",
    "        name = \"eps%d_bs%d_lr%g_class%d_jigClass%d_jigWeight%g\" % (args.epochs, args.batch_size, args.learning_rate, args.n_classes,\n",
    "                                                                   args.jigsaw_n_classes, args.jig_weight)\n",
    "        # if args.ooo_weight > 0:\n",
    "        #     name += \"_oooW%g\" % args.ooo_weight\n",
    "        if args.train_all:\n",
    "            name += \"_TAll\"\n",
    "        if args.bias_whole_image:\n",
    "            name += \"_bias%g\" % args.bias_whole_image\n",
    "        if args.classify_only_sane:\n",
    "            name += \"_classifyOnlySane\"\n",
    "        if args.TTA:\n",
    "            name += \"_TTA\"\n",
    "        try:\n",
    "            name += \"_entropy%g_jig_tW%g\" % (args.entropy_weight, args.target_weight)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        if args.suffix:\n",
    "            name += \"_%s\" % args.suffix\n",
    "        name += \"_%d\" % int(time() % 1000)\n",
    "        return folder_name, name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c630175",
   "metadata": {},
   "source": [
    "## Fichier principal train_jigsaw.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1303518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy_loss(x):\n",
    "    return torch.sum(-F.softmax(x, 1) * F.log_softmax(x, 1), 1).mean()\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, args, device, supervised):\n",
    "        self.args = args\n",
    "        self.supervised = supervised\n",
    "        self.device = device\n",
    "        model = get_network(args.network)(jigsaw_classes=args.jigsaw_n_classes + 1, classes=args.n_classes)\n",
    "        self.model = model.to(device)\n",
    "        # print(self.model)\n",
    "        self.source_loader, self.val_loader = get_train_dataloader(args, patches=model.is_patch_based())\n",
    "        self.target_loader = get_val_dataloader(args, patches=model.is_patch_based())\n",
    "        self.test_loaders = {\"val\": self.val_loader, \"test\": self.target_loader}\n",
    "        self.len_dataloader = len(self.source_loader)\n",
    "        print(\"Dataset size: train %d, val %d, test %d\" % (len(self.source_loader.dataset), len(self.val_loader.dataset), len(self.target_loader.dataset)))\n",
    "        self.optimizer, self.scheduler = get_optim_and_scheduler(model, args.epochs, args.learning_rate, args.train_all, nesterov=args.nesterov)\n",
    "        self.jig_weight = args.jig_weight\n",
    "        self.only_non_scrambled = args.classify_only_sane\n",
    "        self.n_classes = args.n_classes\n",
    "        if args.target in args.source:\n",
    "            self.target_id = args.source.index(args.target)\n",
    "            print(\"Target in source: %d\" % self.target_id)\n",
    "            print(args.source)\n",
    "        else:\n",
    "            self.target_id = None\n",
    "\n",
    "    def _do_epoch(self):\n",
    "        criterion_unsupervised = entropy_loss\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        self.model.train()\n",
    "        for it, ((data, jig_l, class_l), d_idx) in enumerate(self.source_loader):\n",
    "            data, jig_l, class_l, d_idx = data.to(self.device), jig_l.to(self.device), class_l.to(self.device), d_idx.to(self.device)\n",
    "            # absolute_iter_count = it + self.current_epoch * self.len_dataloader\n",
    "            # p = float(absolute_iter_count) / self.args.epochs / self.len_dataloader\n",
    "            # lambda_val = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "            # if domain_error > 2.0:\n",
    "            #     lambda_val  = 0\n",
    "            # print(\"Shutting down LAMBDA to prevent implosion\")\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            jigsaw_logit, class_logit = self.model(data)  # , lambda_val=lambda_val)\n",
    "            jigsaw_loss = criterion(jigsaw_logit, jig_l)\n",
    "            # domain_loss = criterion(domain_logit, d_idx)\n",
    "            # domain_error = domain_loss.item()\n",
    "            if self.supervised == False:\n",
    "                if self.only_non_scrambled:\n",
    "                    if self.target_id is not None:\n",
    "                        idx = (jig_l == 0) & (d_idx != self.target_id)\n",
    "                        class_loss = criterion_unsupervised(class_logit[idx])\n",
    "                    else:\n",
    "                        class_loss = criterion_unsupervised(class_logit[jig_l == 0])\n",
    "\n",
    "                elif self.target_id:\n",
    "                    class_loss = criterion_unsupervised(class_logit[d_idx != self.target_id])\n",
    "                else:\n",
    "                    class_loss = criterion_unsupervised(class_logit)\n",
    "            else :\n",
    "                if self.only_non_scrambled:\n",
    "                    if self.target_id is not None:\n",
    "                        idx = (jig_l == 0) & (d_idx != self.target_id)\n",
    "                        class_loss = criterion(class_logit[idx], class_l[idx])\n",
    "                    else:\n",
    "                        class_loss = criterion(class_logit[jig_l == 0], class_l[jig_l == 0])\n",
    "\n",
    "                elif self.target_id:\n",
    "                    class_loss = criterion(class_logit[d_idx != self.target_id], class_l[d_idx != self.target_id])\n",
    "                else:\n",
    "                    class_loss = criterion(class_logit, class_l)\n",
    "                    \n",
    "            _, cls_pred = class_logit.max(dim=1)\n",
    "            _, jig_pred = jigsaw_logit.max(dim=1)\n",
    "            # _, domain_pred = domain_logit.max(dim=1)\n",
    "            loss = class_loss + jigsaw_loss * self.jig_weight  # + 0.1 * domain_loss\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.logger.log(it, len(self.source_loader),\n",
    "                            {\"jigsaw\": jigsaw_loss.item(), \"class\": class_loss.item()  # , \"domain\": domain_loss.item()\n",
    "                             },\n",
    "                            # ,\"lambda\": lambda_val},\n",
    "                            {\"jigsaw\": torch.sum(jig_pred == jig_l.data).item(),\n",
    "                             \"class\": torch.sum(cls_pred == class_l.data).item(),\n",
    "                             # \"domain\": torch.sum(domain_pred == d_idx.data).item()\n",
    "                             },\n",
    "                            data.shape[0])\n",
    "            del loss, class_loss, jigsaw_loss, jigsaw_logit, class_logit\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for phase, loader in self.test_loaders.items():\n",
    "                total = len(loader.dataset)\n",
    "                if loader.dataset.isMulti():\n",
    "                    jigsaw_correct, class_correct, single_acc = self.do_test_multi(loader)\n",
    "                    print(\"Single vs multi: %g %g\" % (float(single_acc) / total, float(class_correct) / total))\n",
    "                else:\n",
    "                    jigsaw_correct, class_correct = self.do_test(loader)\n",
    "                jigsaw_acc = float(jigsaw_correct) / total\n",
    "                class_acc = float(class_correct) / total\n",
    "                self.logger.log_test(phase, {\"jigsaw\": jigsaw_acc, \"class\": class_acc})\n",
    "                self.results[phase][self.current_epoch] = class_acc\n",
    "\n",
    "    def do_test(self, loader):\n",
    "        jigsaw_correct = 0\n",
    "        class_correct = 0\n",
    "        domain_correct = 0\n",
    "        for it, ((data, jig_l, class_l), _) in enumerate(loader):\n",
    "            data, jig_l, class_l = data.to(self.device), jig_l.to(self.device), class_l.to(self.device)\n",
    "            jigsaw_logit, class_logit = self.model(data)\n",
    "            _, cls_pred = class_logit.max(dim=1)\n",
    "            _, jig_pred = jigsaw_logit.max(dim=1)\n",
    "            class_correct += torch.sum(cls_pred == class_l.data)\n",
    "            jigsaw_correct += torch.sum(jig_pred == jig_l.data)\n",
    "        return jigsaw_correct, class_correct\n",
    "\n",
    "    def do_test_multi(self, loader):\n",
    "        jigsaw_correct = 0\n",
    "        class_correct = 0\n",
    "        single_correct = 0\n",
    "        for it, ((data, jig_l, class_l), d_idx) in enumerate(loader):\n",
    "            data, jig_l, class_l = data.to(self.device), jig_l.to(self.device), class_l.to(self.device)\n",
    "            n_permutations = data.shape[1]\n",
    "            class_logits = torch.zeros(n_permutations, data.shape[0], self.n_classes).to(self.device)\n",
    "            for k in range(n_permutations):\n",
    "                class_logits[k] = F.softmax(self.model(data[:, k])[1], dim=1)\n",
    "            class_logits[0] *= 4 * n_permutations  # bias more the original image\n",
    "            class_logit = class_logits.mean(0)\n",
    "            _, cls_pred = class_logit.max(dim=1)\n",
    "            jigsaw_logit, single_logit = self.model(data[:, 0])\n",
    "            _, jig_pred = jigsaw_logit.max(dim=1)\n",
    "            _, single_logit = single_logit.max(dim=1)\n",
    "            single_correct += torch.sum(single_logit == class_l.data)\n",
    "            class_correct += torch.sum(cls_pred == class_l.data)\n",
    "            jigsaw_correct += torch.sum(jig_pred == jig_l.data[:, 0])\n",
    "        return jigsaw_correct, class_correct, single_correct\n",
    "\n",
    "    def do_training(self):\n",
    "        self.logger = Logger(self.args, update_frequency=30)  # , \"domain\", \"lambda\"\n",
    "        self.results = {\"val\": torch.zeros(self.args.epochs), \"test\": torch.zeros(self.args.epochs)}\n",
    "        for self.current_epoch in range(self.args.epochs):\n",
    "            self.scheduler.step()\n",
    "            self.logger.new_epoch(self.scheduler.get_lr())\n",
    "            self._do_epoch()\n",
    "        val_res = self.results[\"val\"]\n",
    "        test_res = self.results[\"test\"]\n",
    "        idx_best = val_res.argmax()\n",
    "        #print(\"Best val %g, corresponding test %g - best test: %g\" % (val_res.max(), test_res[idx_best], test_res.max()))\n",
    "        self.logger.save_best(test_res[idx_best], test_res.max())\n",
    "        return self.logger, self.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9700c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commentaires sur le choix du réseau :\n",
    "# resnet18 et resnet50 fonctionnent pour image_size=222\n",
    "\n",
    "\n",
    "class Args:\n",
    "    \n",
    "    def __init__(self, source, target):\n",
    "        \n",
    "        \n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        \n",
    "    batch_size = 64\n",
    "    image_size = 222              # 222 \n",
    "    \n",
    "    min_scale = 0.8               # Minimum scale percent\n",
    "    max_scale = 1.0               # Maximum scale percent\n",
    "    random_horiz_flip = 0.0       # Chance of random horizontal flip\n",
    "    jitter = 0.0                  # Color jitter amount\n",
    "    tile_random_grayscale = 0.1   # Chance of randomly greyscaling a tile\n",
    "    \n",
    "    limit_source = None     # If set, it will limit the number of training samples\n",
    "    limit_target = None     # If set, it will limit the number of testing samples\n",
    "    \n",
    "    learning_rate = 0.01\n",
    "    epochs = 5\n",
    "    n_classes = 7              # Number of classes for object prediction\n",
    "    jigsaw_n_classes = 31       # Number of permutation classes for the puzzle\n",
    "    network = \"resnet50\"        # To choose from : 'resnet18', 'resnet50'\n",
    "    jig_weight = 0.7           # Weight for the jigsaw puzzle compared to the classification\n",
    "    ooo_weight = 0              # Weight for odd one out task\n",
    "    tf_logger = True            # If True will save tensorboard compatible logs\n",
    "    val_size = 0.1              # Validation size (between 0 and 1)\n",
    "    folder_name = \"Test\"        # Used by the logger to save logs\n",
    "    bias_whole_image = 0.9      # If set, will bias the training procedure to show more often the whole image\n",
    "    TTA = False                 # Activate test time data augmentation\n",
    "    classify_only_sane = False  # If true, the network will only try to classify the non scrambled images\n",
    "    train_all = True            # If true, all network weights will be trained\n",
    "    suffix = \"\"                 # Suffix for the logger\n",
    "    nesterov = False            # Use nesterov\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e255bca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args, supervision=True):\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    trainer = Trainer(args, device, supervision)\n",
    "    trainer.do_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9ff13a",
   "metadata": {},
   "source": [
    "## Plan d'éxpérience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecd94ac",
   "metadata": {},
   "source": [
    "### Test 3 contre 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a29672",
   "metadata": {},
   "source": [
    "Dans cette partie nous allons entrainer le réseau sur 3 domaines puis le tester sur le domaine restant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "18e4d1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Domain = ['art_painting', 'cartoon', 'sketch', 'photo']\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "for i in range(4):\n",
    "    train.append(Domain[:i] + Domain[i+1:])\n",
    "    test.append(Domain[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ff5aecba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    s = train[i]\n",
    "    t = test[i]\n",
    "    print(\"train : \", s)\n",
    "    print(\"test : \", t)\n",
    "    args = Args(s, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc0d560",
   "metadata": {},
   "outputs": [],
   "source": [
    "main(Args(train[0], test[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d2e992",
   "metadata": {},
   "source": [
    "En faisant tourner le code ci-dessus on obtient les résultats suivants :\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "    Train & Test & Précision \\\\ \\hline\n",
    "    \\text{cartoon, sketch, photo} & \\text{art painting} & 80.27\\\\ \\hline\n",
    "    \\text{art painting, sketch, photo} & \\text{cartoon} & 78.24\\\\ \\hline\n",
    "    \\text{art painting, cartoon, photo} & \\text{sketch} & 72.41\\\\ \\hline\n",
    "    \\text{art painting, cartoon, sketch} & \\text{photo} & 94.07\\\\ \\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9edafb",
   "metadata": {},
   "source": [
    "On remarque dans les résultats que plus un domaine est abstrait comme le cartoon ou le sketch, plus il est difficile de généraliser pour le réseau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c74db9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ResNet-50\n",
      "Dataset size: train 2110, val 234, test 3929\n",
      "Step size: 4\n",
      "Saving to /home/goulm/Bureau/JigenDG-master/../logs/Test/cartoon_to_sketch/eps5_bs64_lr0.01_class7_jigClass31_jigWeight0.33_TAll_bias0.9_47\n",
      "New epoch - lr: 0.01\n",
      "0/32 of epoch 1/5 jigsaw : 3.942, class : 1.876 - acc jigsaw : 0.00, class : 17.19 [bs:64]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m main(Args([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcartoon\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msketch\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m      4\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(args, device, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 6\u001b[0m trainer\u001b[38;5;241m.\u001b[39mdo_training()\n",
      "Cell \u001b[0;32mIn[13], line 145\u001b[0m, in \u001b[0;36mTrainer.do_training\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mnew_epoch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mget_lr())\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_epoch()\n\u001b[1;32m    146\u001b[0m val_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    147\u001b[0m test_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[13], line 77\u001b[0m, in \u001b[0;36mTrainer._do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# _, domain_pred = domain_logit.max(dim=1)\u001b[39;00m\n\u001b[1;32m     75\u001b[0m loss \u001b[38;5;241m=\u001b[39m class_loss \u001b[38;5;241m+\u001b[39m jigsaw_loss \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjig_weight  \u001b[38;5;66;03m# + 0.1 * domain_loss\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog(it, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_loader),\n\u001b[1;32m     81\u001b[0m                 {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjigsaw\u001b[39m\u001b[38;5;124m\"\u001b[39m: jigsaw_loss\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m: class_loss\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# , \"domain\": domain_loss.item()\u001b[39;00m\n\u001b[1;32m     82\u001b[0m                  },\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     87\u001b[0m                  },\n\u001b[1;32m     88\u001b[0m                 data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(Args(['cartoon'], 'sketch'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f54af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "main(Args(['photo'], 'sketch'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab16426f",
   "metadata": {},
   "source": [
    "### Test rapport puzzle-classif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ea1e31",
   "metadata": {},
   "source": [
    "La loss utilisé par le réseau est la somme de deux loss pondéré par un poids $\\beta$ selon la formule suivante :\n",
    "\n",
    "$$\n",
    "TotalLoss = ClassifLoss + \\beta*PuzzleLoss\n",
    "$$\n",
    "\n",
    "Où PuzzleLoss est la loss liée au puzle et ClassifLoss est la loss lié à la classfication. Dans cette partie nous allons essayer différentes valeurs pour le paramètre $\\beta$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3e1fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [0, 0.05, 0.1, 0.2, 0.5, 1, 2, 3]\n",
    "\n",
    "s = train[0]\n",
    "t = test[0]\n",
    "\n",
    "\n",
    "for w in weights:\n",
    "\n",
    "    args = Args(s, t)\n",
    "    args.jig_weight =  w  # 1 contre 0.7 de base le jigsaw est donc un peu moins important\n",
    "    print(\"weight of jigsaw : \", w)\n",
    "    main(args, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49f6733",
   "metadata": {},
   "source": [
    "En faisant tourner le code ci-dessus on obtient les résultats suivants :\n",
    "\n",
    "et en prenant :\n",
    "\n",
    "train : cartoon, sketch, photo\n",
    "\n",
    "test : art\\_painting\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|}\n",
    "\\hline\n",
    "    \\text{Valeur du poids} & \\text{Précision} \\\\ \\hline\n",
    "    0 & 80.81 \\\\ \\hline\n",
    "    0.05 & 80.81\\\\ \\hline\n",
    "    0.1 & 76.86\\\\ \\hline\n",
    "    0.2 & 81.35\\\\ \\hline\n",
    "    0.5 & 80.08\\\\ \\hline\n",
    "    0.7 & 80.27\\\\ \\hline\n",
    "    1 & 78.066\\\\ \\hline\n",
    "    2 & 78.42\\\\ \\hline\n",
    "    3 & 32.57\\\\ \\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99be6b61",
   "metadata": {},
   "source": [
    "Suite à cette expérience on peut tout d'abord remarquer que si la perte de la partie puzzle devient trop importante par rapport à celle la partie classification alors la qualité de la classification chute très rapidemment. Par contre, la précision sur le puzzle reste à 100% dans tous les cas.\n",
    "On peut aussi remarquer que la perte du puzzle ne semble pas avoir un grand impact sur la précision du réseau. En effet, pour des $\\beta$ entre 0 et 1 on obtient sensiblement les même résultats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e81348",
   "metadata": {},
   "source": [
    "### Test nombre de permutation du puzzle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea42c0b4",
   "metadata": {},
   "source": [
    "La première étape de l'algorithme est de transformer une image en puzzle. Lors de cette étape tous les puzzles ne sont pas possibles, les permutations disponibles sont définis une première fois avant l'algorithme. La résolution du puzzle devient un problème de classification avec un nombre raisonnable de classe. On peut alors se demander commment le nombre de permutation disponible influe sur la précision de la classification et de la résolution de puzzle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faa2d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test nombre de permutation possible\n",
    "\n",
    "s = train[2]\n",
    "t = test[2]\n",
    "\n",
    "nb_permutations = [31, 100, 1000] # avant on était à 31 \n",
    "\n",
    "for n in nb_permutations:\n",
    "\n",
    "    args = Args(s, t)\n",
    "    args.jigsaw_n_classes =  n\n",
    "    print(\"Nombres de permutations possibles : \", n)\n",
    "    main(args, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bcf2b95",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{array}{|c|c|}\n",
    "\\hline\n",
    "    \\text{Nombre de Permutation} & \\text{Précision} \\\\ \\hline\n",
    "    31 & 75.49   \\\\ \\hline\n",
    "    100 & 70.63  \\\\ \\hline\n",
    "    1000 & 67.75\\\\ \\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\texcolor{red}{Ajouter précision jigsaw si intéréssant ou changer phrase au dessus}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbfa0d4",
   "metadata": {},
   "source": [
    "### Chance of random Gray tile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4f720a",
   "metadata": {},
   "source": [
    "Lors de la transformation en puzzle il y a une certaine probabilité que l'une des cases du puzzle soit grisés, nous allons faire évoluer ce paramètre pour voir son influence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55cf2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tile_random_grayscales = [0, 0.1, 0.2, 0.3, 0.5, 0.6, 0.7, 0.8]\n",
    "\n",
    "for p in tile_random_grayscales:\n",
    "\n",
    "    args = Args(s, t)\n",
    "    args.tile_random_grayscale =  p\n",
    "    print(\"Gray, probability : \", p)\n",
    "    main(args, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef23d92a",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|}\n",
    "\\hline\n",
    "    \\text{Probabilité d'une case grise} & \\text{Précision train} & \\text{Précision test}\\\\ \\hline\n",
    "    0 & 97.19 & 69.08  \\\\ \\hline\n",
    "    0.1 & 95.87 & 72.31 \\\\ \\hline\n",
    "    0.2 & 97.19 & 76.64 \\\\ \\hline\n",
    "    0.3 & 96.86 & 71.88 \\\\ \\hline\n",
    "    0.5 & 96.03 & 77.81 \\\\ \\hline\n",
    "    0.6 & 95.87 & 72.18 \\\\ \\hline\n",
    "    0.7 & 94.55 & 76.36 \\\\ \\hline\n",
    "    0.8 & 96.86 & 78.24 \\\\ \\hline\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6d11c5",
   "metadata": {},
   "source": [
    "$\\textcolor$ Peut etre enlever Precision train\n",
    "\n",
    "On peut voir que lorsque la probabilité de griser une case augmente on a une amélioration de la précision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85c5b54",
   "metadata": {},
   "source": [
    "### bias_whole_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e9a315",
   "metadata": {},
   "source": [
    "Le réseau n'apprend pas que sur des puzzles, une partie des images n'est pas permuté. La part d'image non permuté dépend d'un paramètre appelé bias_whole_image que nous allons essayer de faire varier. Lorsque celui-ci vaut on ne prend jamais l'image tel quel, on la permute dans tous les cas et si il vaut on ne permute plus aucune image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6406b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_whole_image = [0, 0.3, 0.5, 0.7, 0.9]\n",
    "\n",
    "for p in bias_whole_image:\n",
    "\n",
    "    args = Args(s, t)\n",
    "    args.bias_whole_image =  p\n",
    "    print(\" Probability of whole image : \", p)\n",
    "    main(args, True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e46a2ff",
   "metadata": {},
   "source": [
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|}\n",
    "\\hline\n",
    "    \\text{Biais en faveur de l'image complète} & \\text{Précision} \\\\ \\hline\n",
    "    0 & 66.68   \\\\ \\hline\n",
    "    0.3 & 63.88 \\\\ \\hline\n",
    "    0.5 & 72.36 \\\\ \\hline\n",
    "    0.7 & 73.63 \\\\ \\hline\n",
    "    0.9 & 72.56 \\\\ \\hline\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12500e51",
   "metadata": {},
   "source": [
    "D'aprés les résultats obtenues on peut conclure que pour avoir une précision maximale il est intéressant qu'un partie des images ne soit pas permuté, en effet, lorsque l'on prend un biais à 0 (toutes les images permutés) on obtient une précision plus faible.\n",
    "D'un autre cotés les permutations ne semblent pas appporter grand chose en termes de précision, quand le biais augmente et que l'on prend de moins d'image permuté la précision reste stable. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b50dbd",
   "metadata": {},
   "source": [
    "## Apprentissage non supervisé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9852d894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ResNet-50\n",
      "Dataset size: train 7150, val 793, test 2048\n",
      "Step size: 4\n",
      "Saving to /home/tiphaign/Documents/5A/HDDL/RENDU_PROJET/../logs/Test/cartoon-photo-sketch_to_art_painting/eps5_bs64_lr0.01_class7_jigClass31_jigWeight0.33_TAll_bias0.9_465\n",
      "New epoch - lr: 0.01\n",
      "0/111 of epoch 1/5 jigsaw : 3.550, class : 1.879 - acc jigsaw : 1.56, class : 12.50 [bs:64]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m main(Args(train[\u001b[38;5;241m0\u001b[39m], test[\u001b[38;5;241m0\u001b[39m]), supervision \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args, supervision)\u001b[0m\n\u001b[1;32m      4\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(args, device, supervision)\n\u001b[0;32m----> 6\u001b[0m trainer\u001b[38;5;241m.\u001b[39mdo_training()\n",
      "Cell \u001b[0;32mIn[13], line 145\u001b[0m, in \u001b[0;36mTrainer.do_training\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mnew_epoch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mget_lr())\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_epoch()\n\u001b[1;32m    146\u001b[0m val_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    147\u001b[0m test_res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[13], line 81\u001b[0m, in \u001b[0;36mTrainer._do_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     77\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mlog(it, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_loader),\n\u001b[0;32m---> 81\u001b[0m                     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjigsaw\u001b[39m\u001b[38;5;124m\"\u001b[39m: jigsaw_loss\u001b[38;5;241m.\u001b[39mitem(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m: class_loss\u001b[38;5;241m.\u001b[39mitem()  \u001b[38;5;66;03m# , \"domain\": domain_loss.item()\u001b[39;00m\n\u001b[1;32m     82\u001b[0m                      },\n\u001b[1;32m     83\u001b[0m                     \u001b[38;5;66;03m# ,\"lambda\": lambda_val},\u001b[39;00m\n\u001b[1;32m     84\u001b[0m                     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjigsaw\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39msum(jig_pred \u001b[38;5;241m==\u001b[39m jig_l\u001b[38;5;241m.\u001b[39mdata)\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m     85\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m: torch\u001b[38;5;241m.\u001b[39msum(cls_pred \u001b[38;5;241m==\u001b[39m class_l\u001b[38;5;241m.\u001b[39mdata)\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m     86\u001b[0m                      \u001b[38;5;66;03m# \"domain\": torch.sum(domain_pred == d_idx.data).item()\u001b[39;00m\n\u001b[1;32m     87\u001b[0m                      },\n\u001b[1;32m     88\u001b[0m                     data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m loss, class_loss, jigsaw_loss, jigsaw_logit, class_logit\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main(Args(train[0], test[0]), supervision = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc44f4db",
   "metadata": {},
   "source": [
    "# Base de données Office Home"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248c1240",
   "metadata": {},
   "source": [
    "Pour télécharger : https://www.hemanthdv.org/officeHomeDataset.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e3e102",
   "metadata": {},
   "source": [
    "## Présentation de la base de données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d82827",
   "metadata": {},
   "source": [
    "Nous allons maintenant tester notre réseau sur un autre jeu de données, le dataset OfficeHome. Il comporte en tout 15588 images, de quatres domaines différents :\n",
    "\n",
    "- *Art* : des oeuvres d'art en peinture ou en dessin,\n",
    "- *Clipart* : des images de clip art,\n",
    "- *Product* : des images sans arrière-plan,\n",
    "- *Real-World* : des photographies réelles.\n",
    "\n",
    "De plus, le dataset comporte soixante-cinq catégories correspondant à des objets qui se trouvent généralement dans les maisons et les bureaux (d'où le nom du jeu de données). \\\n",
    "Le tableau ci-dessous référence la proportion d'images dans chacun des quatres domaines.\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "    \\text{Domaine} & \\text{Nb Images} & \\text{Proportion} \\\\ \\hline\n",
    "    \\text{Art} & 2427 & 0.156 \\\\ \\hline\n",
    "    \\text{Clipart} & 4365 & 0.280 \\\\ \\hline\n",
    "    \\text{Real World} & 4357 & 0.279 \\\\ \\hline\n",
    "    \\text{Product} & 4439 & 0.285 \\\\ \\hline\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad357cf",
   "metadata": {},
   "source": [
    "Des exemples issus du dataset Office Home sont présentés ci-dessous, pour chaque domaine et pour sept des 65 catégories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a219e62",
   "metadata": {},
   "source": [
    "![images exemple2](exemples2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cd0c4e",
   "metadata": {},
   "source": [
    "## Plan d'éxpérience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "27f8bf2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3 contre 1\n",
    "\n",
    "Domain = ['art', 'clipart', 'real_world', 'product']\n",
    "\n",
    "train = []\n",
    "test = []\n",
    "for i in range(4):\n",
    "    train.append(Domain[:i] + Domain[i+1:])\n",
    "    test.append(Domain[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eab7b72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ResNet-50\n",
      "Dataset size: train 11847, val 1314, test 2427\n",
      "Step size: 4\n",
      "Saving to /home/tiphaign/Documents/5A/HDDL/Projet_HDDL/JigenDG-master/../logs/Test/clipart-product-real_world_to_art/eps5_bs64_lr0.01_class66_jigClass31_jigWeight0.33_TAll_bias0.9_369\n",
      "New epoch - lr: 0.01\n",
      "0/185 of epoch 1/5 jigsaw : 3.690, class : 4.170 - acc jigsaw : 0.00, class : 0.00 [bs:64]\n",
      "30/185 of epoch 1/5 jigsaw : 0.308, class : 1.908 - acc jigsaw : 92.19, class : 54.69 [bs:64]\n",
      "60/185 of epoch 1/5 jigsaw : 0.468, class : 1.385 - acc jigsaw : 90.62, class : 62.50 [bs:64]\n",
      "90/185 of epoch 1/5 jigsaw : 0.124, class : 1.363 - acc jigsaw : 98.44, class : 60.94 [bs:64]\n",
      "120/185 of epoch 1/5 jigsaw : 0.781, class : 1.236 - acc jigsaw : 82.81, class : 68.75 [bs:64]\n",
      "150/185 of epoch 1/5 jigsaw : 0.577, class : 1.169 - acc jigsaw : 85.94, class : 65.62 [bs:64]\n",
      "180/185 of epoch 1/5 jigsaw : 0.442, class : 0.912 - acc jigsaw : 90.62, class : 70.31 [bs:64]\n",
      "Accuracies on val: jigsaw : 100.00, class : 74.35\n",
      "Accuracies on test: jigsaw : 100.00, class : 50.19\n",
      "New epoch - lr: 0.01\n",
      "0/185 of epoch 2/5 jigsaw : 0.524, class : 0.691 - acc jigsaw : 87.50, class : 81.25 [bs:64]\n",
      "30/185 of epoch 2/5 jigsaw : 0.452, class : 0.612 - acc jigsaw : 90.62, class : 84.38 [bs:64]\n",
      "60/185 of epoch 2/5 jigsaw : 0.317, class : 0.595 - acc jigsaw : 92.19, class : 87.50 [bs:64]\n",
      "90/185 of epoch 2/5 jigsaw : 0.181, class : 0.611 - acc jigsaw : 96.88, class : 81.25 [bs:64]\n",
      "120/185 of epoch 2/5 jigsaw : 0.482, class : 0.715 - acc jigsaw : 87.50, class : 81.25 [bs:64]\n",
      "150/185 of epoch 2/5 jigsaw : 0.437, class : 0.878 - acc jigsaw : 89.06, class : 75.00 [bs:64]\n",
      "180/185 of epoch 2/5 jigsaw : 0.551, class : 0.807 - acc jigsaw : 85.94, class : 81.25 [bs:64]\n",
      "Accuracies on val: jigsaw : 100.00, class : 76.48\n",
      "Accuracies on test: jigsaw : 99.84, class : 50.14\n",
      "New epoch - lr: 0.01\n",
      "0/185 of epoch 3/5 jigsaw : 0.410, class : 0.415 - acc jigsaw : 90.62, class : 90.62 [bs:64]\n",
      "30/185 of epoch 3/5 jigsaw : 0.297, class : 0.797 - acc jigsaw : 92.19, class : 82.81 [bs:64]\n",
      "60/185 of epoch 3/5 jigsaw : 0.163, class : 0.217 - acc jigsaw : 96.88, class : 92.19 [bs:64]\n",
      "90/185 of epoch 3/5 jigsaw : 0.610, class : 0.712 - acc jigsaw : 84.38, class : 76.56 [bs:64]\n",
      "120/185 of epoch 3/5 jigsaw : 0.539, class : 0.563 - acc jigsaw : 85.94, class : 81.25 [bs:64]\n",
      "150/185 of epoch 3/5 jigsaw : 0.399, class : 0.328 - acc jigsaw : 90.62, class : 90.62 [bs:64]\n",
      "180/185 of epoch 3/5 jigsaw : 0.391, class : 0.528 - acc jigsaw : 90.62, class : 85.94 [bs:64]\n",
      "Accuracies on val: jigsaw : 100.00, class : 77.32\n",
      "Accuracies on test: jigsaw : 99.88, class : 46.39\n",
      "New epoch - lr: 0.0001\n",
      "0/185 of epoch 4/5 jigsaw : 0.591, class : 0.325 - acc jigsaw : 84.38, class : 93.75 [bs:64]\n",
      "30/185 of epoch 4/5 jigsaw : 0.411, class : 0.375 - acc jigsaw : 89.06, class : 87.50 [bs:64]\n",
      "60/185 of epoch 4/5 jigsaw : 0.251, class : 0.083 - acc jigsaw : 93.75, class : 100.00 [bs:64]\n",
      "90/185 of epoch 4/5 jigsaw : 0.419, class : 0.382 - acc jigsaw : 89.06, class : 90.62 [bs:64]\n",
      "120/185 of epoch 4/5 jigsaw : 0.131, class : 0.124 - acc jigsaw : 96.88, class : 98.44 [bs:64]\n",
      "150/185 of epoch 4/5 jigsaw : 0.385, class : 0.156 - acc jigsaw : 89.06, class : 96.88 [bs:64]\n",
      "180/185 of epoch 4/5 jigsaw : 0.486, class : 0.307 - acc jigsaw : 85.94, class : 87.50 [bs:64]\n",
      "Accuracies on val: jigsaw : 100.00, class : 84.86\n",
      "Accuracies on test: jigsaw : 99.96, class : 59.95\n",
      "New epoch - lr: 0.001\n",
      "0/185 of epoch 5/5 jigsaw : 0.116, class : 0.156 - acc jigsaw : 96.88, class : 93.75 [bs:64]\n",
      "30/185 of epoch 5/5 jigsaw : 0.132, class : 0.053 - acc jigsaw : 96.88, class : 100.00 [bs:64]\n",
      "60/185 of epoch 5/5 jigsaw : 0.346, class : 0.112 - acc jigsaw : 90.62, class : 96.88 [bs:64]\n",
      "90/185 of epoch 5/5 jigsaw : 0.417, class : 0.246 - acc jigsaw : 87.50, class : 93.75 [bs:64]\n",
      "120/185 of epoch 5/5 jigsaw : 0.456, class : 0.222 - acc jigsaw : 89.06, class : 95.31 [bs:64]\n",
      "150/185 of epoch 5/5 jigsaw : 0.293, class : 0.189 - acc jigsaw : 92.19, class : 95.31 [bs:64]\n",
      "180/185 of epoch 5/5 jigsaw : 0.334, class : 0.186 - acc jigsaw : 92.19, class : 95.31 [bs:64]\n",
      "Accuracies on val: jigsaw : 100.00, class : 85.01\n",
      "Accuracies on test: jigsaw : 99.92, class : 61.19\n",
      "It took 1375.53\n"
     ]
    }
   ],
   "source": [
    "args = Args(train[0], test[0])\n",
    "args.n_classes = 66\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b48f788",
   "metadata": {},
   "source": [
    "Avec les mêmes paramètres pour chaque scénario (jig_weigth = 0.33, epochs = 5, batch_size = 64), on obtient les résultats suivants en moyennant sur deux apprentissages:\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|}\n",
    "\\hline\n",
    "    \\text{Train} & \\text{Test} & \\text{Précision} \\\\ \\hline\n",
    "    \\text{Clipart, RealWorld, Product} & \\text{Art} & 60.16 \\\\ \\hline\n",
    "    \\text{Art, RealWorld, Product} & \\text{Clipart} & 52.07 \\\\ \\hline\n",
    "    \\text{Art, Clipart, Product} & \\text{RealWorld} & 77.00 \\\\ \\hline\n",
    "    \\text{Art, Clipart, RealWorld} & \\text{Product} & 76.61 \\\\ \\hline\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a9ce758",
   "metadata": {},
   "source": [
    "# Ouverture à la généralisation de domaines par des tâches prétextes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77e47fa",
   "metadata": {},
   "source": [
    "A travers ce notebook, nous avons exploré une méthode de reconnaissance d'objets à l'intérieur d'images appartenant à un domaine non utilisé pendant l'apprentissage. L'idée générale est d'apprendre simultanément à reconnaître les objects dans les images, qui est l'objectif souhaité, et de résoudre un puzzle. Cette tâche simultanée permet au modèle d'apprendre les caractéristiques générales des objets indépendamment de leur prédiction. Cela diminue le surajustement lié au domaine de l'image et améliore ainsi la géneralisation à de nouveux domaines. La tâche supplémentaire ajoutée pour la géneralisation, qui dans JiGen est la résolution d'un puzzle, est appelée tâche prétexte. L'ajout de tâches prétextes durant l'apprentissage est l'une des techniques utilisées dans la géneralisation de domaine. Résoudre un puzzle est un exemple de tâche prétexte mais il en existe d'autres, qui peuvent aussi être combinées. [Zhou et al (2022)]  \n",
    "\\\n",
    "Une caractéristique importante dans le choix d'une tâche prétexte est qu'elle soit non supervisée. En effet, cela facilite l'implémentation (aucun label n'est à ajouter), permet d'augmenter le nombre d'images associées à la tâche et peut-être appliqué à n'importe quel nouveau domaine. L'ajout de tâches prétextes, comme dans JiGen, améliore le plus souvent la généralisation de la prédiction comparée à un algorithme de classification simple. En revanche, le choix de la ou des tâches prétextes n'est pas évident. Aucune tâche ou combinaison de tâches n'est toujours meilleure que les autres. Le domaine non-vu ainsi que les objets à prédire influent sur la performance de chacune des tâches. Pour augmenter les performances, il faudrait donc connaître à priori les domaines non-vus et les objets, ce qui n'est pas l'objectif de la géneralisation. Cette remarque est une des limites principales des techniques de tâches prétextes pour des jeux de données complexes. [Zhou et al (2022), Bucci et al (2021)] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860476c3",
   "metadata": {},
   "source": [
    "## Exemples de tâches prétextes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ded2107",
   "metadata": {},
   "source": [
    "### Rotation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78530eb",
   "metadata": {},
   "source": [
    "Une des tâches prétextes les plus simples est d'ajouter une rotation à l'image de 0°, 90°, 180° ou 270°. La méthode générale est très similaire à ce que nous avons vu pour le puzzle. Chacune des rotations peut être associée à une variable entière dans {1,2,3,4}. En entrée du réseau, l'image est pivotée d'un angle aléatoire parmis les quatre. La fonction de perte est une comibnaison linéaire d'une perte de cross-entropy sur la classification et d'une perte de cross-entropy sur la rotation. [Gidaris et al. (2018)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fad5435",
   "metadata": {},
   "source": [
    "### Colorisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e23e24a",
   "metadata": {},
   "source": [
    "Une autre tâche prétexte utilisée est la colorisation de l'image. Cela consiste à transformer l'image initial en noir et blanc puis d'apprendre à reconstruire l'image colorée. Une fonction de perte mesurant l'écart entre la couleur prédite et la couleur originale peut être calculée. Choisir la distance euclidienne entre les valeurs RBG de chaque pixel n'est pas optimal car elle favorise les images ternes (ayant des couleurs intermédiaires). Il peut-être souhaitable de choisir une autre fonction de perte. A nouveau, le réseau complet apprendra à la fois la classification et la colorisation. Alors que les rotations et le puzzle se concentrent sur les caractéristiques et les contours des objets à prédire, la colorisation est centré sur les couleurs des objets. Dans le cas des données PACS, ce n'est surement pas la tâche à choisir. En effet, la plupart des images de la classe 'sketch' sont en noir et blanc, ce qui perd l'effet de la colorisation. [Zhang et al. (2016)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df293257",
   "metadata": {},
   "source": [
    "### Filtre de Gabor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0508788",
   "metadata": {},
   "source": [
    "Dans [Albuquerque et al. (2020)], les auteurs utilisent comme tâche prétexte l'apprentissage de filtres de Gabor. Ces filtres peuvent être assimilés à une décomposition en ondelettes. En utilisant plusieurs filtres associés à des orientations différentes, il est possible d'obtenir des informations sur les motifs et les textures présents dans l'image. Il existe une expression analytique associée à ce filtre et la tâche est donc utilisable en non-supervisé. Pour calculer la fonction de perte associée, les auteurs ont transformé la réponse du filtre de Gabor et sa prédiction en image avec des pixels soit noirs soit blancs, en appliquant un seuil à l'image noir et blanc. Pour chacune des 7 orientations du filtre utilisées, la fonction de perte est alors la proportion de pixels différents entre la prédiction et la vraie sortie du filtre."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0e75c7",
   "metadata": {},
   "source": [
    "### Combinaison de rotations et de puzzles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63e9f51",
   "metadata": {},
   "source": [
    "Comme nous l'avons dit au dessus, il est également possible de combiner les tâches prétextes. Le réseau peut par exemple apprendre à la fois à reconnaître la rotation et à résoudre un puzzle. La fonction de perte est alors la somme de trois pertes distinctes. L'objectif est d'augmenter davantage la généralisation qu'en ayant une seule tâche prétexte. Le risque est de délaisser l'apprentissage de la classification, qui est ce que l'on souhaite au bout du compte, même sur de nouveaux domaines. Ainsi, dans les tests efectués par [Bucci et al. (2021)], combiner rotation et puzzle a parfois de meilleurs résultats qu'en ayant qu'un seul des deux, mais ce n'est pas systématique."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95410eed",
   "metadata": {},
   "source": [
    "### References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af82bae9",
   "metadata": {},
   "source": [
    "**[Albuquerque et al. (2020)]** Albuquerque, I., Naik, N., Li, J., Keskar, N., & Socher, R. (2020). Improving out-of-distribution generalization via multi-task self-supervised pretraining. arXiv preprint arXiv:2003.13525. [URL](https://arxiv.org/pdf/2003.13525.pdf)\n",
    "\n",
    "**[Bucci et al. (2021)]** Bucci, S., D’Innocente, A., Liao, Y., Carlucci, F. M., Caputo, B., & Tommasi, T. (2021). Self-supervised learning across domains. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(9), 5516-5528. arXiv preprint arXiv:2007.12368. [URL](https://arxiv.org/pdf/2007.12368.pdf) \n",
    "\n",
    "**[Gidaris et al. (2018)]** Gidaris, S., Singh, P., & Komodakis, N. (2018). Unsupervised representation learning by predicting image rotations. arXiv preprint arXiv:1803.07728. [URL](https://arxiv.org/pdf/1803.07728.pdf)\n",
    "\n",
    "**[Zhang et al. (2016)]** Zhang, R., Isola, P., & Efros, A. A. (2016). Colorful image colorization. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14 (pp. 649-666). Springer International Publishing. [URL](https://arxiv.org/pdf/1603.08511.pdf)\n",
    "\n",
    "**[Zhou et al. (2022)]** Zhou, K., Liu, Z., Qiao, Y., Xiang, T., & Loy, C. C. (2022). Domain generalization: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence. arXiv preprint arXiv:2103.02503. [URL](https://arxiv.org/pdf/2103.02503.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67bc7912",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Louison Bocquet--Nouaille"
   },
   {
    "name": "Pierre-Alain Goulm"
   },
   {
    "name": "Romain Tiphaigne"
   },
   {
    "name": "Axelle Tragné"
   },
   {
    "name": "Alicia Zady"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "title": "Domain Generalization by Solving Jigsaw Puzzles",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "303.4px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
