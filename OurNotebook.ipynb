{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeaf7c44",
   "metadata": {},
   "source": [
    "Ce notebook a comme source principale [Carlucci et al. (2019)](https://arxiv.org/pdf/1903.06864.pdf).\n",
    "\n",
    "Nous commençons par expliquer les méthodes et les procédures en détail avant de les appliquer à un ensemble de données différent de celui utilisé dans l'article (PACS).   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05aedb4",
   "metadata": {},
   "source": [
    "<font size=\"5\"> **Méthodes et procédures utilisées pour la généralisation de domaine en résolvant des casse-têtes/puzzles**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf67ef1",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "L'être humain sait naturellement apprendre en combinant les méthodes supervisées (le savoir transmis par les parents à l'enfant) et non-supervisées (la découverte par l'enfant lui-même à travers ses expériences). Graĉe à ça, il arrive facilement à détecter les irrégularités et à gérer les invariances pour généraliser son savoir. Alors qu'un humain sachant reconnaître un cheval, pourra aussi bien le reconnaître sur une photographie que sur une peinture, c'est une tâche un peu plus complexe et laborieuse pour les algorithmes d'IA en reconnaissance d'images, auxquels il manque encore souvent cette capacité de généralisation.\n",
    "\n",
    "Ce manque de généralisation dans la tâche de reconnaissance d'images a tendance à être pallié par des méthodes supervisées. Par exemple, dans un précédent TP portant sur la classification d'images, nous avions entrainé des réseaux convolutionnels à l'aide d'images labélisées, et pour tenter d'augmenter la capacité de généralisation, nous avions appliqué de multiples transformations afin de déformer nos images, en conservant les labels, puis nous avions entrainé les réseaux en incluant ces images déformées dans notre jeu de données d'entrainement.\n",
    "\n",
    "Cependant, les méthodes supervisées sont par définition limitées par leur non-exaustivité. C'est pourquoi il est intéressant d'explorer la puissance de l'apprentissage non-supervisé, et nous allons ici l'utiliser à des fins de généralisation entre domaines de style. Le but final est d'obtenir un algorithme de reconnaissance d'images capable par exemple de recconaître un cheval sur une image, que celle-ci soit une photographie, une peinture à l'aquarelle, ou issue d'un cartoon (3 domaines différents, 3 styles).\n",
    "\n",
    "Pour cela, nous allons présenter la méthode JiGen développée par F. Carlucci, A. D'Innocente, S. Bucci, B. Caputo, et T. Tommasi, des chercheurs italiens, en 2019. L'idée est la suivante : améliorer la généralisation en reconnaissance d'images provenant de différents domaines en combinant une clasiffication supervisée et une résolution de puzzle de type JigSaw de ces même images afin d'ajouter une cohérence spatiale à la classification.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada85906",
   "metadata": {},
   "source": [
    "# Les puzzles JigSaw dans la méthode JiGen\n",
    "\n",
    "Un *JigSaw Puzzle* se traduit simplement en français par puzzle, la segmentation d'une image en fragments de formes différentes, qu'il faut réassembler pour retrouver l'image initiale. Ces puzzles sont souvent utilisés pour apprendre la cohérence spaciale aux enfants en les encoureageant à apprendre les relations entre les différentes parties des objets. Cet apprentissage est une des clés donnant aux humains cette grande capacité de généralisation visuelle et c'est un mimétisme informatique de cela qui est mis en place à travers la méthode JiGen.\n",
    "\n",
    "Ici, toutes nos images seront de forme carrée, de taille 227 par 227, et lorsqu'on parlera de puzzle JigSaw, les fragments ne seront pas de formes aléatoires mais simplement un découpage en plus petits segments carrés de tailles identiques.\n",
    "\n",
    "On va donc se concentrer sur la résolution de puzzle JigSaw (retrouver l'image initiale à partir d'une image ayant été mélangée) de façon non-supervisée. Cette résolution ne sera pas l'objectif principal, qui reste la classification des images, mais nous allons voir qu'elle améliore les capacités de généralisation de la méthode globale. Les images originales et les images mélangées ayant la même taille, elles peuvent partager le même réseau convolutionnel, afin de combiner la résolution de puzzles apportant la partie cohérence spatiale de l'image, et la classification, apportant la partie le contenu de l'image.\n",
    "\n",
    "C'est cette double résolution simultanée qui va permettre à la méthode JiGen d'être si performante en matière de généralisation à travers les différents domaines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e35dfe",
   "metadata": {},
   "source": [
    "## Généralisation de domaine\n",
    "\n",
    "La généralisation de domaine fait référence à la capacité d'un modèle de machine learning à généraliser ses capacités sur des domaines non vus ou des données hors distribution. Cela s'oppose à l'apprentissage supervisé traditionnel, qui suppose que les données d'entraînement et de test proviennent du même domaine ou de la même distribution. [Wang et al. (2022)]\n",
    "\n",
    "Le modèle est entraîné sur différents domaines, les données sources (si l'on considère un style d'image, cela pourrait être un dessin, une peinture, un dessin animé, etc.). Nous voulons que le modèle soit capable de prédire avec précision la classe dans un domaine non vu, les données cibles (par exemple, des photos).\n",
    "\n",
    "*Définition 1 (Domaine).*\n",
    "Soit X un espace d'entrée non vide (input) et Y un espace de sortie (output). Un domaine est composé de données échantillonnées à partir d'une distribution. Nous le notons comme $S = {(x_i, y_i)}^n_{i=1}∼P_{XY}$ , où $x \\in X \\subset \\mathbb{R}^d$, $y \\in Y \\subset \\mathbb{R}$ représente le label, et $P_{XY}$ représente la distribution conjointe de l'échantillon d'entrée et du label de sortie. X et Y désignent les variables aléatoires correspondantes. [Wang et al. (2022)]\n",
    "\n",
    "*Définition 2 (Généralisation de domaine).*\n",
    "En généralisation de domaine, on pose M domaines d'entraînement (source) $S_{train} = \\{S^i | i = 1, ..., M \\}$, où $S^i = {(x^i_j , y^i_j )}^{n_i}_{j=1}$ désigne le i-ème domaine. Les distributions conjointes entre chaque paire de domaines sont différentes : $P^i_{XY} \\neq P^j_{XY}$, $1 \\leq i \\neq j \\leq M$. L'objectif de la généralisation de domaine est d'apprendre une fonction prédictive robuste et généralisable $h : X → Y$ à partir des M domaines d'entraînement pour obtenir une erreur de prédiction minimale sur un domaine de test non vu $S_{test}$ (c'est-à-dire, $S_{test}$ n'est pas accessible lors de l'entraînement et $P^{test}_{XY} \\neq P^i_{XY}$ pour $i \\in \\{1, ..., M\\}$):\n",
    "$$ \\min_h \\mathbb{E}_{(x,y) \\in S_{test}} [\\ell(h(x), y)] $$\n",
    "où $\\ell(·, ·)$ est la fonction de perte. [Wang et al. (2022)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d08c2e2",
   "metadata": {},
   "source": [
    "## JiGENDG\n",
    "L'algorithme repose sur l'idée d'utiliser des casse-têtes/puzzles pour entraîner un modèle à être invariant sur différents domaines. Le réseau apprend simultanément à résoudre les casse-têtes et à classifier les images. [Carlucci et al. (2019)]\n",
    "\n",
    "\n",
    "### Dataset\n",
    "Les données d'entrée sont un ensemble de N images provenant de S domaines. Dans chaque domaine i, nous avons $N_i$ observations labellisées. Nous écrivons $\\{ x^i_j, y^i_j \\} _{j=1}^{N_i}$, ce qui signifie que pour la j-ème image du i-ème domaine $x^i_j$, le label associé est $y^i_j$.\n",
    "\n",
    "Nous avons $x^i_j \\in \\mathbb{R}^{n_p \\times n_p}$ où $n_p \\times n_p$ est la taille des images en pixels, en supposant que les images sont des carrés. Nous avons $y^i_j \\in \\mathbb{R}^{C}$ où C est le nombre de classes, car l'étiquette $y^i_j$ est encodée en one-hot.\n",
    "\n",
    "En termes de dimensions, $\\{x^i_j, y^i_j\\}_{j=1}^{N_i} \\in \\left( \\left( \\mathbb{R}^{n_p \\times n_p} \\times \\mathbb{R}^{C} \\right) ^ {N_i} \\right) ^ {S}$ où $N_i \\times S \\leq N$ car le nombre d'images étiquetées $N_i \\times S$ ne dépasse pas le nombre total d'images $N$.\n",
    "\n",
    "\n",
    "### Dataset permuté\n",
    "À partir de l'ensemble de données non permuté, nous créons un nouveau jeu de données utilisé pour la tâche de résolution de casse-têtes. Nous considérons des permutations sur une grille $n \\times n$ (dans l'article et notre travail, nous fixons $n=3$).\n",
    "\n",
    "Bien que nous ayons un total de $n^2!$ permutations possibles, nous n'en considérons que P. Nous les choisissons en fonction de la distance de Hamming, ce qui signifie que nous ne conservons que celles avec le moins de différences de position. Cela permet de simplifier un peu la tâche et également de réduire le temps d'inférence (utiliser les $n^2!=362 880$ possibilités serait beaucoup plus chronophage que d'utiliser $P=30$ permutations).\n",
    "\n",
    "La non-permutation est toujours incluse dans le sous-ensemble des permutations P.\n",
    "\n",
    "Chaque permutation possible est associée à un indice qui permet de traiter le problème comme une tâche de classification où l'étiquette est un vecteur encodé en one-hot des indices des permutations.\n",
    "\n",
    "Nous notons $\\left\\{z^i_k, p^i_k\\right\\}_{k=1}^{K_i} \\in \\left( \\left( \\mathbb{R}^{n_p \\times n_p} \\times \\mathbb{R}^{P} \\right) ^ {K_i} \\right) ^ {S}$ où $z^i_k$ est l'image permutée, $p^i_k$ est l'indice de la permutation utilisée sur l'image associée, $K_i$ est le nombre d'instances étiquetées et $P$ est le nombre de permutations considérées.\n",
    "\n",
    "\n",
    "\n",
    "###  Fonction de perte\n",
    "Rappelons brièvement comment un réseau de neurones est entraîné :\n",
    "- Le modèle traite un batch de $b$ échantillons d'entrée. Chaque échantillon passe à travers le réseau, et la sortie est calculée.\n",
    "- La fonction de perte est appliquée à la sortie prédite et aux valeurs cibles pour le lot. Cette perte représente la dissimilarité entre les valeurs prédites et réelles.\n",
    "- Les gradients cumulés de la perte sont calculés par rapport à chaque paramètre.\n",
    "- Les paramètres du modèle sont mis à jour en fonction des gradients calculés. Le taux d'apprentissage $\\eta$ contrôle dans quelle mesure les paramètres du modèle changent dans la direction qui minimise la perte.\n",
    "\n",
    "Ce processus est répété pendant $E$ époques. Chaque époque implique le traitement de l'ensemble du jeu de données.\n",
    "\n",
    "Les batches sont composés d'un mélange d'images ordonnées et mélangées. Le ratio est défini par $\\beta$ : pour $\\beta=0.75$, 75 % du batch est composé d'images ordonnées et le reste d'images mélangées. Si nous avons un batch de taille $b=128$, cela signifierait que nous avons $N_i=0.75\\times128=96$ et $K_i=(1-0.75)\\times128=32$.\n",
    "\n",
    "Dans JiGen, la fonction de perte prend une forme particulière car deux tâches sont apprises.\n",
    "\n",
    "\n",
    "#### Cas supervisé\n",
    "Nous cherchons à optimiser les paramètres avec le problème de minimisation suivant :\n",
    "\n",
    "$$ argmin_{\\theta_f, \\theta_p, \\theta_c} \\sum_{i=1}^{S} \\sum_{j=1}^{N_i} \\mathcal{L}_c \\left( h(x^i_j|\\theta_f, \\theta_c), y^i_j\\right) + \\sum_{k=1}^{K_i} \\alpha \\mathcal{L}_p \\left( h(z^i_k|\\theta_f, \\theta_p), p^i_k\\right) $$\n",
    "\n",
    "- $\\mathcal{L}_c$ est une perte (la cross-entropy) pour la tâche de classification d'image. Nous rappelons que $\\mathcal{L}c \\left( h(x^i_j|\\theta_f, \\theta_c), y^i_j\\right) = - \\sum_{c \\in C} y^i_j \\log(\\mathbb{P}(h(x^i_j|\\theta_f, \\theta_c)=c))$ ;\n",
    "- $\\mathcal{L}_p$ est une perte (la cross-entropy) pour la tâche de résolution de casse-têtes ;\n",
    "- $\\alpha$ est le poids de la perte pour le casse-tête (l'importance que nous accordons à la tâche de résolution de casse-tête par rapport à celle de la tâche de classification) ;\n",
    "- $h$ est la fonction d'activation du modèle profond (deep model), elle prédit le label ;\n",
    "- $\\theta_f$ est l'ensemble des paramètres (poids et biais) pour la couche entièrement connectée (fully connected layer);\n",
    "- $\\theta_p$ est l'ensemble des paramètres pour la dernière couche entièrement connectée dédiée à la reconnaissance de permutation ;\n",
    "- $\\theta_c$ est l'ensemble des paramètres pour la couche de convolution.\n",
    "\n",
    "La perte du casse-tête $\\mathcal{L}_p$ est calculée sur l'image ordonnée, mais la perte de classification $\\mathcal{L}_c$ n'est pas calculée sur les images mélangées car cela rendrait la reconnaissance d'objets plus difficile.\n",
    "\n",
    "\n",
    "#### Unsupervised case\n",
    "JiGen a été conçu dans le but de la généralisation de domaine non supervisée. La seule différence avec JiGen dans le cas supervisé réside dans la perte pour la tâche de classification d'image :\n",
    "\n",
    "$$ argmin_{\\theta_f, \\theta_p, \\theta_c} \\sum_{i=1}^{S} \\mathcal{L}_E (x^i) + \\sum_{k=1}^{K_i} \\alpha \\mathcal{L}_p \\left( h(z^i_k|\\theta_f, \\theta_p), p^i_k\\right) $$\n",
    "\n",
    "avec $\\mathcal{L}_E (x^i) = \\sum_{y \\in \\mathcal{Y}} h(x^i|\\theta_f, \\theta_c) \\log(h(x^i|\\theta_f, \\theta_c))$, la cross-entropy empirique.\n",
    "\n",
    "Remarque : la somme $\\sum_{j=1}^{N_i}$ disparaît car nous considérons toutes les images et non pas seulement celles étiquetées.\n",
    "\n",
    "\n",
    "### Test\n",
    "Pour tester le modèle, nous ne considérons que la partie classification du réseau : nous n'utilisons pas la couche entièrement connectée finale qui sert à la résolution de casse-tête. Cela revient à fixer $\\alpha=0$.\n",
    "\n",
    "\n",
    "### Parameters\n",
    "Pour toutes les expériences, nous préciserons clairement les valeurs des caractéristiques de l'ensemble de données : les tailles des images $n_b$, le nombre d'images $N$, et le nombre de classes $C$.\n",
    "\n",
    "Pour tous les paramètres, nous considérerons les mêmes paramètres pour le casse-tête : la taille de la grille $n$, le nombre de permutations considérées $P$, et le biais des données $\\beta$. Les auteurs choisissent ces paramètres avec une validation croisée sur 10 % de l'ensemble de données, pour chaque expérience.\n",
    "\n",
    "Nous fixerons les paramètres d'expérience : la taille des batchs $b$, le nombre d'époques $E$, le taux d'apprentissage $\\eta$, et le poids du casse-tête $\\alpha$. (Les auteurs ont fixé ...)\n",
    "\n",
    "Les paramètres du modèle optimisés par rétropropagation et non choisis par l'utilisateur sont $\\theta_f$, $\\theta_p$, et $\\theta_c$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d617ee3b",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1928832",
   "metadata": {},
   "source": [
    "**[Carlucci et al. (2019)]** Carlucci, F. M., D'Innocente, A., Bucci, S., Caputo, B., & Tommasi, T. (2019). Domain Generalization by Solving Jigsaw Puzzles. arXiv preprint arXiv:1903.06864. [URL](https://arxiv.org/pdf/1903.06864.pdf)\n",
    "\n",
    "**[Wang et al. (2022)]** Wang, J., Lan, C., Liu, C., Ouyang, Y., Qin, T., Lu, W., Chen, Y., Zeng, W., & Yu, P. S. (2022). Generalizing to Unseen Domains: A Survey on Domain Generalization. arXiv preprint arXiv:2103.03097. [URL](https://arxiv.org/pdf/2103.03097.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5083e865",
   "metadata": {},
   "source": [
    "# Using JiGen on PACS (as in the article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "16989a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stocker le dossier PACS au même endroit que ce notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "415285ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython.core.debugger import set_trace\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torch.autograd import Function\n",
    "from torchvision.models.resnet import BasicBlock,Bottleneck\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch import optim\n",
    "import torchvision\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import join, dirname\n",
    "from collections import OrderedDict\n",
    "from itertools import chain\n",
    "from PIL import Image\n",
    "from random import sample, random\n",
    "import bisect\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import scipy.misc \n",
    "try:\n",
    "    from StringIO import StringIO  # Python 2.7\n",
    "except ImportError:\n",
    "    from io import BytesIO         # Python 3.x\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0747db0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commentaires sur le choix du réseau :\n",
    "# resnet18 et resnet50 fonctionnent pour image_size=222\n",
    "\n",
    "\n",
    "class Args:\n",
    "    \n",
    "    def __init__(self, source, target):\n",
    "        \n",
    "        \n",
    "        self.source = source\n",
    "        self.target = target\n",
    "        \n",
    "    batch_size = 64\n",
    "    image_size = 222              # 222 \n",
    "    \n",
    "    min_scale = 0.8               # Minimum scale percent\n",
    "    max_scale = 1.0               # Maximum scale percent\n",
    "    random_horiz_flip = 0.0       # Chance of random horizontal flip\n",
    "    jitter = 0.0                  # Color jitter amount\n",
    "    tile_random_grayscale = 0.1   # Chance of randomly greyscaling a tile\n",
    "    \n",
    "    limit_source = None     # If set, it will limit the number of training samples\n",
    "    limit_target = None     # If set, it will limit the number of testing samples\n",
    "    \n",
    "    learning_rate = 0.01\n",
    "    epochs = 5\n",
    "    n_classes = 7              # Number of classes for object prediction\n",
    "    jigsaw_n_classes = 31       # Number of permutation classes for the puzzle\n",
    "    network = \"resnet50\"        # To choose from : 'resnet18', 'resnet50'\n",
    "    jig_weight = 0.7            # Weight for the jigsaw puzzle compared to the classification\n",
    "    ooo_weight = 0              # Weight for odd one out task\n",
    "    tf_logger = True            # If True will save tensorboard compatible logs\n",
    "    val_size = 0.1              # Validation size (between 0 and 1)\n",
    "    folder_name = \"Test\"        # Used by the logger to save logs\n",
    "    bias_whole_image = 0.9      # If set, will bias the training procedure to show more often the whole image\n",
    "    TTA = False                 # Activate test time data augmentation\n",
    "    classify_only_sane = False  # If true, the network will only try to classify the non scrambled images\n",
    "    train_all = True            # If true, all network weights will be trained\n",
    "    suffix = \"\"                 # Suffix for the logger\n",
    "    nesterov = False            # Use nesterov\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1443b1",
   "metadata": {},
   "source": [
    "## Fichiers de /model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "afac8f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common to all networks definition\n",
    "class Id(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Id, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8555331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_utils.py\n",
    "\n",
    "class GradientKillerLayer(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, **kwargs):\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return None, None\n",
    "\n",
    "\n",
    "class ReverseLayerF(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_val):\n",
    "        ctx.lambda_val = lambda_val\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.lambda_val\n",
    "\n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bbf8cf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet.py\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, jigsaw_classes=1000, classes=100, domains=3):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.jigsaw_classifier = nn.Linear(512 * block.expansion, jigsaw_classes)\n",
    "        self.class_classifier = nn.Linear(512 * block.expansion, classes)\n",
    "        #self.domain_classifier = nn.Linear(512 * block.expansion, domains)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def is_patch_based(self):\n",
    "        return False\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.jigsaw_classifier(x),self.class_classifier(x)\n",
    "\n",
    "\n",
    "def resnet18(pretrained=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    print(\"Using ResNet-18\")\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url('https://download.pytorch.org/models/resnet18-5c106cde.pth'), strict=False)\n",
    "        #model.load_state_dict(model_zoo.load_url(model_urls['resnet18']), strict=False)\n",
    "    return model\n",
    "\n",
    "def resnet50(pretrained=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    print(\"Using ResNet-50\")\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url('https://download.pytorch.org/models/resnet50-19c8e357.pth'), strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "48112cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_factory.py\n",
    "\n",
    "nets_map = {\n",
    "    'resnet18': resnet18,\n",
    "    'resnet50': resnet50,\n",
    "}\n",
    "\n",
    "\n",
    "def get_network(name):\n",
    "    if name not in nets_map:\n",
    "        raise ValueError('Name of network unknown %s' % name)\n",
    "\n",
    "    def get_network_fn(**kwargs):\n",
    "        return nets_map[name](**kwargs)\n",
    "\n",
    "    return get_network_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984998b8",
   "metadata": {},
   "source": [
    "## Fichiers de /data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f2e3fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardDataset.py\n",
    "\n",
    "def get_dataset(path, mode, image_size):\n",
    "    if mode == \"train\":\n",
    "        img_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(image_size, scale=(0.7, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[1/256., 1/256., 1/256.])  # std=[1/256., 1/256., 1/256.] #[0.229, 0.224, 0.225]\n",
    "        ])\n",
    "    else:\n",
    "        img_transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            # transforms.CenterCrop(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], std=[1/256., 1/256., 1/256.])  # std=[1/256., 1/256., 1/256.]\n",
    "        ])\n",
    "    return datasets.ImageFolder(path, transform=img_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d03bdae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JigsawLoader.py\n",
    "\n",
    "\n",
    "def get_random_subset(names, labels, percent):\n",
    "    \"\"\"\n",
    "\n",
    "    :param names: list of names\n",
    "    :param labels:  list of labels\n",
    "    :param percent: 0 < float < 1\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    samples = len(names)\n",
    "    amount = int(samples * percent)\n",
    "    random_index = sample(range(samples), amount)\n",
    "    name_val = [names[k] for k in random_index]\n",
    "    name_train = [v for k, v in enumerate(names) if k not in random_index]\n",
    "    labels_val = [labels[k] for k in random_index]\n",
    "    labels_train = [v for k, v in enumerate(labels) if k not in random_index]\n",
    "    return name_train, name_val, labels_train, labels_val\n",
    "\n",
    "\n",
    "def _dataset_info(txt_labels):\n",
    "    with open(txt_labels, 'r') as f:\n",
    "        images_list = f.readlines()\n",
    "\n",
    "    file_names = []\n",
    "    labels = []\n",
    "    for row in images_list:\n",
    "        row = row.split(' ')\n",
    "        file_names.append(row[0])\n",
    "        labels.append(int(row[1]))\n",
    "\n",
    "    return file_names, labels\n",
    "\n",
    "\n",
    "def get_split_dataset_info(txt_list, val_percentage):\n",
    "    names, labels = _dataset_info(txt_list)\n",
    "    return get_random_subset(names, labels, val_percentage)\n",
    "\n",
    "\n",
    "class JigsawDataset(data.Dataset):\n",
    "    def __init__(self, names, labels, jig_classes=100, img_transformer=None, tile_transformer=None, patches=True, bias_whole_image=None):\n",
    "        self.data_path = \"\"\n",
    "        self.names = names\n",
    "        self.labels = labels\n",
    "\n",
    "        self.N = len(self.names)\n",
    "        self.permutations = self.__retrieve_permutations(jig_classes)\n",
    "        self.grid_size = 3\n",
    "        self.bias_whole_image = bias_whole_image\n",
    "        if patches:\n",
    "            self.patch_size = 64\n",
    "        self._image_transformer = img_transformer\n",
    "        self._augment_tile = tile_transformer\n",
    "        if patches:\n",
    "            self.returnFunc = lambda x: x\n",
    "        else:\n",
    "            def make_grid(x):\n",
    "                return torchvision.utils.make_grid(x, self.grid_size, padding=0)\n",
    "            self.returnFunc = make_grid\n",
    "\n",
    "    def get_tile(self, img, n):\n",
    "        w = float(img.size[0]) / self.grid_size\n",
    "        y = int(n / self.grid_size)\n",
    "        x = n % self.grid_size\n",
    "        tile = img.crop([x * w, y * w, (x + 1) * w, (y + 1) * w])\n",
    "        tile = self._augment_tile(tile)\n",
    "        return tile\n",
    "    \n",
    "    def get_image(self, index):\n",
    "        framename = self.data_path + self.names[index]\n",
    "        img = Image.open(framename).convert('RGB')\n",
    "        return self._image_transformer(img)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = self.get_image(index)\n",
    "        n_grids = self.grid_size ** 2\n",
    "        tiles = [None] * n_grids\n",
    "        for n in range(n_grids):\n",
    "            tiles[n] = self.get_tile(img, n)\n",
    "\n",
    "        order = np.random.randint(len(self.permutations) + 1)  # added 1 for class 0: unsorted\n",
    "        if self.bias_whole_image:\n",
    "            if self.bias_whole_image > random():\n",
    "                order = 0\n",
    "        if order == 0:\n",
    "            data = tiles\n",
    "        else:\n",
    "            data = [tiles[self.permutations[order - 1][t]] for t in range(n_grids)]\n",
    "            \n",
    "        data = torch.stack(data, 0)\n",
    "        return self.returnFunc(data), int(order), int(self.labels[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __retrieve_permutations(self, classes):\n",
    "        all_perm = np.load('permutations_%d.npy' % (classes))\n",
    "        # from range [1,9] to [0,8]\n",
    "        if all_perm.min() == 1:\n",
    "            all_perm = all_perm - 1\n",
    "\n",
    "        return all_perm\n",
    "\n",
    "\n",
    "class JigsawTestDataset(JigsawDataset):\n",
    "    def __init__(self, *args, **xargs):\n",
    "        super().__init__(*args, **xargs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        framename = self.data_path + self.names[index]\n",
    "        img = Image.open(framename).convert('RGB')\n",
    "        return self._image_transformer(img), 0, int(self.labels[index])\n",
    "\n",
    "\n",
    "class JigsawTestDatasetMultiple(JigsawDataset):\n",
    "    def __init__(self, *args, **xargs):\n",
    "        super().__init__(*args, **xargs)\n",
    "        self._image_transformer = transforms.Compose([\n",
    "            transforms.Resize(255, Image.BILINEAR),\n",
    "        ])\n",
    "        self._image_transformer_full = transforms.Compose([\n",
    "            transforms.Resize(225, Image.BILINEAR),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self._augment_tile = transforms.Compose([\n",
    "            transforms.Resize((75, 75), Image.BILINEAR),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        framename = self.data_path + self.names[index]\n",
    "        _img = Image.open(framename).convert('RGB')\n",
    "        img = self._image_transformer(_img)\n",
    "\n",
    "        w = float(img.size[0]) / self.grid_size\n",
    "        n_grids = self.grid_size ** 2\n",
    "        images = []\n",
    "        jig_labels = []\n",
    "        tiles = [None] * n_grids\n",
    "        for n in range(n_grids):\n",
    "            y = int(n / self.grid_size)\n",
    "            x = n % self.grid_size\n",
    "            tile = img.crop([x * w, y * w, (x + 1) * w, (y + 1) * w])\n",
    "            tile = self._augment_tile(tile)\n",
    "            tiles[n] = tile\n",
    "        for order in range(0, len(self.permutations)+1, 3):\n",
    "            if order==0:\n",
    "                data = tiles\n",
    "            else:\n",
    "                data = [tiles[self.permutations[order-1][t]] for t in range(n_grids)]\n",
    "            data = self.returnFunc(torch.stack(data, 0))\n",
    "            images.append(data)\n",
    "            jig_labels.append(order)\n",
    "        images = torch.stack(images, 0)\n",
    "        jig_labels = torch.LongTensor(jig_labels)\n",
    "        return images, jig_labels, int(self.labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4877e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat_dataset.py\n",
    "\n",
    "\n",
    "class ConcatDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset to concatenate multiple datasets.\n",
    "    Purpose: useful to assemble different existing datasets, possibly\n",
    "    large-scale datasets as the concatenation operation is done in an\n",
    "    on-the-fly manner.\n",
    "\n",
    "    Arguments:\n",
    "        datasets (sequence): List of datasets to be concatenated\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def cumsum(sequence):\n",
    "        r, s = [], 0\n",
    "        for e in sequence:\n",
    "            l = len(e)\n",
    "            r.append(l + s)\n",
    "            s += l\n",
    "        return r\n",
    "\n",
    "    def isMulti(self):\n",
    "        return isinstance(self.datasets[0], JigsawTestDatasetMultiple)\n",
    "\n",
    "    def __init__(self, datasets):\n",
    "        super(ConcatDataset, self).__init__()\n",
    "        assert len(datasets) > 0, 'datasets should not be an empty iterable'\n",
    "        self.datasets = list(datasets)\n",
    "        self.cumulative_sizes = self.cumsum(self.datasets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.cumulative_sizes[-1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)\n",
    "        if dataset_idx == 0:\n",
    "            sample_idx = idx\n",
    "        else:\n",
    "            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]\n",
    "        return self.datasets[dataset_idx][sample_idx], dataset_idx\n",
    "\n",
    "    @property\n",
    "    def cummulative_sizes(self):\n",
    "        warnings.warn(\"cummulative_sizes attribute is renamed to \"\n",
    "                      \"cumulative_sizes\", DeprecationWarning, stacklevel=2)\n",
    "        return self.cumulative_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8e4ab900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_helper.py\n",
    "\n",
    "mnist = 'mnist'\n",
    "mnist_m = 'mnist_m'\n",
    "svhn = 'svhn'\n",
    "synth = 'synth'\n",
    "usps = 'usps'\n",
    "\n",
    "vlcs_datasets = [\"CALTECH\", \"LABELME\", \"PASCAL\", \"SUN\"]\n",
    "pacs_datasets = [\"art_painting\", \"cartoon\", \"photo\", \"sketch\"]\n",
    "office_datasets = [\"amazon\", \"dslr\", \"webcam\"]\n",
    "digits_datasets = [mnist, mnist, svhn, usps]\n",
    "available_datasets = office_datasets + pacs_datasets + vlcs_datasets + digits_datasets\n",
    "#office_paths = {dataset: \"/home/enoon/data/images/office/%s\" % dataset for dataset in office_datasets}\n",
    "#pacs_paths = {dataset: \"/home/enoon/data/images/PACS/kfold/%s\" % dataset for dataset in pacs_datasets}\n",
    "vlcs_paths = {dataset: \"/home/goulmdata/images/VLCS/%s/test\" % dataset for dataset in vlcs_datasets}\n",
    "#paths = {**office_paths, **pacs_paths, **vlcs_paths}\n",
    "\n",
    "dataset_std = {mnist: (0.30280363, 0.30280363, 0.30280363),\n",
    "               mnist_m: (0.2384788, 0.22375608, 0.24496263),\n",
    "               svhn: (0.1951134, 0.19804622, 0.19481073),\n",
    "               synth: (0.29410212, 0.2939651, 0.29404707),\n",
    "               usps: (0.25887518, 0.25887518, 0.25887518),\n",
    "               }\n",
    "\n",
    "dataset_mean = {mnist: (0.13909429, 0.13909429, 0.13909429),\n",
    "                mnist_m: (0.45920207, 0.46326601, 0.41085603),\n",
    "                svhn: (0.43744073, 0.4437959, 0.4733686),\n",
    "                synth: (0.46332872, 0.46316052, 0.46327512),\n",
    "                usps: (0.17025368, 0.17025368, 0.17025368),\n",
    "                }\n",
    "\n",
    "\n",
    "class Subset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, limit):\n",
    "        indices = torch.randperm(len(dataset))[:limit]\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "\n",
    "def get_train_dataloader(args, patches):\n",
    "    dataset_list = args.source\n",
    "    assert isinstance(dataset_list, list)\n",
    "    datasets = []\n",
    "    val_datasets = []\n",
    "    img_transformer, tile_transformer = get_train_transformers(args)\n",
    "    limit = args.limit_source\n",
    "    for dname in dataset_list:\n",
    "        name_train, name_val, labels_train, labels_val = get_split_dataset_info(join(os.path.abspath(''), 'data/txt_lists', '%s_train.txt' % dname), args.val_size)\n",
    "        train_dataset = JigsawDataset(name_train, labels_train, patches=patches, img_transformer=img_transformer,\n",
    "                                      tile_transformer=tile_transformer, jig_classes=args.jigsaw_n_classes, bias_whole_image=args.bias_whole_image)\n",
    "        if limit:\n",
    "            train_dataset = Subset(train_dataset, limit)\n",
    "        datasets.append(train_dataset)\n",
    "        val_datasets.append(JigsawTestDataset(name_val, labels_val, img_transformer=get_val_transformer(args),\n",
    "                              patches=patches, jig_classes=args.jigsaw_n_classes))\n",
    "    dataset = ConcatDataset(datasets)\n",
    "    val_dataset = ConcatDataset(val_datasets)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True, drop_last=False)\n",
    "    return loader, val_loader\n",
    "\n",
    "\n",
    "def get_val_dataloader(args, patches=False):\n",
    "    names, labels = _dataset_info(join(os.path.abspath(''), 'data/txt_lists', '%s_test.txt' % args.target))\n",
    "    img_tr = get_val_transformer(args)\n",
    "    val_dataset = JigsawTestDataset(names, labels, patches=patches, img_transformer=img_tr, jig_classes=args.jigsaw_n_classes)\n",
    "    if args.limit_target and len(val_dataset) > args.limit_target:\n",
    "        val_dataset = Subset(val_dataset, args.limit_target)\n",
    "        print(\"Using %d subset of val dataset\" % args.limit_target)\n",
    "    dataset = ConcatDataset([val_dataset])\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True, drop_last=False)\n",
    "    return loader\n",
    "\n",
    "\n",
    "def get_jigsaw_val_dataloader(args, patches=False):\n",
    "    names, labels = _dataset_info(join(os.path.abspath(''), 'data/txt_lists', '%s_test.txt' % args.target))\n",
    "    img_tr = [transforms.Resize((args.image_size, args.image_size))]\n",
    "    tile_tr = [transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
    "    img_transformer = transforms.Compose(img_tr)\n",
    "    tile_transformer = transforms.Compose(tile_tr)\n",
    "    val_dataset = JigsawDataset(names, labels, patches=patches, img_transformer=img_transformer,\n",
    "                                      tile_transformer=tile_transformer, jig_classes=args.jigsaw_n_classes, bias_whole_image=args.bias_whole_image)\n",
    "    if args.limit_target and len(val_dataset) > args.limit_target:\n",
    "        val_dataset = Subset(val_dataset, args.limit_target)\n",
    "        print(\"Using %d subset of val dataset\" % args.limit_target)\n",
    "    dataset = ConcatDataset([val_dataset])\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True, drop_last=False)\n",
    "    return loader\n",
    "\n",
    "\n",
    "def get_train_transformers(args):\n",
    "    img_tr = [transforms.RandomResizedCrop((int(args.image_size), int(args.image_size)), (args.min_scale, args.max_scale))]\n",
    "    if args.random_horiz_flip > 0.0:\n",
    "        img_tr.append(transforms.RandomHorizontalFlip(args.random_horiz_flip))\n",
    "    if args.jitter > 0.0:\n",
    "        img_tr.append(transforms.ColorJitter(brightness=args.jitter, contrast=args.jitter, saturation=args.jitter, hue=min(0.5, args.jitter)))\n",
    "\n",
    "    tile_tr = []\n",
    "    if args.tile_random_grayscale:\n",
    "        tile_tr.append(transforms.RandomGrayscale(args.tile_random_grayscale))\n",
    "    tile_tr = tile_tr + [transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
    "\n",
    "    return transforms.Compose(img_tr), transforms.Compose(tile_tr)\n",
    "\n",
    "\n",
    "def get_val_transformer(args):\n",
    "    img_tr = [transforms.Resize((args.image_size, args.image_size)), transforms.ToTensor(),transforms.Normalize([0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
    "    return transforms.Compose(img_tr)\n",
    "\n",
    "\n",
    "def get_target_jigsaw_loader(args):\n",
    "    img_transformer, tile_transformer = get_train_transformers(args)\n",
    "    name_train, _, labels_train, _ = get_split_dataset_info(join(os.path.abspath(''), 'data/txt_lists', '%s_train.txt' % args.target), 0)\n",
    "    dataset = JigsawDataset(name_train, labels_train, patches=False, img_transformer=img_transformer,tile_transformer=tile_transformer, jig_classes=args.jigsaw_n_classes, bias_whole_image=args.bias_whole_image)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e04504a",
   "metadata": {},
   "source": [
    "## Fichier de /optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4999283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer_helper.py\n",
    "\n",
    "def get_optim_and_scheduler(network, epochs, lr, train_all, nesterov=False):\n",
    "    if train_all:\n",
    "        params = network.parameters()\n",
    "    else:\n",
    "        params = network.get_params(lr)\n",
    "    optimizer = optim.SGD(params, weight_decay=.0005, momentum=.9, nesterov=nesterov, lr=lr)\n",
    "    #optimizer = optim.Adam(params, lr=lr)\n",
    "    step_size = int(epochs * .8)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size)\n",
    "    print(\"Step size: %d\" % step_size)\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29b0b71",
   "metadata": {},
   "source": [
    "## Fichiers de /utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c0b803f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_logger.py\n",
    "\n",
    "class TFLogger(object):\n",
    "    \n",
    "    def __init__(self, log_dir):\n",
    "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
    "        self.writer = tf.summary.create_file_writer(log_dir)\n",
    "        #self.writer = tf.summary.FileWriter(log_dir)\n",
    "\n",
    "    def scalar_summary(self, tag, value, step):\n",
    "        \"\"\"Log a scalar variable.\"\"\"\n",
    "        #summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
    "        #self.writer.add_summary(summary, step)\n",
    "        with self.writer.as_default():\n",
    "            tf.summary.scalar(tag, value, step=step)\n",
    "            self.writer.flush()\n",
    "            \n",
    "    def image_summary(self, tag, images, step):\n",
    "        \"\"\"Log a list of images.\"\"\"\n",
    "\n",
    "        img_summaries = []\n",
    "        for i, img in enumerate(images):\n",
    "            # Write the image to a string\n",
    "            try:\n",
    "                s = StringIO()\n",
    "            except:\n",
    "                s = BytesIO()\n",
    "            scipy.misc.toimage(img).save(s, format=\"png\")\n",
    "\n",
    "            # Create an Image object\n",
    "            img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n",
    "                                       height=img.shape[0],\n",
    "                                       width=img.shape[1])\n",
    "            # Create a Summary value\n",
    "            img_summaries.append(tf.Summary.Value(tag='%s/%d' % (tag, i), image=img_sum))\n",
    "\n",
    "        # Create and write Summary\n",
    "        summary = tf.Summary(value=img_summaries)\n",
    "        self.writer.add_summary(summary, step)\n",
    "        \n",
    "    def histo_summary(self, tag, values, step, bins=1000):\n",
    "        \"\"\"Log a histogram of the tensor of values.\"\"\"\n",
    "\n",
    "        # Create a histogram using numpy\n",
    "        counts, bin_edges = np.histogram(values, bins=bins)\n",
    "\n",
    "        # Fill the fields of the histogram proto\n",
    "        hist = tf.HistogramProto()\n",
    "        hist.min = float(np.min(values))\n",
    "        hist.max = float(np.max(values))\n",
    "        hist.num = int(np.prod(values.shape))\n",
    "        hist.sum = float(np.sum(values))\n",
    "        hist.sum_squares = float(np.sum(values**2))\n",
    "\n",
    "        # Drop the start of the first bin\n",
    "        bin_edges = bin_edges[1:]\n",
    "\n",
    "        # Add bin edges and counts\n",
    "        for edge in bin_edges:\n",
    "            hist.bucket_limit.append(edge)\n",
    "        for c in counts:\n",
    "            hist.bucket.append(c)\n",
    "\n",
    "        # Create and write Summary\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "        self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7049ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger.py\n",
    "\n",
    "\n",
    "_log_path = join(os.path.abspath(''), '../logs')\n",
    "\n",
    "\n",
    "# high level wrapper for tf_logger.TFLogger\n",
    "class Logger():\n",
    "    def __init__(self, args, update_frequency=10):\n",
    "        self.current_epoch = 0\n",
    "        self.max_epochs = args.epochs\n",
    "        self.last_update = time()\n",
    "        self.start_time = time()\n",
    "        self._clean_epoch_stats()\n",
    "        self.update_f = update_frequency\n",
    "        folder, logname = self.get_name_from_args(args)\n",
    "        log_path = join(_log_path, folder, logname)\n",
    "        if args.tf_logger:\n",
    "            self.tf_logger = TFLogger(log_path)\n",
    "            print(\"Saving to %s\" % log_path)\n",
    "        else:\n",
    "            self.tf_logger = None\n",
    "        self.current_iter = 0\n",
    "\n",
    "    def new_epoch(self, learning_rates):\n",
    "        self.current_epoch += 1\n",
    "        self.last_update = time()\n",
    "        self.lrs = learning_rates\n",
    "        print(\"New epoch - lr: %s\" % \", \".join([str(lr) for lr in self.lrs]))\n",
    "        self._clean_epoch_stats()\n",
    "        if self.tf_logger:\n",
    "            for n, v in enumerate(self.lrs):\n",
    "                self.tf_logger.scalar_summary(\"aux/lr%d\" % n, v, self.current_iter)\n",
    "\n",
    "    def log(self, it, iters, losses, samples_right, total_samples):\n",
    "        self.current_iter += 1\n",
    "        loss_string = \", \".join([\"%s : %.3f\" % (k, v) for k, v in losses.items()])\n",
    "        for k, v in samples_right.items():\n",
    "            past = self.epoch_stats.get(k, 0.0)\n",
    "            self.epoch_stats[k] = past + v\n",
    "        self.total += total_samples\n",
    "        acc_string = \", \".join([\"%s : %.2f\" % (k, 100 * (v / total_samples)) for k, v in samples_right.items()])\n",
    "        if it % self.update_f == 0:\n",
    "            print(\"%d/%d of epoch %d/%d %s - acc %s [bs:%d]\" % (it, iters, self.current_epoch, self.max_epochs, loss_string,\n",
    "                                                                acc_string, total_samples))\n",
    "            # update tf log\n",
    "            if self.tf_logger:\n",
    "                for k, v in losses.items(): self.tf_logger.scalar_summary(\"train/loss_%s\" % k, v, self.current_iter)\n",
    "\n",
    "    def _clean_epoch_stats(self):\n",
    "        self.epoch_stats = {}\n",
    "        self.total = 0\n",
    "\n",
    "    def log_test(self, phase, accuracies):\n",
    "        print(\"Accuracies on %s: \" % phase + \", \".join([\"%s : %.2f\" % (k, v * 100) for k, v in accuracies.items()]))\n",
    "        if self.tf_logger:\n",
    "            for k, v in accuracies.items(): self.tf_logger.scalar_summary(\"%s/acc_%s\" % (phase, k), v, self.current_iter)\n",
    "\n",
    "    def save_best(self, val_test, best_test):\n",
    "        print(\"It took %g\" % (time() - self.start_time))\n",
    "        if self.tf_logger:\n",
    "            for x in range(10):\n",
    "                self.tf_logger.scalar_summary(\"best/from_val_test\", val_test, x)\n",
    "                self.tf_logger.scalar_summary(\"best/max_test\", best_test, x)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_name_from_args(args):\n",
    "        folder_name = \"%s_to_%s\" % (\"-\".join(sorted(args.source)), args.target)\n",
    "        if args.folder_name:\n",
    "            folder_name = join(args.folder_name, folder_name)\n",
    "        name = \"eps%d_bs%d_lr%g_class%d_jigClass%d_jigWeight%g\" % (args.epochs, args.batch_size, args.learning_rate, args.n_classes,\n",
    "                                                                   args.jigsaw_n_classes, args.jig_weight)\n",
    "        # if args.ooo_weight > 0:\n",
    "        #     name += \"_oooW%g\" % args.ooo_weight\n",
    "        if args.train_all:\n",
    "            name += \"_TAll\"\n",
    "        if args.bias_whole_image:\n",
    "            name += \"_bias%g\" % args.bias_whole_image\n",
    "        if args.classify_only_sane:\n",
    "            name += \"_classifyOnlySane\"\n",
    "        if args.TTA:\n",
    "            name += \"_TTA\"\n",
    "        try:\n",
    "            name += \"_entropy%g_jig_tW%g\" % (args.entropy_weight, args.target_weight)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        if args.suffix:\n",
    "            name += \"_%s\" % args.suffix\n",
    "        name += \"_%d\" % int(time() % 1000)\n",
    "        return folder_name, name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c630175",
   "metadata": {},
   "source": [
    "## Fichier principal train_jigsaw.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1303518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, args, device):\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        model = get_network(args.network)(jigsaw_classes=args.jigsaw_n_classes + 1, classes=args.n_classes)\n",
    "        self.model = model.to(device)\n",
    "        # print(self.model)\n",
    "        self.source_loader, self.val_loader = get_train_dataloader(args, patches=model.is_patch_based())\n",
    "        self.target_loader = get_val_dataloader(args, patches=model.is_patch_based())\n",
    "        self.test_loaders = {\"val\": self.val_loader, \"test\": self.target_loader}\n",
    "        self.len_dataloader = len(self.source_loader)\n",
    "        print(\"Dataset size: train %d, val %d, test %d\" % (len(self.source_loader.dataset), len(self.val_loader.dataset), len(self.target_loader.dataset)))\n",
    "        self.optimizer, self.scheduler = get_optim_and_scheduler(model, args.epochs, args.learning_rate, args.train_all, nesterov=args.nesterov)\n",
    "        self.jig_weight = args.jig_weight\n",
    "        self.only_non_scrambled = args.classify_only_sane\n",
    "        self.n_classes = args.n_classes\n",
    "        if args.target in args.source:\n",
    "            self.target_id = args.source.index(args.target)\n",
    "            print(\"Target in source: %d\" % self.target_id)\n",
    "            print(args.source)\n",
    "        else:\n",
    "            self.target_id = None\n",
    "\n",
    "    def _do_epoch(self):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        self.model.train()\n",
    "        for it, ((data, jig_l, class_l), d_idx) in enumerate(self.source_loader):\n",
    "            data, jig_l, class_l, d_idx = data.to(self.device), jig_l.to(self.device), class_l.to(self.device), d_idx.to(self.device)\n",
    "            # absolute_iter_count = it + self.current_epoch * self.len_dataloader\n",
    "            # p = float(absolute_iter_count) / self.args.epochs / self.len_dataloader\n",
    "            # lambda_val = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "            # if domain_error > 2.0:\n",
    "            #     lambda_val  = 0\n",
    "            # print(\"Shutting down LAMBDA to prevent implosion\")\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            jigsaw_logit, class_logit = self.model(data)  # , lambda_val=lambda_val)\n",
    "            jigsaw_loss = criterion(jigsaw_logit, jig_l)\n",
    "            # domain_loss = criterion(domain_logit, d_idx)\n",
    "            # domain_error = domain_loss.item()\n",
    "            if self.only_non_scrambled:\n",
    "                if self.target_id is not None:\n",
    "                    idx = (jig_l == 0) & (d_idx != self.target_id)\n",
    "                    class_loss = criterion(class_logit[idx], class_l[idx])\n",
    "                else:\n",
    "                    class_loss = criterion(class_logit[jig_l == 0], class_l[jig_l == 0])\n",
    "\n",
    "            elif self.target_id:\n",
    "                class_loss = criterion(class_logit[d_idx != self.target_id], class_l[d_idx != self.target_id])\n",
    "            else:\n",
    "                class_loss = criterion(class_logit, class_l)\n",
    "            _, cls_pred = class_logit.max(dim=1)\n",
    "            _, jig_pred = jigsaw_logit.max(dim=1)\n",
    "            # _, domain_pred = domain_logit.max(dim=1)\n",
    "            loss = class_loss + jigsaw_loss * self.jig_weight  # + 0.1 * domain_loss\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.logger.log(it, len(self.source_loader),\n",
    "                            {\"jigsaw\": jigsaw_loss.item(), \"class\": class_loss.item()  # , \"domain\": domain_loss.item()\n",
    "                             },\n",
    "                            # ,\"lambda\": lambda_val},\n",
    "                            {\"jigsaw\": torch.sum(jig_pred == jig_l.data).item(),\n",
    "                             \"class\": torch.sum(cls_pred == class_l.data).item(),\n",
    "                             # \"domain\": torch.sum(domain_pred == d_idx.data).item()\n",
    "                             },\n",
    "                            data.shape[0])\n",
    "            del loss, class_loss, jigsaw_loss, jigsaw_logit, class_logit\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for phase, loader in self.test_loaders.items():\n",
    "                total = len(loader.dataset)\n",
    "                if loader.dataset.isMulti():\n",
    "                    jigsaw_correct, class_correct, single_acc = self.do_test_multi(loader)\n",
    "                    print(\"Single vs multi: %g %g\" % (float(single_acc) / total, float(class_correct) / total))\n",
    "                else:\n",
    "                    jigsaw_correct, class_correct = self.do_test(loader)\n",
    "                jigsaw_acc = float(jigsaw_correct) / total\n",
    "                class_acc = float(class_correct) / total\n",
    "                self.logger.log_test(phase, {\"jigsaw\": jigsaw_acc, \"class\": class_acc})\n",
    "                self.results[phase][self.current_epoch] = class_acc\n",
    "\n",
    "    def do_test(self, loader):\n",
    "        jigsaw_correct = 0\n",
    "        class_correct = 0\n",
    "        domain_correct = 0\n",
    "        for it, ((data, jig_l, class_l), _) in enumerate(loader):\n",
    "            data, jig_l, class_l = data.to(self.device), jig_l.to(self.device), class_l.to(self.device)\n",
    "            jigsaw_logit, class_logit = self.model(data)\n",
    "            _, cls_pred = class_logit.max(dim=1)\n",
    "            _, jig_pred = jigsaw_logit.max(dim=1)\n",
    "            class_correct += torch.sum(cls_pred == class_l.data)\n",
    "            jigsaw_correct += torch.sum(jig_pred == jig_l.data)\n",
    "        return jigsaw_correct, class_correct\n",
    "\n",
    "    def do_test_multi(self, loader):\n",
    "        jigsaw_correct = 0\n",
    "        class_correct = 0\n",
    "        single_correct = 0\n",
    "        for it, ((data, jig_l, class_l), d_idx) in enumerate(loader):\n",
    "            data, jig_l, class_l = data.to(self.device), jig_l.to(self.device), class_l.to(self.device)\n",
    "            n_permutations = data.shape[1]\n",
    "            class_logits = torch.zeros(n_permutations, data.shape[0], self.n_classes).to(self.device)\n",
    "            for k in range(n_permutations):\n",
    "                class_logits[k] = F.softmax(self.model(data[:, k])[1], dim=1)\n",
    "            class_logits[0] *= 4 * n_permutations  # bias more the original image\n",
    "            class_logit = class_logits.mean(0)\n",
    "            _, cls_pred = class_logit.max(dim=1)\n",
    "            jigsaw_logit, single_logit = self.model(data[:, 0])\n",
    "            _, jig_pred = jigsaw_logit.max(dim=1)\n",
    "            _, single_logit = single_logit.max(dim=1)\n",
    "            single_correct += torch.sum(single_logit == class_l.data)\n",
    "            class_correct += torch.sum(cls_pred == class_l.data)\n",
    "            jigsaw_correct += torch.sum(jig_pred == jig_l.data[:, 0])\n",
    "        return jigsaw_correct, class_correct, single_correct\n",
    "\n",
    "    def do_training(self):\n",
    "        self.logger = Logger(self.args, update_frequency=30)  # , \"domain\", \"lambda\"\n",
    "        self.results = {\"val\": torch.zeros(self.args.epochs), \"test\": torch.zeros(self.args.epochs)}\n",
    "        for self.current_epoch in range(self.args.epochs):\n",
    "            self.scheduler.step()\n",
    "            self.logger.new_epoch(self.scheduler.get_lr())\n",
    "            self._do_epoch()\n",
    "        val_res = self.results[\"val\"]\n",
    "        test_res = self.results[\"test\"]\n",
    "        idx_best = val_res.argmax()\n",
    "        #print(\"Best val %g, corresponding test %g - best test: %g\" % (val_res.max(), test_res[idx_best], test_res.max()))\n",
    "        self.logger.save_best(test_res[idx_best], test_res.max())\n",
    "        return self.logger, self.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e569807e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ResNet-50\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/goulm/Bureau/JigenDG-master/data/txt_lists/art painting_train.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m Trainer(args, device)\n\u001b[1;32m     10\u001b[0m     trainer\u001b[38;5;241m.\u001b[39mdo_training()\n\u001b[0;32m---> 12\u001b[0m main(args)\n",
      "Cell \u001b[0;32mIn[47], line 9\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m      7\u001b[0m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39mbenchmark \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      8\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(args, device)\n\u001b[1;32m     10\u001b[0m trainer\u001b[38;5;241m.\u001b[39mdo_training()\n",
      "Cell \u001b[0;32mIn[42], line 8\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, args, device)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# print(self.model)\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_loader, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_loader \u001b[38;5;241m=\u001b[39m get_train_dataloader(args, patches\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mis_patch_based())\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_loader \u001b[38;5;241m=\u001b[39m get_val_dataloader(args, patches\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mis_patch_based())\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_loaders \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_loader, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_loader}\n",
      "Cell \u001b[0;32mIn[38], line 55\u001b[0m, in \u001b[0;36mget_train_dataloader\u001b[0;34m(args, patches)\u001b[0m\n\u001b[1;32m     53\u001b[0m limit \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mlimit_source\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dname \u001b[38;5;129;01min\u001b[39;00m dataset_list:\n\u001b[0;32m---> 55\u001b[0m     name_train, name_val, labels_train, labels_val \u001b[38;5;241m=\u001b[39m get_split_dataset_info(join(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mabspath(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/txt_lists\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_train.txt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m dname), args\u001b[38;5;241m.\u001b[39mval_size)\n\u001b[1;32m     56\u001b[0m     train_dataset \u001b[38;5;241m=\u001b[39m JigsawDataset(name_train, labels_train, patches\u001b[38;5;241m=\u001b[39mpatches, img_transformer\u001b[38;5;241m=\u001b[39mimg_transformer,\n\u001b[1;32m     57\u001b[0m                                   tile_transformer\u001b[38;5;241m=\u001b[39mtile_transformer, jig_classes\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mjigsaw_n_classes, bias_whole_image\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbias_whole_image)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m limit:\n",
      "Cell \u001b[0;32mIn[36], line 37\u001b[0m, in \u001b[0;36mget_split_dataset_info\u001b[0;34m(txt_list, val_percentage)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_split_dataset_info\u001b[39m(txt_list, val_percentage):\n\u001b[0;32m---> 37\u001b[0m     names, labels \u001b[38;5;241m=\u001b[39m _dataset_info(txt_list)\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_random_subset(names, labels, val_percentage)\n",
      "Cell \u001b[0;32mIn[36], line 23\u001b[0m, in \u001b[0;36m_dataset_info\u001b[0;34m(txt_labels)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_dataset_info\u001b[39m(txt_labels):\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(txt_labels, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     24\u001b[0m         images_list \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m     26\u001b[0m     file_names \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/usr/local/insa/anaconda/lib/python3.11/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/goulm/Bureau/JigenDG-master/data/txt_lists/art painting_train.txt'"
     ]
    }
   ],
   "source": [
    "# main du code python\n",
    "\n",
    "args = Args([\"art painting\", \"photo\"], \"sketch\")\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    trainer = Trainer(args, device)\n",
    "    trainer.do_training()\n",
    "\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8703445e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "416d1ada",
   "metadata": {},
   "source": [
    "# Base de données PACS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa5545e",
   "metadata": {},
   "source": [
    "## Présentation de la base de données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167a5e8a",
   "metadata": {},
   "source": [
    "Le dataset PACS est une base de données qui est souvent utilisée  pour évaluer les performances d'algorithmes de machine learning sur des tâches de généralisation de domaine.\n",
    "\n",
    "Il comporte en tout 19982 images, de quatres domaines différents :\n",
    "\n",
    "- *Photo* : des photographies réelles,\n",
    "- *Art Painting* : des oeuvres d'art en peinture,\n",
    "- *Cartoon* : des dessins,\n",
    "- *Sketch* : des esquisses en noir et blanc.\n",
    "\n",
    "De plus, le dataset comporte sept catégories, c'est à dire que les sujets des images (de n'importe quel domaine) sont soit :\n",
    "- des chiens,\n",
    "- des élephants,\n",
    "- des girafes,\n",
    "- des guitares,\n",
    "- des cheveaux,\n",
    "- des maisons,\n",
    "- ou des humains."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8562507a",
   "metadata": {},
   "source": [
    "Des exemples issus du dataset PACS sont présentés ci-dessous, pour chaque domaine et pour chaque catégorie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3121f1",
   "metadata": {},
   "source": [
    "![images exemple](exemples.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764f7e27",
   "metadata": {},
   "source": [
    "## Importation des échantillons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbe7f03",
   "metadata": {},
   "source": [
    "Pour importer les images de la base de données, on utilise *deeplake*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "844efaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/goulm/.local/lib/python3.11/site-packages/deeplake/util/check_latest_version.py:32: UserWarning: A newer version of deeplake (3.8.14) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import deeplake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74c5013",
   "metadata": {},
   "source": [
    "Trois échantillons ont déjà été formés :\n",
    "- un échantillon d'apprentissage avec 8977 images (45 % de l'ensemble des données)\n",
    "- un échantillon de validation avec 1014 images (5 % de l'ensemble des données)\n",
    "- un échantillon de test avec 9991 images (50 % de l'ensemble des données)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "50e767f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening dataset in read-only mode as you don't have write permissions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/pacs-train\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://activeloop/pacs-train loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening dataset in read-only mode as you don't have write permissions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/pacs-val\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\\"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://activeloop/pacs-val loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "-"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening dataset in read-only mode as you don't have write permissions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/pacs-test\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hub://activeloop/pacs-test loaded successfully.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      " \r\n",
      "\r\n",
      "\r"
     ]
    }
   ],
   "source": [
    "df_train = deeplake.load(\"hub://activeloop/pacs-train\")\n",
    "df_val = deeplake.load(\"hub://activeloop/pacs-val\")\n",
    "df_test = deeplake.load(\"hub://activeloop/pacs-test\")\n",
    "#df_train.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7f57cd8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset(path='hub://activeloop/pacs-train', read_only=True, tensors=['images', 'labels', 'domains'])\n",
      "\n",
      " tensor      htype            shape          dtype  compression\n",
      " -------    -------          -------        -------  ------- \n",
      " images      image     (8977, 227, 227, 3)   uint8    jpeg   \n",
      " labels   class_label       (8977, 1)       uint32    None   \n",
      " domains  class_label       (8977, 1)       uint32    None   \n",
      "Dataset(path='hub://activeloop/pacs-val', read_only=True, tensors=['images', 'labels', 'domains'])\n",
      "\n",
      " tensor      htype            shape          dtype  compression\n",
      " -------    -------          -------        -------  ------- \n",
      " images      image     (1014, 227, 227, 3)   uint8    jpeg   \n",
      " labels   class_label       (1014, 1)       uint32    None   \n",
      " domains  class_label       (1014, 1)       uint32    None   \n",
      "Dataset(path='hub://activeloop/pacs-test', read_only=True, tensors=['images', 'labels', 'domains'])\n",
      "\n",
      " tensor      htype            shape          dtype  compression\n",
      " -------    -------          -------        -------  ------- \n",
      " images      image     (9991, 227, 227, 3)   uint8    jpeg   \n",
      " labels   class_label       (9991, 1)       uint32    None   \n",
      " domains  class_label       (9991, 1)       uint32    None   \n"
     ]
    }
   ],
   "source": [
    "df_train.summary()\n",
    "df_val.summary()\n",
    "df_test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e9a352",
   "metadata": {},
   "source": [
    "## Structure de la base de données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae4dc37",
   "metadata": {},
   "source": [
    "Pour chaque index, on a trois élements :\n",
    "- un tenseur qui contient une image en couleur de taille $227 \\times 227$,\n",
    "- un tenseur qui représente le label de cette image par un chiffre entre 0 et 6,\n",
    "- un tenseur qui indique son domaine avec un chiffre entre 0 et 4.\n",
    "\n",
    "Les correspondances entre les valeurs et les noms des labels et des domaines peuvent être affichées comme suit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe6503f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Labels  :\", dict(zip(np.unique(df_train.labels.data()['value']), np.unique(df_train.labels.data()['text']))))\n",
    "print(\"Domains :\", dict(zip(np.unique(df_train.domains.data()['value']), np.unique(df_train.domains.data()['text']))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fe44bb",
   "metadata": {},
   "source": [
    "## Répartition des données"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2c827a",
   "metadata": {},
   "source": [
    "Il est possible d'étudier la répartition des données à l'intérieur de chacun des trois échantillons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42322d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "@deeplake.compute\n",
    "def filter_elem(sample_in, elem, labels_or_domains):\n",
    "    \n",
    "    return sample_in[labels_or_domains].data()['value'][0]==elem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9f90d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_p=np.array([[round(100*len(df.filter(filter_elem(label, 'labels')))/len(df),2) for label in range(7)] for df in [df_train, df_val, df_test]])\n",
    "pd.DataFrame(labels_p, columns=['label {}'.format(i) for i in range(7)], index=['Train', 'Validation', 'Test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b13388f",
   "metadata": {},
   "source": [
    "Le tableau ci-dessus présente les pourcentages d'images de chaque catégorie dans chacun des échantillons : par exemple, 17.32% des images de l'échantillon d'apprentissage représentent des chiens (label 0).\n",
    "\n",
    "Ce tableau met donc en évidence que les données sont réparties relativement équitablement entre les sept catégories. Les images les plus présentes sont celles de chiens et d'élephants (labels 0 et 1), alors qu'on a un peu moins d'images de maisons (label 5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ac1ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_p=np.array([[round(100*len(df.filter(filter_elem(domain, 'domains')))/len(df),2) for domain in range(4)] for df in [df_train, df_val, df_test]])\n",
    "pd.DataFrame(domains_p, columns=['domain {}'.format(i) for i in range(4)], index=['Train', 'Validation', 'Test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7648d852",
   "metadata": {},
   "source": [
    "Le tableau ci-dessus présente les pourcentages d'images de chaque domaine pour tous les échantillons. On observe qu'il y a plus de croquis (domain 3) que de photographies (domain 0) mais que les données restent bien équilibrées."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920c8da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_train = df_train.tensorflow()\n",
    "dataloader_val = df_val.tensorflow()\n",
    "dataloader_test = df_test.tensorflow()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9ff13a",
   "metadata": {},
   "source": [
    "# Plan d'éxpérience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098ca1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools as it\n",
    "\n",
    "train = [['art_painting', 'cartoon'], ['art_painting', 'sketch'], ['photo']]\n",
    "test = ['']\n",
    "\n",
    "#Labels  : {0: 'dog', 1: 'elephant', 2: 'giraffe', 3: 'guitar', 4: 'horse', 5: 'house', 6: 'person'}\n",
    "#Domains : {0: 'art_painting', 1: 'cartoon', 2: 'photo', 3: 'sketch'}\n",
    "\n",
    "\n",
    "\n",
    "experiences = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d236e41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfexp= pd.DataFrame(list(it.product(*experiences.values())),columns=experiences.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b558550",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfexp)"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Louison Bocquet--Nouaille"
   },
   {
    "name": "Pierre-Alain Goulm"
   },
   {
    "name": "Romain Tiphaigne"
   },
   {
    "name": "Axelle Tragné"
   },
   {
    "name": "Alicia Zady"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "title": "Domain Generalization by Solving Jigsaw Puzzles",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "355.8px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
