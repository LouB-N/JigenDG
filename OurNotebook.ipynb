{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eeaf7c44",
   "metadata": {},
   "source": [
    "Ce notebook a comme source principale [Carlucci et al. (2019)](https://arxiv.org/pdf/1903.06864.pdf).\n",
    "Nous commençons par expliquer les méthodes et les procédures en détail avant de les appliquer à un ensemble de données différent de celui utilisé dans l'article (PACS)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05aedb4",
   "metadata": {},
   "source": [
    "# Méthodes et procédures utilisées pour la généralisation de domaine en résolvant des casse-têtes/puzzles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e35dfe",
   "metadata": {},
   "source": [
    "## Généralisation de domaine\n",
    "\n",
    "La généralisation de domaine fait référence à la capacité d'un modèle de machine learning à généraliser ses capacités sur des domaines non vus ou des données hors distribution. Cela s'oppose à l'apprentissage supervisé traditionnel, qui suppose que les données d'entraînement et de test proviennent du même domaine ou de la même distribution. [Wang et al. (2022)]\n",
    "\n",
    "Le modèle est entraîné sur différents domaines, les données sources (si l'on considère un style d'image, cela pourrait être un dessin, une peinture, un dessin animé, etc.). Nous voulons que le modèle soit capable de prédire avec précision la classe dans un domaine non vu, les données cibles (par exemple, des photos).\n",
    "\n",
    "*Définition 1 (Domaine).*\n",
    "Soit X un espace d'entrée non vide (input) et Y un espace de sortie (output). Un domaine est composé de données échantillonnées à partir d'une distribution. Nous le notons comme $\\mathcal{S} = {(x_i, y_i)}^n_{i=1}∼P_{XY}$ , où $x \\in \\mathcal{X} \\subset \\mathbb{R}^d$, $y \\in \\mathcal{Y} \\subset \\mathbb{R}$ représente le label, et $P_{XY}$ représente la distribution conjointe de l'échantillon d'entrée et du label de sortie. X et Y désignent les variables aléatoires correspondantes. [Wang et al. (2022)]\n",
    "\n",
    "*Définition 2 (Généralisation de domaine).*\n",
    "En généralisation de domaine, on pose M domaines d'entraînement (source) $S_{train} = {S^i | i = 1, ..., M }$, où $S^i = {(x^i_j , y^i_j )}^{n_i}{j=1}$ désigne le i-ème domaine. Les distributions conjointes entre chaque paire de domaines sont différentes : $P^i{XY} \\neq P^j_{XY}$, $1 \\leq i \\neq j \\leq M$. L'objectif de la généralisation de domaine est d'apprendre une fonction prédictive robuste et généralisable $h : X → Y$ à partir des M domaines d'entraînement pour obtenir une erreur de prédiction minimale sur un domaine de test non vu $S_{test}$ (c'est-à-dire, $S_{test}$ n'est pas accessible lors de l'entraînement et $P^{test}{XY} \\neq P^i{XY}$ pour i ∈ {1, ..., M}):\n",
    "$$ min_h \\mathcal{E}(x,y) \\in S_{test} [\\mathcal{l}(h(x), y)] $$\n",
    "où $\\mathcal{l}(·, ·)$ est la fonction de perte. [Wang et al. (2022)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d08c2e2",
   "metadata": {},
   "source": [
    "## JiGENDG\n",
    "L'algorithme repose sur l'idée d'utiliser des casse-têtes/puzzles pour entraîner un modèle à être invariant sur différents domaines. Le réseau apprend simultanément à résoudre les casse-têtes et à classifier les images. [Carlucci et al. (2019)]\n",
    "\n",
    "\n",
    "### Dataset\n",
    "Les données d'entrée sont un ensemble de N images provenant de S domaines. Dans chaque domaine i, nous avons $N_i$ observations labellisées. Nous écrivons $\\left{ x^i_j, y^i_j \\right} _{j=1}^{N_i}$, ce qui signifie que pour la j-ème image du i-ème domaine $x^i_j$, le label associé est $y^i_j$.\n",
    "\n",
    "Nous avons $x^i_j \\in \\mathbb{R}^{n_p \\times n_p}$ où $n_p \\times n_p$ est la taille des images en pixels, en supposant que les images sont des carrés. Nous avons $y^i_j \\in \\mathbb{R}^{C}$ où C est le nombre de classes, car l'étiquette $y^i_j$ est encodée en one-hot.\n",
    "\n",
    "En termes de dimensions, $\\left{x^i_j, y^i_j\\right}_{j=1}^{N_i} \\in \\left( \\left( \\mathbb{R}^{n_p \\times n_p} \\times \\mathbb{R}^{C} \\right) ^ {N_i} \\right) ^ {S}$ où $N_i \\times S \\leq N$ car le nombre d'images étiquetées $N_i \\times S$ ne dépasse pas le nombre total d'images $N$.\n",
    "\n",
    "\n",
    "### Dataset permuté\n",
    "À partir de l'ensemble de données non permuté, nous créons un nouveau jeu de données utilisé pour la tâche de résolution de casse-têtes. Nous considérons des permutations sur une grille $n \\times n$ (dans l'article et notre travail, nous fixons $n=3$).\n",
    "\n",
    "Bien que nous ayons un total de $n^2!$ permutations possibles, nous n'en considérons que P. Nous les choisissons en fonction de la distance de Hamming, ce qui signifie que nous ne conservons que celles avec le moins de différences de position. Cela permet de simplifier un peu la tâche et également de réduire le temps d'inférence (utiliser les $n^2!=362 880$ possibilités serait beaucoup plus chronophage que d'utiliser $P=30$ permutations).\n",
    "\n",
    "La non-permutation est toujours incluse dans le sous-ensemble des permutations P.\n",
    "\n",
    "Chaque permutation possible est associée à un indice qui permet de traiter le problème comme une tâche de classification où l'étiquette est un vecteur encodé en one-hot des indices des permutations.\n",
    "\n",
    "Nous notons $\\left\\{z^i_k, p^i_k\\right\\}_{k=1}^{K_i} \\in \\left( \\left( \\mathbb{R}^{n_p \\times n_p} \\times \\mathbb{R}^{P} \\right) ^ {K_i} \\right) ^ {S}$ où $z^i_k$ est l'image permutée, $p^i_k$ est l'indice de la permutation utilisée sur l'image associée, $K_i$ est le nombre d'instances étiquetées et $P$ est le nombre de permutations considérées.\n",
    "\n",
    "\n",
    "\n",
    "###  Fonction de perte\n",
    "Rappelons brièvement comment un réseau de neurones est entraîné :\n",
    "- Le modèle traite un batch de $b$ échantillons d'entrée. Chaque échantillon passe à travers le réseau, et la sortie est calculée.\n",
    "- La fonction de perte est appliquée à la sortie prédite et aux valeurs cibles pour le lot. Cette perte représente la dissimilarité entre les valeurs prédites et réelles.\n",
    "- Les gradients cumulés de la perte sont calculés par rapport à chaque paramètre.\n",
    "- Les paramètres du modèle sont mis à jour en fonction des gradients calculés. Le taux d'apprentissage $\\eta$ contrôle dans quelle mesure les paramètres du modèle changent dans la direction qui minimise la perte.\n",
    "\n",
    "Ce processus est répété pendant $E$ époques. Chaque époque implique le traitement de l'ensemble du jeu de données.\n",
    "\n",
    "Les batches sont composés d'un mélange d'images ordonnées et mélangées. Le ratio est défini par $\\beta$ : pour $\\beta=0.75$, 75 % du lot est composé d'images ordonnées et le reste d'images mélangées. Si nous avons un batch de taille $b=128$, cela signifierait que nous avons $N_i=0.75\\times128=96$ et $K_i=(1-0.75)\\times128=32$.\n",
    "\n",
    "Dans JiGen, la fonction de perte prend une forme particulière car deux tâches sont apprises.\n",
    "\n",
    "\n",
    "#### Cas supervisé\n",
    "Nous cherchons à optimiser les paramètres avec le problème de minimisation suivant :\n",
    "\n",
    "$$ argmin_{\\theta_f, \\theta_p, \\theta_c} \\sum_{i=1}^{S} \\sum_{j=1}^{N_i} \\mathcal{L}_c \\left( h(x^i_j|\\theta_f, \\theta_c), y^i_j\\right) + \\sum_{k=1}^{K_i} \\alpha \\mathcal{L}_p \\left( h(z^i_k|\\theta_f, \\theta_p), p^i_k\\right) $$\n",
    "\n",
    "- $\\mathcal{L}_c$ est une perte (la cross-entropy) pour la tâche de classification d'image. Nous rappelons que $\\mathcal{L}c \\left( h(x^i_j|\\theta_f, \\theta_c), y^i_j\\right) = - \\sum{c \\in C} y^i_j \\log(\\mathbb{P}(h(x^i_j|\\theta_f, \\theta_c)=c))$ ;\n",
    "- $\\mathcal{L}_p$ est une perte (la cross-entropy) pour la tâche de résolution de casse-têtes ;\n",
    "- $\\alpha$ est le poids de la perte pour le casse-tête (l'importance que nous accordons à la tâche de résolution de casse-tête par rapport à celle de la tâche de classification) ;\n",
    "- $h$ est la fonction d'activation du modèle profond (deep model), elle prédit le label ;\n",
    "- $\\theta_f$ est l'ensemble des paramètres (poids et biais) pour la couche entièrement connectée (fully connected layer);\n",
    "- $\\theta_p$ est l'ensemble des paramètres pour la dernière couche entièrement connectée dédiée à la reconnaissance de permutation ;\n",
    "- $\\theta_c$ est l'ensemble des paramètres pour la couche de convolution.\n",
    "\n",
    "La perte du casse-tête $\\mathcal{L}_p$ est calculée sur l'image ordonnée, mais la perte de classification $\\mathcal{L}_c$ n'est pas calculée sur les images mélangées car cela rendrait la reconnaissance d'objets plus difficile.\n",
    "\n",
    "\n",
    "#### Unsupervised case\n",
    "JiGen a été conçu dans le but de la généralisation de domaine non supervisée. La seule différence avec JiGen dans le cas supervisé réside dans la perte pour la tâche de classification d'image :\n",
    "\n",
    "$$ argmin_{\\theta_f, \\theta_p, \\theta_c} \\sum_{i=1}^{S} \\mathcal{L}_E (x^i) + \\sum_{k=1}^{K_i} \\alpha \\mathcal{L}_p \\left( h(z^i_k|\\theta_f, \\theta_p), p^i_k\\right) $$\n",
    "\n",
    "avec $\\mathcal{L}E (x^i) = \\sum{y \\in \\mathcal{Y}} h(x^i|\\theta_f, \\theta_c) \\log(h(x^i|\\theta_f, \\theta_c))$, la cross-entropy empirique.\n",
    "\n",
    "Remarque : la somme $\\sum_{j=1}^{N_i}$ disparaît car nous considérons toutes les images et non pas seulement celles étiquetées.\n",
    "\n",
    "\n",
    "### Test\n",
    "Pour tester le modèle, nous ne considérons que la partie classification du réseau : nous n'utilisons pas la couche entièrement connectée finale qui sert à la résolution de casse-tête. Cela revient à fixer $\\alpha=0$.\n",
    "\n",
    "\n",
    "### Parameters\n",
    "Pour toutes les expériences, nous préciserons clairement les valeurs des caractéristiques de l'ensemble de données : les tailles des images $n_b$, le nombre d'images $N$, et le nombre de classes $C$.\n",
    "\n",
    "Pour tous les paramètres, nous considérerons les mêmes paramètres pour le casse-tête : la taille de la grille $n$, le nombre de permutations considérées $P$, et le biais des données $\\beta$. Les auteurs choisissent ces paramètres avec une validation croisée sur 10 % de l'ensemble de données, pour chaque expérience.\n",
    "\n",
    "Nous fixerons les paramètres d'expérience : la taille des lots $b$, le nombre d'époques $E$, le taux d'apprentissage $\\eta$, et le poids du casse-tête $\\alpha$. (Les auteurs ont fixé ...)\n",
    "\n",
    "Les paramètres du modèle optimisés par rétropropagation et non choisis par l'utilisateur sont $\\theta_f$, $\\theta_p$, et $\\theta_c$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d617ee3b",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1928832",
   "metadata": {},
   "source": [
    "**[Carlucci et al. (2019)]** Carlucci, F. M., D'Innocente, A., Bucci, S., Caputo, B., & Tommasi, T. (2019). Domain Generalization by Solving Jigsaw Puzzles. arXiv preprint arXiv:1903.06864. [URL](https://arxiv.org/pdf/1903.06864.pdf)\n",
    "\n",
    "**[Wang et al. (2022)]** Wang, J., Lan, C., Liu, C., Ouyang, Y., Qin, T., Lu, W., Chen, Y., Zeng, W., & Yu, P. S. (2022). Generalizing to Unseen Domains: A Survey on Domain Generalization. arXiv preprint arXiv:2103.03097. [URL](https://arxiv.org/pdf/2103.03097.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5083e865",
   "metadata": {},
   "source": [
    "# Using JiGen on PACS (as in the article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16989a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stocker le dossier PACS au même endroit que ce notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "415285ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 14:41:37.040169: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from IPython.core.debugger import set_trace\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torch.autograd import Function\n",
    "from torchvision.models.resnet import BasicBlock,Bottleneck\n",
    "import torch.utils.data as data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torch import optim\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import join, dirname\n",
    "from collections import OrderedDict\n",
    "from itertools import chain\n",
    "from PIL import Image\n",
    "from random import sample, random\n",
    "import bisect\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "import scipy.misc \n",
    "try:\n",
    "    from StringIO import StringIO  # Python 2.7\n",
    "except ImportError:\n",
    "    from io import BytesIO         # Python 3.x\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0747db0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commentaires sur le choix du réseau :\n",
    "# resnet18 fonctionne, nécessite image_size=222\n",
    "# resnet50 fonctionne pour image_size=222\n",
    "# alexnet ne fonctionne pas, a un argument en trop avec jigsaw_classes (??)\n",
    "# caffenet ne fonctionne pas, car a besoin du réseau pré-entrainé\n",
    "# lenet ne fonctionne pas, on doit surment trouver la valeur de image_size qui convient\n",
    "\n",
    "\n",
    "class Args:\n",
    "    source = ['photo','cartoon','sketch']\n",
    "    target = 'art_painting'\n",
    "    batch_size = 64\n",
    "    image_size = 222              # 222 si resnet18\n",
    "    \n",
    "    min_scale = 0.8               # Minimum scale percent\n",
    "    max_scale = 1.0               # Maximum scale percent\n",
    "    random_horiz_flip = 0.0       # Chance of random horizontal flip\n",
    "    jitter = 0.0                  # Color jitter amount\n",
    "    tile_random_grayscale = 0.1   # Chance of randomly greyscaling a tile\n",
    "    \n",
    "    limit_source = None     # If set, it will limit the number of training samples\n",
    "    limit_target = None     # If set, it will limit the number of testing samples\n",
    "    \n",
    "    learning_rate = 0.01\n",
    "    epochs = 5\n",
    "    n_classes = 7              # Number of classes for object prediction\n",
    "    jigsaw_n_classes = 31       # Number of permutation classes for the puzzle\n",
    "    network = \"resnet50\"        # To choose from : 'caffenet', 'alexnet', 'resnet18', 'resnet50', 'lenet'\n",
    "    jig_weight = 0.7            # Weight for the jigsaw puzzle compared to the classification\n",
    "    ooo_weight = 0              # Weight for odd one out task\n",
    "    tf_logger = True            # If True will save tensorboard compatible logs\n",
    "    val_size = 0.1              # Validation size (between 0 and 1)\n",
    "    folder_name = \"Test\"        # Used by the logger to save logs\n",
    "    bias_whole_image = 0.9      # If set, will bias the training procedure to show more often the whole image\n",
    "    TTA = False                 # Activate test time data augmentation\n",
    "    classify_only_sane = False  # If true, the network will only try to classify the non scrambled images\n",
    "    train_all = True            # If true, all network weights will be trained\n",
    "    suffix = \"\"                 # Suffix for the logger\n",
    "    nesterov = False            # Use nesterov\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2a667a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bda70cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a1443b1",
   "metadata": {},
   "source": [
    "#### Fichiers de /model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "afac8f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common to all networks definition\n",
    "class Id(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Id, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8555331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_utils.py\n",
    "\n",
    "class GradientKillerLayer(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, **kwargs):\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        return None, None\n",
    "\n",
    "\n",
    "class ReverseLayerF(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, lambda_val):\n",
    "        ctx.lambda_val = lambda_val\n",
    "\n",
    "        return x.view_as(x)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        output = grad_output.neg() * ctx.lambda_val\n",
    "\n",
    "        return output, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8620984b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# caffenet\n",
    "\n",
    "\n",
    "class AlexNetCaffe(nn.Module):\n",
    "    def __init__(self, jigsaw_classes=1000, n_classes=100, domains=3, dropout=True):\n",
    "        super(AlexNetCaffe, self).__init__()\n",
    "        print(\"Using Caffe AlexNet\")\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            (\"conv1\", nn.Conv2d(3, 96, kernel_size=11, stride=4)),\n",
    "            (\"relu1\", nn.ReLU(inplace=True)),\n",
    "            (\"pool1\", nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)),\n",
    "            (\"norm1\", nn.LocalResponseNorm(5, 1.e-4, 0.75)),\n",
    "            (\"conv2\", nn.Conv2d(96, 256, kernel_size=5, padding=2, groups=2)),\n",
    "            (\"relu2\", nn.ReLU(inplace=True)),\n",
    "            (\"pool2\", nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)),\n",
    "            (\"norm2\", nn.LocalResponseNorm(5, 1.e-4, 0.75)),\n",
    "            (\"conv3\", nn.Conv2d(256, 384, kernel_size=3, padding=1)),\n",
    "            (\"relu3\", nn.ReLU(inplace=True)),\n",
    "            (\"conv4\", nn.Conv2d(384, 384, kernel_size=3, padding=1, groups=2)),\n",
    "            (\"relu4\", nn.ReLU(inplace=True)),\n",
    "            (\"conv5\", nn.Conv2d(384, 256, kernel_size=3, padding=1, groups=2)),\n",
    "            (\"relu5\", nn.ReLU(inplace=True)),\n",
    "            (\"pool5\", nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True)),\n",
    "        ]))\n",
    "        self.classifier = nn.Sequential(OrderedDict([\n",
    "            (\"fc6\", nn.Linear(256 * 6 * 6, 4096)),\n",
    "            (\"relu6\", nn.ReLU(inplace=True)),\n",
    "            (\"drop6\", nn.Dropout() if dropout else Id()),\n",
    "            (\"fc7\", nn.Linear(4096, 4096)),\n",
    "            (\"relu7\", nn.ReLU(inplace=True)),\n",
    "            (\"drop7\", nn.Dropout() if dropout else Id())]))\n",
    "\n",
    "        self.jigsaw_classifier = nn.Linear(4096, jigsaw_classes)\n",
    "        self.class_classifier = nn.Linear(4096, n_classes)\n",
    "        # self.domain_classifier = nn.Sequential(\n",
    "        #     nn.Linear(256 * 6 * 6, 1024),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(),\n",
    "        #     nn.Linear(1024, 1024),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Dropout(),\n",
    "        #     nn.Linear(1024, domains))\n",
    "\n",
    "    def get_params(self, base_lr):\n",
    "        return [{\"params\": self.features.parameters(), \"lr\": 0.},\n",
    "                {\"params\": chain(self.classifier.parameters(), self.jigsaw_classifier.parameters()\n",
    "                                 , self.class_classifier.parameters()#, self.domain_classifier.parameters()\n",
    "                                 ), \"lr\": base_lr}]\n",
    "\n",
    "    def is_patch_based(self):\n",
    "        return False\n",
    "\n",
    "    def forward(self, x, lambda_val=0):\n",
    "        x = self.features(x*57.6)  #57.6 is the magic number needed to bring torch data back to the range of caffe data, based on used std\n",
    "        x = x.view(x.size(0), -1)\n",
    "        #d = ReverseLayerF.apply(x, lambda_val)\n",
    "        x = self.classifier(x)\n",
    "        return self.jigsaw_classifier(x), self.class_classifier(x)#, self.domain_classifier(d)\n",
    "\n",
    "\n",
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x.view(x.size(0), -1)\n",
    "\n",
    "\n",
    "def caffenet(jigsaw_classes, classes):\n",
    "    model = AlexNetCaffe(jigsaw_classes, classes)\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.xavier_uniform_(m.weight, .1)\n",
    "            nn.init.constant_(m.bias, 0.)\n",
    "\n",
    "    state_dict = torch.load(os.path.join(os.path.abspath(''), \"pretrained/alexnet_caffe.pth.tar\"))\n",
    "    del state_dict[\"classifier.fc8.weight\"]\n",
    "    del state_dict[\"classifier.fc8.bias\"]\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def caffenet_gap(jigsaw_classes, classes):\n",
    "    model = AlexNetCaffe(jigsaw_classes, classes)\n",
    "    state_dict = torch.load(os.path.join(os.path.abspath(''), \"pretrained/alexnet_caffe.pth.tar\"))\n",
    "    del state_dict[\"classifier.fc6.weight\"]\n",
    "    del state_dict[\"classifier.fc6.bias\"]\n",
    "    del state_dict[\"classifier.fc7.weight\"]\n",
    "    del state_dict[\"classifier.fc7.bias\"]\n",
    "    del state_dict[\"classifier.fc8.weight\"]\n",
    "    del state_dict[\"classifier.fc8.bias\"]\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    # weights are initialized in the constructor\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4750f87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# alexnet.py\n",
    "\n",
    "model_urls = {\n",
    "    'alexnet': 'https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth',\n",
    "}\n",
    "\n",
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000, dropout=True):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout() if dropout else Id(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout() if dropout else Id(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def alexnet(classes, pretrained=False):\n",
    "    r\"\"\"AlexNet model architecture from the\n",
    "    `\"One weird trick...\" <https://arxiv.org/abs/1404.5997>`_ paper.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    model = AlexNet(classes, True)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url(model_urls['alexnet']))\n",
    "\n",
    "    model.classifier[-1] = nn.Linear(4096, classes)\n",
    "    nn.init.xavier_uniform_(model.classifier[-1].weight, .1)\n",
    "    nn.init.constant_(model.classifier[-1].bias, 0.)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bbf8cf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet.py\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, jigsaw_classes=1000, classes=100, domains=3):\n",
    "        self.inplanes = 64\n",
    "        super(ResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
    "                               bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n",
    "        self.avgpool = nn.AvgPool2d(7, stride=1)\n",
    "        self.jigsaw_classifier = nn.Linear(512 * block.expansion, jigsaw_classes)\n",
    "        self.class_classifier = nn.Linear(512 * block.expansion, classes)\n",
    "        #self.domain_classifier = nn.Linear(512 * block.expansion, domains)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                          kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def is_patch_based(self):\n",
    "        return False\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.jigsaw_classifier(x),self.class_classifier(x)\n",
    "\n",
    "\n",
    "def resnet18(pretrained=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-18 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    print(\"Using ResNet-18\")\n",
    "    model = ResNet(BasicBlock, [2, 2, 2, 2], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url('https://download.pytorch.org/models/resnet18-5c106cde.pth'), strict=False)\n",
    "        #model.load_state_dict(model_zoo.load_url(model_urls['resnet18']), strict=False)\n",
    "    return model\n",
    "\n",
    "def resnet50(pretrained=True, **kwargs):\n",
    "    \"\"\"Constructs a ResNet-50 model.\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "    \"\"\"\n",
    "    print(\"Using ResNet-50\")\n",
    "    model = ResNet(Bottleneck, [3, 4, 6, 3], **kwargs)\n",
    "    if pretrained:\n",
    "        model.load_state_dict(model_zoo.load_url('https://download.pytorch.org/models/resnet50-19c8e357.pth'), strict=False)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d616cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist.py\n",
    "\n",
    "# built as https://github.com/ricvolpi/generalize-unseen-domains/blob/master/model.py\n",
    "class MnistModel(nn.Module):\n",
    "    def __init__(self, jigsaw_classes=1000, n_classes=100):\n",
    "        super().__init__()\n",
    "        \n",
    "        outfeats = 1024 \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 5),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 5),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(128 * 4 * 4, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "#         outfeats = 100\n",
    "#         self.features = nn.Sequential(\n",
    "#             nn.Conv2d(3, 32, 5),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(2, 2),\n",
    "#             nn.Conv2d(32, 48, 5),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.MaxPool2d(2, 2)\n",
    "#         )\n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(48 * 4 * 4, 100),\n",
    "#             nn.ReLU(True),\n",
    "#             nn.Linear(100, outfeats),\n",
    "#             nn.ReLU(True),\n",
    "#         )\n",
    "        print(\"Using LeNet (%d)\" % outfeats)\n",
    "        self.jigsaw_classifier = nn.Linear(outfeats, jigsaw_classes)\n",
    "        self.class_classifier = nn.Linear(outfeats, n_classes)\n",
    "\n",
    "    def get_params(self, base_lr):\n",
    "        raise \"No pretrained exists for LeNet - use train all\"\n",
    "\n",
    "    def is_patch_based(self):\n",
    "        return False\n",
    "\n",
    "    def forward(self, x, lambda_val=0):\n",
    "        # print(x.shape)\n",
    "        x = self.features(x)\n",
    "        # print(x.shape)\n",
    "        x = self.classifier(x.view(x.size(0), -1))\n",
    "        return self.jigsaw_classifier(x), self.class_classifier(x)\n",
    "\n",
    "\n",
    "def lenet(jigsaw_classes, classes):\n",
    "    model = MnistModel(jigsaw_classes, classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48112cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_factory.py\n",
    "\n",
    "nets_map = {\n",
    "    'caffenet': caffenet,\n",
    "    'alexnet': alexnet,\n",
    "    'resnet18': resnet18,\n",
    "    'resnet50': resnet50,\n",
    "    'lenet': lenet\n",
    "}\n",
    "\n",
    "\n",
    "def get_network(name):\n",
    "    if name not in nets_map:\n",
    "        raise ValueError('Name of network unknown %s' % name)\n",
    "\n",
    "    def get_network_fn(**kwargs):\n",
    "        return nets_map[name](**kwargs)\n",
    "\n",
    "    return get_network_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb555817",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ca89fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "984998b8",
   "metadata": {},
   "source": [
    "#### Fichiers de /data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f2e3fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# StandardDataset.py\n",
    "\n",
    "def get_dataset(path, mode, image_size):\n",
    "    if mode == \"train\":\n",
    "        img_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(image_size, scale=(0.7, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[1/256., 1/256., 1/256.])  # std=[1/256., 1/256., 1/256.] #[0.229, 0.224, 0.225]\n",
    "        ])\n",
    "    else:\n",
    "        img_transform = transforms.Compose([\n",
    "            transforms.Resize(image_size),\n",
    "            # transforms.CenterCrop(image_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], std=[1/256., 1/256., 1/256.])  # std=[1/256., 1/256., 1/256.]\n",
    "        ])\n",
    "    return datasets.ImageFolder(path, transform=img_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d03bdae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# JigsawLoader.py\n",
    "\n",
    "\n",
    "\n",
    "def get_random_subset(names, labels, percent):\n",
    "    \"\"\"\n",
    "\n",
    "    :param names: list of names\n",
    "    :param labels:  list of labels\n",
    "    :param percent: 0 < float < 1\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    samples = len(names)\n",
    "    amount = int(samples * percent)\n",
    "    random_index = sample(range(samples), amount)\n",
    "    name_val = [names[k] for k in random_index]\n",
    "    name_train = [v for k, v in enumerate(names) if k not in random_index]\n",
    "    labels_val = [labels[k] for k in random_index]\n",
    "    labels_train = [v for k, v in enumerate(labels) if k not in random_index]\n",
    "    return name_train, name_val, labels_train, labels_val\n",
    "\n",
    "\n",
    "def _dataset_info(txt_labels):\n",
    "    with open(txt_labels, 'r') as f:\n",
    "        images_list = f.readlines()\n",
    "\n",
    "    file_names = []\n",
    "    labels = []\n",
    "    for row in images_list:\n",
    "        row = row.split(' ')\n",
    "        file_names.append(row[0])\n",
    "        labels.append(int(row[1]))\n",
    "\n",
    "    return file_names, labels\n",
    "\n",
    "\n",
    "def get_split_dataset_info(txt_list, val_percentage):\n",
    "    names, labels = _dataset_info(txt_list)\n",
    "    return get_random_subset(names, labels, val_percentage)\n",
    "\n",
    "\n",
    "class JigsawDataset(data.Dataset):\n",
    "    def __init__(self, names, labels, jig_classes=100, img_transformer=None, tile_transformer=None, patches=True, bias_whole_image=None):\n",
    "        self.data_path = \"\"\n",
    "        self.names = names\n",
    "        self.labels = labels\n",
    "\n",
    "        self.N = len(self.names)\n",
    "        self.permutations = self.__retrieve_permutations(jig_classes)\n",
    "        self.grid_size = 3\n",
    "        self.bias_whole_image = bias_whole_image\n",
    "        if patches:\n",
    "            self.patch_size = 64\n",
    "        self._image_transformer = img_transformer\n",
    "        self._augment_tile = tile_transformer\n",
    "        if patches:\n",
    "            self.returnFunc = lambda x: x\n",
    "        else:\n",
    "            def make_grid(x):\n",
    "                return torchvision.utils.make_grid(x, self.grid_size, padding=0)\n",
    "            self.returnFunc = make_grid\n",
    "\n",
    "    def get_tile(self, img, n):\n",
    "        w = float(img.size[0]) / self.grid_size\n",
    "        y = int(n / self.grid_size)\n",
    "        x = n % self.grid_size\n",
    "        tile = img.crop([x * w, y * w, (x + 1) * w, (y + 1) * w])\n",
    "        tile = self._augment_tile(tile)\n",
    "        return tile\n",
    "    \n",
    "    def get_image(self, index):\n",
    "        framename = self.data_path + self.names[index]\n",
    "        img = Image.open(framename).convert('RGB')\n",
    "        return self._image_transformer(img)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img = self.get_image(index)\n",
    "        n_grids = self.grid_size ** 2\n",
    "        tiles = [None] * n_grids\n",
    "        for n in range(n_grids):\n",
    "            tiles[n] = self.get_tile(img, n)\n",
    "\n",
    "        order = np.random.randint(len(self.permutations) + 1)  # added 1 for class 0: unsorted\n",
    "        if self.bias_whole_image:\n",
    "            if self.bias_whole_image > random():\n",
    "                order = 0\n",
    "        if order == 0:\n",
    "            data = tiles\n",
    "        else:\n",
    "            data = [tiles[self.permutations[order - 1][t]] for t in range(n_grids)]\n",
    "            \n",
    "        data = torch.stack(data, 0)\n",
    "        return self.returnFunc(data), int(order), int(self.labels[index])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.names)\n",
    "\n",
    "    def __retrieve_permutations(self, classes):\n",
    "        all_perm = np.load('permutations_%d.npy' % (classes))\n",
    "        # from range [1,9] to [0,8]\n",
    "        if all_perm.min() == 1:\n",
    "            all_perm = all_perm - 1\n",
    "\n",
    "        return all_perm\n",
    "\n",
    "\n",
    "class JigsawTestDataset(JigsawDataset):\n",
    "    def __init__(self, *args, **xargs):\n",
    "        super().__init__(*args, **xargs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        framename = self.data_path + self.names[index]\n",
    "        img = Image.open(framename).convert('RGB')\n",
    "        return self._image_transformer(img), 0, int(self.labels[index])\n",
    "\n",
    "\n",
    "class JigsawTestDatasetMultiple(JigsawDataset):\n",
    "    def __init__(self, *args, **xargs):\n",
    "        super().__init__(*args, **xargs)\n",
    "        self._image_transformer = transforms.Compose([\n",
    "            transforms.Resize(255, Image.BILINEAR),\n",
    "        ])\n",
    "        self._image_transformer_full = transforms.Compose([\n",
    "            transforms.Resize(225, Image.BILINEAR),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self._augment_tile = transforms.Compose([\n",
    "            transforms.Resize((75, 75), Image.BILINEAR),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        framename = self.data_path + self.names[index]\n",
    "        _img = Image.open(framename).convert('RGB')\n",
    "        img = self._image_transformer(_img)\n",
    "\n",
    "        w = float(img.size[0]) / self.grid_size\n",
    "        n_grids = self.grid_size ** 2\n",
    "        images = []\n",
    "        jig_labels = []\n",
    "        tiles = [None] * n_grids\n",
    "        for n in range(n_grids):\n",
    "            y = int(n / self.grid_size)\n",
    "            x = n % self.grid_size\n",
    "            tile = img.crop([x * w, y * w, (x + 1) * w, (y + 1) * w])\n",
    "            tile = self._augment_tile(tile)\n",
    "            tiles[n] = tile\n",
    "        for order in range(0, len(self.permutations)+1, 3):\n",
    "            if order==0:\n",
    "                data = tiles\n",
    "            else:\n",
    "                data = [tiles[self.permutations[order-1][t]] for t in range(n_grids)]\n",
    "            data = self.returnFunc(torch.stack(data, 0))\n",
    "            images.append(data)\n",
    "            jig_labels.append(order)\n",
    "        images = torch.stack(images, 0)\n",
    "        jig_labels = torch.LongTensor(jig_labels)\n",
    "        return images, jig_labels, int(self.labels[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4877e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat_dataset.py\n",
    "\n",
    "\n",
    "class ConcatDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset to concatenate multiple datasets.\n",
    "    Purpose: useful to assemble different existing datasets, possibly\n",
    "    large-scale datasets as the concatenation operation is done in an\n",
    "    on-the-fly manner.\n",
    "\n",
    "    Arguments:\n",
    "        datasets (sequence): List of datasets to be concatenated\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def cumsum(sequence):\n",
    "        r, s = [], 0\n",
    "        for e in sequence:\n",
    "            l = len(e)\n",
    "            r.append(l + s)\n",
    "            s += l\n",
    "        return r\n",
    "\n",
    "    def isMulti(self):\n",
    "        return isinstance(self.datasets[0], JigsawTestDatasetMultiple)\n",
    "\n",
    "    def __init__(self, datasets):\n",
    "        super(ConcatDataset, self).__init__()\n",
    "        assert len(datasets) > 0, 'datasets should not be an empty iterable'\n",
    "        self.datasets = list(datasets)\n",
    "        self.cumulative_sizes = self.cumsum(self.datasets)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.cumulative_sizes[-1]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        dataset_idx = bisect.bisect_right(self.cumulative_sizes, idx)\n",
    "        if dataset_idx == 0:\n",
    "            sample_idx = idx\n",
    "        else:\n",
    "            sample_idx = idx - self.cumulative_sizes[dataset_idx - 1]\n",
    "        return self.datasets[dataset_idx][sample_idx], dataset_idx\n",
    "\n",
    "    @property\n",
    "    def cummulative_sizes(self):\n",
    "        warnings.warn(\"cummulative_sizes attribute is renamed to \"\n",
    "                      \"cumulative_sizes\", DeprecationWarning, stacklevel=2)\n",
    "        return self.cumulative_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e4ab900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_helper.py\n",
    "\n",
    "mnist = 'mnist'\n",
    "mnist_m = 'mnist_m'\n",
    "svhn = 'svhn'\n",
    "synth = 'synth'\n",
    "usps = 'usps'\n",
    "\n",
    "vlcs_datasets = [\"CALTECH\", \"LABELME\", \"PASCAL\", \"SUN\"]\n",
    "pacs_datasets = [\"art_painting\", \"cartoon\", \"photo\", \"sketch\"]\n",
    "office_datasets = [\"amazon\", \"dslr\", \"webcam\"]\n",
    "digits_datasets = [mnist, mnist, svhn, usps]\n",
    "available_datasets = office_datasets + pacs_datasets + vlcs_datasets + digits_datasets\n",
    "#office_paths = {dataset: \"/home/enoon/data/images/office/%s\" % dataset for dataset in office_datasets}\n",
    "#pacs_paths = {dataset: \"/home/enoon/data/images/PACS/kfold/%s\" % dataset for dataset in pacs_datasets}\n",
    "vlcs_paths = {dataset: \"/home/goulmdata/images/VLCS/%s/test\" % dataset for dataset in vlcs_datasets}\n",
    "#paths = {**office_paths, **pacs_paths, **vlcs_paths}\n",
    "\n",
    "dataset_std = {mnist: (0.30280363, 0.30280363, 0.30280363),\n",
    "               mnist_m: (0.2384788, 0.22375608, 0.24496263),\n",
    "               svhn: (0.1951134, 0.19804622, 0.19481073),\n",
    "               synth: (0.29410212, 0.2939651, 0.29404707),\n",
    "               usps: (0.25887518, 0.25887518, 0.25887518),\n",
    "               }\n",
    "\n",
    "dataset_mean = {mnist: (0.13909429, 0.13909429, 0.13909429),\n",
    "                mnist_m: (0.45920207, 0.46326601, 0.41085603),\n",
    "                svhn: (0.43744073, 0.4437959, 0.4733686),\n",
    "                synth: (0.46332872, 0.46316052, 0.46327512),\n",
    "                usps: (0.17025368, 0.17025368, 0.17025368),\n",
    "                }\n",
    "\n",
    "\n",
    "class Subset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, limit):\n",
    "        indices = torch.randperm(len(dataset))[:limit]\n",
    "        self.dataset = dataset\n",
    "        self.indices = indices\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.dataset[self.indices[idx]]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "\n",
    "def get_train_dataloader(args, patches):\n",
    "    dataset_list = args.source\n",
    "    assert isinstance(dataset_list, list)\n",
    "    datasets = []\n",
    "    val_datasets = []\n",
    "    img_transformer, tile_transformer = get_train_transformers(args)\n",
    "    limit = args.limit_source\n",
    "    for dname in dataset_list:\n",
    "        name_train, name_val, labels_train, labels_val = get_split_dataset_info(join(os.path.abspath(''), 'data/txt_lists', '%s_train.txt' % dname), args.val_size)\n",
    "        train_dataset = JigsawDataset(name_train, labels_train, patches=patches, img_transformer=img_transformer,\n",
    "                                      tile_transformer=tile_transformer, jig_classes=args.jigsaw_n_classes, bias_whole_image=args.bias_whole_image)\n",
    "        if limit:\n",
    "            train_dataset = Subset(train_dataset, limit)\n",
    "        datasets.append(train_dataset)\n",
    "        val_datasets.append(JigsawTestDataset(name_val, labels_val, img_transformer=get_val_transformer(args),\n",
    "                              patches=patches, jig_classes=args.jigsaw_n_classes))\n",
    "    dataset = ConcatDataset(datasets)\n",
    "    val_dataset = ConcatDataset(val_datasets)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True, drop_last=False)\n",
    "    return loader, val_loader\n",
    "\n",
    "\n",
    "def get_val_dataloader(args, patches=False):\n",
    "    names, labels = _dataset_info(join(os.path.abspath(''), 'data/txt_lists', '%s_test.txt' % args.target))\n",
    "    img_tr = get_val_transformer(args)\n",
    "    val_dataset = JigsawTestDataset(names, labels, patches=patches, img_transformer=img_tr, jig_classes=args.jigsaw_n_classes)\n",
    "    if args.limit_target and len(val_dataset) > args.limit_target:\n",
    "        val_dataset = Subset(val_dataset, args.limit_target)\n",
    "        print(\"Using %d subset of val dataset\" % args.limit_target)\n",
    "    dataset = ConcatDataset([val_dataset])\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True, drop_last=False)\n",
    "    return loader\n",
    "\n",
    "\n",
    "def get_jigsaw_val_dataloader(args, patches=False):\n",
    "    names, labels = _dataset_info(join(os.path.abspath(''), 'data/txt_lists', '%s_test.txt' % args.target))\n",
    "    img_tr = [transforms.Resize((args.image_size, args.image_size))]\n",
    "    tile_tr = [transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
    "    img_transformer = transforms.Compose(img_tr)\n",
    "    tile_transformer = transforms.Compose(tile_tr)\n",
    "    val_dataset = JigsawDataset(names, labels, patches=patches, img_transformer=img_transformer,\n",
    "                                      tile_transformer=tile_transformer, jig_classes=args.jigsaw_n_classes, bias_whole_image=args.bias_whole_image)\n",
    "    if args.limit_target and len(val_dataset) > args.limit_target:\n",
    "        val_dataset = Subset(val_dataset, args.limit_target)\n",
    "        print(\"Using %d subset of val dataset\" % args.limit_target)\n",
    "    dataset = ConcatDataset([val_dataset])\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=False, num_workers=4, pin_memory=True, drop_last=False)\n",
    "    return loader\n",
    "\n",
    "\n",
    "def get_train_transformers(args):\n",
    "    img_tr = [transforms.RandomResizedCrop((int(args.image_size), int(args.image_size)), (args.min_scale, args.max_scale))]\n",
    "    if args.random_horiz_flip > 0.0:\n",
    "        img_tr.append(transforms.RandomHorizontalFlip(args.random_horiz_flip))\n",
    "    if args.jitter > 0.0:\n",
    "        img_tr.append(transforms.ColorJitter(brightness=args.jitter, contrast=args.jitter, saturation=args.jitter, hue=min(0.5, args.jitter)))\n",
    "\n",
    "    tile_tr = []\n",
    "    if args.tile_random_grayscale:\n",
    "        tile_tr.append(transforms.RandomGrayscale(args.tile_random_grayscale))\n",
    "    tile_tr = tile_tr + [transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
    "\n",
    "    return transforms.Compose(img_tr), transforms.Compose(tile_tr)\n",
    "\n",
    "\n",
    "def get_val_transformer(args):\n",
    "    img_tr = [transforms.Resize((args.image_size, args.image_size)), transforms.ToTensor(),transforms.Normalize([0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
    "    return transforms.Compose(img_tr)\n",
    "\n",
    "\n",
    "def get_target_jigsaw_loader(args):\n",
    "    img_transformer, tile_transformer = get_train_transformers(args)\n",
    "    name_train, _, labels_train, _ = get_split_dataset_info(join(os.path.abspath(''), 'data/txt_lists', '%s_train.txt' % args.target), 0)\n",
    "    dataset = JigsawDataset(name_train, labels_train, patches=False, img_transformer=img_transformer,tile_transformer=tile_transformer, jig_classes=args.jigsaw_n_classes, bias_whole_image=args.bias_whole_image)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=True, num_workers=4, pin_memory=True, drop_last=True)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854bd607",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d2f4ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e04504a",
   "metadata": {},
   "source": [
    "#### Fichier de /optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4999283b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer_helper.py\n",
    "\n",
    "def get_optim_and_scheduler(network, epochs, lr, train_all, nesterov=False):\n",
    "    if train_all:\n",
    "        params = network.parameters()\n",
    "    else:\n",
    "        params = network.get_params(lr)\n",
    "    optimizer = optim.SGD(params, weight_decay=.0005, momentum=.9, nesterov=nesterov, lr=lr)\n",
    "    #optimizer = optim.Adam(params, lr=lr)\n",
    "    step_size = int(epochs * .8)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size)\n",
    "    print(\"Step size: %d\" % step_size)\n",
    "    return optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cf1089",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665c0187",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c29b0b71",
   "metadata": {},
   "source": [
    "#### Fichiers de /utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0b803f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_logger.py\n",
    "\n",
    "class TFLogger(object):\n",
    "    \n",
    "    def __init__(self, log_dir):\n",
    "        \"\"\"Create a summary writer logging to log_dir.\"\"\"\n",
    "        self.writer = tf.summary.create_file_writer(log_dir)\n",
    "        #self.writer = tf.summary.FileWriter(log_dir)\n",
    "\n",
    "    def scalar_summary(self, tag, value, step):\n",
    "        \"\"\"Log a scalar variable.\"\"\"\n",
    "        #summary = tf.Summary(value=[tf.Summary.Value(tag=tag, simple_value=value)])\n",
    "        #self.writer.add_summary(summary, step)\n",
    "        with self.writer.as_default():\n",
    "            tf.summary.scalar(tag, value, step=step)\n",
    "            self.writer.flush()\n",
    "            \n",
    "    def image_summary(self, tag, images, step):\n",
    "        \"\"\"Log a list of images.\"\"\"\n",
    "\n",
    "        img_summaries = []\n",
    "        for i, img in enumerate(images):\n",
    "            # Write the image to a string\n",
    "            try:\n",
    "                s = StringIO()\n",
    "            except:\n",
    "                s = BytesIO()\n",
    "            scipy.misc.toimage(img).save(s, format=\"png\")\n",
    "\n",
    "            # Create an Image object\n",
    "            img_sum = tf.Summary.Image(encoded_image_string=s.getvalue(),\n",
    "                                       height=img.shape[0],\n",
    "                                       width=img.shape[1])\n",
    "            # Create a Summary value\n",
    "            img_summaries.append(tf.Summary.Value(tag='%s/%d' % (tag, i), image=img_sum))\n",
    "\n",
    "        # Create and write Summary\n",
    "        summary = tf.Summary(value=img_summaries)\n",
    "        self.writer.add_summary(summary, step)\n",
    "        \n",
    "    def histo_summary(self, tag, values, step, bins=1000):\n",
    "        \"\"\"Log a histogram of the tensor of values.\"\"\"\n",
    "\n",
    "        # Create a histogram using numpy\n",
    "        counts, bin_edges = np.histogram(values, bins=bins)\n",
    "\n",
    "        # Fill the fields of the histogram proto\n",
    "        hist = tf.HistogramProto()\n",
    "        hist.min = float(np.min(values))\n",
    "        hist.max = float(np.max(values))\n",
    "        hist.num = int(np.prod(values.shape))\n",
    "        hist.sum = float(np.sum(values))\n",
    "        hist.sum_squares = float(np.sum(values**2))\n",
    "\n",
    "        # Drop the start of the first bin\n",
    "        bin_edges = bin_edges[1:]\n",
    "\n",
    "        # Add bin edges and counts\n",
    "        for edge in bin_edges:\n",
    "            hist.bucket_limit.append(edge)\n",
    "        for c in counts:\n",
    "            hist.bucket.append(c)\n",
    "\n",
    "        # Create and write Summary\n",
    "        summary = tf.Summary(value=[tf.Summary.Value(tag=tag, histo=hist)])\n",
    "        self.writer.add_summary(summary, step)\n",
    "        self.writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7049ad21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger.py\n",
    "\n",
    "\n",
    "_log_path = join(os.path.abspath(''), '../logs')\n",
    "\n",
    "\n",
    "# high level wrapper for tf_logger.TFLogger\n",
    "class Logger():\n",
    "    def __init__(self, args, update_frequency=10):\n",
    "        self.current_epoch = 0\n",
    "        self.max_epochs = args.epochs\n",
    "        self.last_update = time()\n",
    "        self.start_time = time()\n",
    "        self._clean_epoch_stats()\n",
    "        self.update_f = update_frequency\n",
    "        folder, logname = self.get_name_from_args(args)\n",
    "        log_path = join(_log_path, folder, logname)\n",
    "        if args.tf_logger:\n",
    "            self.tf_logger = TFLogger(log_path)\n",
    "            print(\"Saving to %s\" % log_path)\n",
    "        else:\n",
    "            self.tf_logger = None\n",
    "        self.current_iter = 0\n",
    "\n",
    "    def new_epoch(self, learning_rates):\n",
    "        self.current_epoch += 1\n",
    "        self.last_update = time()\n",
    "        self.lrs = learning_rates\n",
    "        print(\"New epoch - lr: %s\" % \", \".join([str(lr) for lr in self.lrs]))\n",
    "        self._clean_epoch_stats()\n",
    "        if self.tf_logger:\n",
    "            for n, v in enumerate(self.lrs):\n",
    "                self.tf_logger.scalar_summary(\"aux/lr%d\" % n, v, self.current_iter)\n",
    "\n",
    "    def log(self, it, iters, losses, samples_right, total_samples):\n",
    "        self.current_iter += 1\n",
    "        loss_string = \", \".join([\"%s : %.3f\" % (k, v) for k, v in losses.items()])\n",
    "        for k, v in samples_right.items():\n",
    "            past = self.epoch_stats.get(k, 0.0)\n",
    "            self.epoch_stats[k] = past + v\n",
    "        self.total += total_samples\n",
    "        acc_string = \", \".join([\"%s : %.2f\" % (k, 100 * (v / total_samples)) for k, v in samples_right.items()])\n",
    "        if it % self.update_f == 0:\n",
    "            print(\"%d/%d of epoch %d/%d %s - acc %s [bs:%d]\" % (it, iters, self.current_epoch, self.max_epochs, loss_string,\n",
    "                                                                acc_string, total_samples))\n",
    "            # update tf log\n",
    "            if self.tf_logger:\n",
    "                for k, v in losses.items(): self.tf_logger.scalar_summary(\"train/loss_%s\" % k, v, self.current_iter)\n",
    "\n",
    "    def _clean_epoch_stats(self):\n",
    "        self.epoch_stats = {}\n",
    "        self.total = 0\n",
    "\n",
    "    def log_test(self, phase, accuracies):\n",
    "        print(\"Accuracies on %s: \" % phase + \", \".join([\"%s : %.2f\" % (k, v * 100) for k, v in accuracies.items()]))\n",
    "        if self.tf_logger:\n",
    "            for k, v in accuracies.items(): self.tf_logger.scalar_summary(\"%s/acc_%s\" % (phase, k), v, self.current_iter)\n",
    "\n",
    "    def save_best(self, val_test, best_test):\n",
    "        print(\"It took %g\" % (time() - self.start_time))\n",
    "        if self.tf_logger:\n",
    "            for x in range(10):\n",
    "                self.tf_logger.scalar_summary(\"best/from_val_test\", val_test, x)\n",
    "                self.tf_logger.scalar_summary(\"best/max_test\", best_test, x)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_name_from_args(args):\n",
    "        folder_name = \"%s_to_%s\" % (\"-\".join(sorted(args.source)), args.target)\n",
    "        if args.folder_name:\n",
    "            folder_name = join(args.folder_name, folder_name)\n",
    "        name = \"eps%d_bs%d_lr%g_class%d_jigClass%d_jigWeight%g\" % (args.epochs, args.batch_size, args.learning_rate, args.n_classes,\n",
    "                                                                   args.jigsaw_n_classes, args.jig_weight)\n",
    "        # if args.ooo_weight > 0:\n",
    "        #     name += \"_oooW%g\" % args.ooo_weight\n",
    "        if args.train_all:\n",
    "            name += \"_TAll\"\n",
    "        if args.bias_whole_image:\n",
    "            name += \"_bias%g\" % args.bias_whole_image\n",
    "        if args.classify_only_sane:\n",
    "            name += \"_classifyOnlySane\"\n",
    "        if args.TTA:\n",
    "            name += \"_TTA\"\n",
    "        try:\n",
    "            name += \"_entropy%g_jig_tW%g\" % (args.entropy_weight, args.target_weight)\n",
    "        except AttributeError:\n",
    "            pass\n",
    "        if args.suffix:\n",
    "            name += \"_%s\" % args.suffix\n",
    "        name += \"_%d\" % int(time() % 1000)\n",
    "        return folder_name, name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37579963",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696f91e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c630175",
   "metadata": {},
   "source": [
    "#### Fichier principal train_jigsaw.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1303518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, args, device):\n",
    "        self.args = args\n",
    "        self.device = device\n",
    "        model = get_network(args.network)(jigsaw_classes=args.jigsaw_n_classes + 1, classes=args.n_classes)\n",
    "        self.model = model.to(device)\n",
    "        # print(self.model)\n",
    "        self.source_loader, self.val_loader = get_train_dataloader(args, patches=model.is_patch_based())\n",
    "        self.target_loader = get_val_dataloader(args, patches=model.is_patch_based())\n",
    "        self.test_loaders = {\"val\": self.val_loader, \"test\": self.target_loader}\n",
    "        self.len_dataloader = len(self.source_loader)\n",
    "        print(\"Dataset size: train %d, val %d, test %d\" % (len(self.source_loader.dataset), len(self.val_loader.dataset), len(self.target_loader.dataset)))\n",
    "        self.optimizer, self.scheduler = get_optim_and_scheduler(model, args.epochs, args.learning_rate, args.train_all, nesterov=args.nesterov)\n",
    "        self.jig_weight = args.jig_weight\n",
    "        self.only_non_scrambled = args.classify_only_sane\n",
    "        self.n_classes = args.n_classes\n",
    "        if args.target in args.source:\n",
    "            self.target_id = args.source.index(args.target)\n",
    "            print(\"Target in source: %d\" % self.target_id)\n",
    "            print(args.source)\n",
    "        else:\n",
    "            self.target_id = None\n",
    "\n",
    "    def _do_epoch(self):\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        self.model.train()\n",
    "        for it, ((data, jig_l, class_l), d_idx) in enumerate(self.source_loader):\n",
    "            data, jig_l, class_l, d_idx = data.to(self.device), jig_l.to(self.device), class_l.to(self.device), d_idx.to(self.device)\n",
    "            # absolute_iter_count = it + self.current_epoch * self.len_dataloader\n",
    "            # p = float(absolute_iter_count) / self.args.epochs / self.len_dataloader\n",
    "            # lambda_val = 2. / (1. + np.exp(-10 * p)) - 1\n",
    "            # if domain_error > 2.0:\n",
    "            #     lambda_val  = 0\n",
    "            # print(\"Shutting down LAMBDA to prevent implosion\")\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            jigsaw_logit, class_logit = self.model(data)  # , lambda_val=lambda_val)\n",
    "            jigsaw_loss = criterion(jigsaw_logit, jig_l)\n",
    "            # domain_loss = criterion(domain_logit, d_idx)\n",
    "            # domain_error = domain_loss.item()\n",
    "            if self.only_non_scrambled:\n",
    "                if self.target_id is not None:\n",
    "                    idx = (jig_l == 0) & (d_idx != self.target_id)\n",
    "                    class_loss = criterion(class_logit[idx], class_l[idx])\n",
    "                else:\n",
    "                    class_loss = criterion(class_logit[jig_l == 0], class_l[jig_l == 0])\n",
    "\n",
    "            elif self.target_id:\n",
    "                class_loss = criterion(class_logit[d_idx != self.target_id], class_l[d_idx != self.target_id])\n",
    "            else:\n",
    "                class_loss = criterion(class_logit, class_l)\n",
    "            _, cls_pred = class_logit.max(dim=1)\n",
    "            _, jig_pred = jigsaw_logit.max(dim=1)\n",
    "            # _, domain_pred = domain_logit.max(dim=1)\n",
    "            loss = class_loss + jigsaw_loss * self.jig_weight  # + 0.1 * domain_loss\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            self.logger.log(it, len(self.source_loader),\n",
    "                            {\"jigsaw\": jigsaw_loss.item(), \"class\": class_loss.item()  # , \"domain\": domain_loss.item()\n",
    "                             },\n",
    "                            # ,\"lambda\": lambda_val},\n",
    "                            {\"jigsaw\": torch.sum(jig_pred == jig_l.data).item(),\n",
    "                             \"class\": torch.sum(cls_pred == class_l.data).item(),\n",
    "                             # \"domain\": torch.sum(domain_pred == d_idx.data).item()\n",
    "                             },\n",
    "                            data.shape[0])\n",
    "            del loss, class_loss, jigsaw_loss, jigsaw_logit, class_logit\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for phase, loader in self.test_loaders.items():\n",
    "                total = len(loader.dataset)\n",
    "                if loader.dataset.isMulti():\n",
    "                    jigsaw_correct, class_correct, single_acc = self.do_test_multi(loader)\n",
    "                    print(\"Single vs multi: %g %g\" % (float(single_acc) / total, float(class_correct) / total))\n",
    "                else:\n",
    "                    jigsaw_correct, class_correct = self.do_test(loader)\n",
    "                jigsaw_acc = float(jigsaw_correct) / total\n",
    "                class_acc = float(class_correct) / total\n",
    "                self.logger.log_test(phase, {\"jigsaw\": jigsaw_acc, \"class\": class_acc})\n",
    "                self.results[phase][self.current_epoch] = class_acc\n",
    "\n",
    "    def do_test(self, loader):\n",
    "        jigsaw_correct = 0\n",
    "        class_correct = 0\n",
    "        domain_correct = 0\n",
    "        for it, ((data, jig_l, class_l), _) in enumerate(loader):\n",
    "            data, jig_l, class_l = data.to(self.device), jig_l.to(self.device), class_l.to(self.device)\n",
    "            jigsaw_logit, class_logit = self.model(data)\n",
    "            _, cls_pred = class_logit.max(dim=1)\n",
    "            _, jig_pred = jigsaw_logit.max(dim=1)\n",
    "            class_correct += torch.sum(cls_pred == class_l.data)\n",
    "            jigsaw_correct += torch.sum(jig_pred == jig_l.data)\n",
    "        return jigsaw_correct, class_correct\n",
    "\n",
    "    def do_test_multi(self, loader):\n",
    "        jigsaw_correct = 0\n",
    "        class_correct = 0\n",
    "        single_correct = 0\n",
    "        for it, ((data, jig_l, class_l), d_idx) in enumerate(loader):\n",
    "            data, jig_l, class_l = data.to(self.device), jig_l.to(self.device), class_l.to(self.device)\n",
    "            n_permutations = data.shape[1]\n",
    "            class_logits = torch.zeros(n_permutations, data.shape[0], self.n_classes).to(self.device)\n",
    "            for k in range(n_permutations):\n",
    "                class_logits[k] = F.softmax(self.model(data[:, k])[1], dim=1)\n",
    "            class_logits[0] *= 4 * n_permutations  # bias more the original image\n",
    "            class_logit = class_logits.mean(0)\n",
    "            _, cls_pred = class_logit.max(dim=1)\n",
    "            jigsaw_logit, single_logit = self.model(data[:, 0])\n",
    "            _, jig_pred = jigsaw_logit.max(dim=1)\n",
    "            _, single_logit = single_logit.max(dim=1)\n",
    "            single_correct += torch.sum(single_logit == class_l.data)\n",
    "            class_correct += torch.sum(cls_pred == class_l.data)\n",
    "            jigsaw_correct += torch.sum(jig_pred == jig_l.data[:, 0])\n",
    "        return jigsaw_correct, class_correct, single_correct\n",
    "\n",
    "    def do_training(self):\n",
    "        self.logger = Logger(self.args, update_frequency=30)  # , \"domain\", \"lambda\"\n",
    "        self.results = {\"val\": torch.zeros(self.args.epochs), \"test\": torch.zeros(self.args.epochs)}\n",
    "        for self.current_epoch in range(self.args.epochs):\n",
    "            self.scheduler.step()\n",
    "            self.logger.new_epoch(self.scheduler.get_lr())\n",
    "            self._do_epoch()\n",
    "        val_res = self.results[\"val\"]\n",
    "        test_res = self.results[\"test\"]\n",
    "        idx_best = val_res.argmax()\n",
    "        #print(\"Best val %g, corresponding test %g - best test: %g\" % (val_res.max(), test_res[idx_best], test_res.max()))\n",
    "        self.logger.save_best(test_res[idx_best], test_res.max())\n",
    "        return self.logger, self.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e569807e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: train 7150, val 793, test 2048\n",
      "Step size: 4\n",
      "Saving to /home/tiphaign/Documents/5A/HDDL/Projet_HDDL/JigenDG-master/../logs/Test/cartoon-photo-sketch_to_art_painting/eps5_bs64_lr0.01_class7_jigClass31_jigWeight0.7_TAll_bias0.9_113\n",
      "New epoch - lr: 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 14:41:53.003685: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "/home/tiphaign/.local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/tiphaign/.local/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:384: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/111 of epoch 1/5 jigsaw : 3.254, class : 1.974 - acc jigsaw : 10.94, class : 17.19 [bs:64]\n",
      "30/111 of epoch 1/5 jigsaw : 0.440, class : 0.848 - acc jigsaw : 89.06, class : 67.19 [bs:64]\n",
      "60/111 of epoch 1/5 jigsaw : 0.321, class : 0.378 - acc jigsaw : 92.19, class : 85.94 [bs:64]\n",
      "90/111 of epoch 1/5 jigsaw : 0.427, class : 0.434 - acc jigsaw : 89.06, class : 85.94 [bs:64]\n",
      "Accuracies on val: jigsaw : 100.00, class : 89.28\n",
      "Accuracies on test: jigsaw : 100.00, class : 69.34\n",
      "New epoch - lr: 0.01\n",
      "0/111 of epoch 2/5 jigsaw : 0.140, class : 0.289 - acc jigsaw : 96.88, class : 92.19 [bs:64]\n",
      "30/111 of epoch 2/5 jigsaw : 0.567, class : 0.213 - acc jigsaw : 84.38, class : 89.06 [bs:64]\n",
      "60/111 of epoch 2/5 jigsaw : 0.226, class : 0.311 - acc jigsaw : 93.75, class : 92.19 [bs:64]\n",
      "90/111 of epoch 2/5 jigsaw : 0.217, class : 0.329 - acc jigsaw : 95.31, class : 90.62 [bs:64]\n",
      "Accuracies on val: jigsaw : 100.00, class : 90.67\n",
      "Accuracies on test: jigsaw : 99.90, class : 72.61\n",
      "New epoch - lr: 0.01\n",
      "0/111 of epoch 3/5 jigsaw : 0.382, class : 0.122 - acc jigsaw : 89.06, class : 93.75 [bs:64]\n",
      "30/111 of epoch 3/5 jigsaw : 0.263, class : 0.040 - acc jigsaw : 93.75, class : 100.00 [bs:64]\n",
      "60/111 of epoch 3/5 jigsaw : 0.373, class : 0.100 - acc jigsaw : 89.06, class : 96.88 [bs:64]\n",
      "90/111 of epoch 3/5 jigsaw : 0.488, class : 0.109 - acc jigsaw : 85.94, class : 95.31 [bs:64]\n",
      "Accuracies on val: jigsaw : 100.00, class : 91.68\n",
      "Accuracies on test: jigsaw : 99.95, class : 74.66\n",
      "New epoch - lr: 0.0001\n",
      "0/111 of epoch 4/5 jigsaw : 0.257, class : 0.277 - acc jigsaw : 93.75, class : 90.62 [bs:64]\n",
      "30/111 of epoch 4/5 jigsaw : 0.320, class : 0.133 - acc jigsaw : 90.62, class : 98.44 [bs:64]\n",
      "60/111 of epoch 4/5 jigsaw : 0.561, class : 0.118 - acc jigsaw : 84.38, class : 96.88 [bs:64]\n",
      "90/111 of epoch 4/5 jigsaw : 0.325, class : 0.140 - acc jigsaw : 92.19, class : 95.31 [bs:64]\n",
      "Accuracies on val: jigsaw : 100.00, class : 96.72\n",
      "Accuracies on test: jigsaw : 99.95, class : 77.98\n",
      "New epoch - lr: 0.001\n",
      "0/111 of epoch 5/5 jigsaw : 0.328, class : 0.082 - acc jigsaw : 90.62, class : 98.44 [bs:64]\n",
      "30/111 of epoch 5/5 jigsaw : 0.332, class : 0.036 - acc jigsaw : 90.62, class : 98.44 [bs:64]\n",
      "60/111 of epoch 5/5 jigsaw : 0.297, class : 0.058 - acc jigsaw : 92.19, class : 98.44 [bs:64]\n",
      "90/111 of epoch 5/5 jigsaw : 0.467, class : 0.016 - acc jigsaw : 85.94, class : 100.00 [bs:64]\n",
      "Accuracies on val: jigsaw : 100.00, class : 96.85\n",
      "Accuracies on test: jigsaw : 99.95, class : 76.95\n",
      "It took 240.888\n"
     ]
    }
   ],
   "source": [
    "# main du code python\n",
    "\n",
    "args = Args()\n",
    "\n",
    "def main(args):\n",
    "\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    trainer = Trainer(args, device)\n",
    "    trainer.do_training()\n",
    "\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8703445e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e03472",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "Louison Bocquet--Nouaille"
   },
   {
    "name": "Pierre-Alain Goulm"
   },
   {
    "name": "Romain Tiphaigne"
   },
   {
    "name": "Axelle Tragné"
   },
   {
    "name": "Alicia Zady"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "title": "Domain Generalization by Solving Jigsaw Puzzles",
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
